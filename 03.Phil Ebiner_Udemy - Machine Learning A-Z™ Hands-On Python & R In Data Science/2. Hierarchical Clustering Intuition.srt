1

00:00:00,730  -->  00:00:03,330
Hello and welcome back to the course on machine learning.

2

00:00:03,420  -->  00:00:07,650
And in this section we're kicking off the tutorials for hierarchical clustering.

3

00:00:07,650  -->  00:00:13,380
All right so another very interesting topic ahead of us and as always let's make the complex a simple

4

00:00:13,410  -->  00:00:16,350
and demystify what it's all about.

5

00:00:16,350  -->  00:00:18,640
All right so what is hierarchical clustering.

6

00:00:18,810  -->  00:00:20,000
Well believe it or not.

7

00:00:20,130  -->  00:00:27,180
But if you have points on your scatterplot or data points like we looked at previously this is a two

8

00:00:27,180  -->  00:00:28,200
dimensional space.

9

00:00:28,380  -->  00:00:32,790
If you apply a hierarchical clustering or just say H.C. for short.

10

00:00:32,800  -->  00:00:40,620
What'll happen is you will get clusters again a very very similar to Kamins In fact sometimes the result

11

00:00:40,950  -->  00:00:45,510
of no results can be exactly the same as k means clustering.

12

00:00:45,720  -->  00:00:48,260
But the whole process is a bit different.

13

00:00:48,270  -->  00:00:50,510
So let's talk about it in a bit more detail.

14

00:00:50,760  -->  00:00:55,530
So the first thing that we need to note is that there's two types of hierarchical clustering agglomerative

15

00:00:55,950  -->  00:00:57,070
and divisive.

16

00:00:57,080  -->  00:01:03,000
So agglomerated is the bottom up approach and you'll see in more detail just now what that means so

17

00:01:03,000  -->  00:01:07,680
we'll be starting at the very bottom and then building our way up device of his opposite starting at

18

00:01:07,680  -->  00:01:14,580
the top and devising a Klosters into multiple in this course we'll be focusing on our glomerata of approach

19

00:01:15,000  -->  00:01:19,890
the divisive or we'll mentioned Ebbetts just now where we're talking about a glitch but it's basically

20

00:01:19,890  -->  00:01:20,570
the same thing.

21

00:01:20,580  -->  00:01:23,380
But the other way around it's the reverse of the glummer trip.

22

00:01:23,550  -->  00:01:27,220
And if you like you can definitely research more about the device approach.

23

00:01:27,330  -->  00:01:30,880
But for now we're going to focus on the glummer of one.

24

00:01:30,900  -->  00:01:31,190
All right.

25

00:01:31,200  -->  00:01:35,850
So the glomerata of hierarchical clustering How does it work well to start off we're going to break

26

00:01:35,850  -->  00:01:40,440
it down into step by step approach and then we'll look at an example and manually perform the classroom

27

00:01:40,450  -->  00:01:40,670
.

28

00:01:40,890  -->  00:01:47,260
All right so step one in H.C. is to make each data point a single point cluster so that forms and or

29

00:01:47,520  -->  00:01:48,620
and data points.

30

00:01:48,630  -->  00:01:53,560
Your first step is to look at each one of them as an individual cluster.

31

00:01:53,640  -->  00:01:59,910
Then Step two is to take the two closest data points and make them one cluster to combine them into

32

00:01:59,910  -->  00:02:05,910
one cluster that forms and minus one clusters then sip that is to take the two closest classes out of

33

00:02:05,910  -->  00:02:11,800
the ones now that you now have and make them one cluster that forms and minus two clusters then it's

34

00:02:11,820  -->  00:02:18,060
a to repeat step 3 until there is only one class to basically just keep repeating step three and combining

35

00:02:18,300  -->  00:02:21,310
points into bigger and bigger and bigger clusters.

36

00:02:21,360  -->  00:02:24,180
And so there's only one huge cluster left.

37

00:02:24,180  -->  00:02:26,110
So you just keep repeating step three.

38

00:02:26,190  -->  00:02:33,810
And at the end you're done and you'll have one huge cluster left and how to get from one to two or three

39

00:02:33,810  -->  00:02:34,290
classes.

40

00:02:34,290  -->  00:02:36,390
How do you get the final result that you actually want.

41

00:02:36,390  -->  00:02:40,660
We'll talk about that in this section as well so that's that's the goal of course.

42

00:02:40,800  -->  00:02:47,490
But one thing that stands out here is the words closest clusters.

43

00:02:47,700  -->  00:02:52,580
Now we've already talked about distances and we mentioned you clearly that you can use Euclidean distances

44

00:02:52,580  -->  00:02:56,160
or other distances and that's totally fine when you're working with the usual point.

45

00:02:56,160  -->  00:03:01,500
But here we're actually going a step even further we're talking about not just the proximity of points

46

00:03:01,530  -->  00:03:03,730
but actually proximity of clusters.

47

00:03:03,810  -->  00:03:09,270
And this is something worth noting so I'd like to pause here for a bit and or I kind of step to the

48

00:03:09,270  -->  00:03:14,970
side and talk about the closeness of clusters and how you measure distance between clusters because

49

00:03:14,970  -->  00:03:19,020
that can really affect your algorithm if using hierarchical clustering.

50

00:03:19,020  -->  00:03:21,530
So let's talk about that for for a few minutes.

51

00:03:21,540  -->  00:03:27,180
So first of all Euclidean distance just to get this out of the way once and for all Euclidean distance

52

00:03:27,180  -->  00:03:30,120
is in a two dimensional space it's calculated like that.

53

00:03:30,120  -->  00:03:37,110
So the distance so x if you got two points one with the coordinates x 1 and Y 1 p to coordinate x to

54

00:03:37,260  -->  00:03:44,040
y to then nuclear and distance or the length of this line is calculated as X to minus x 1 so that the

55

00:03:44,040  -->  00:03:49,980
distance between the x's squared plus the distance between Y is squared.

56

00:03:49,980  -->  00:03:53,160
So basically and then you add them up and then take the square.

57

00:03:53,160  -->  00:03:58,960
So basically it's that and you've got a right angled triangle over here and you've got the cattle out

58

00:03:59,370  -->  00:04:01,800
here and you've got another one catgut here.

59

00:04:01,830  -->  00:04:06,510
I hope I'm pronouncing it right and then this is the hypothenuse right.

60

00:04:06,510  -->  00:04:10,860
So that is how you calculate the distance between the two points.

61

00:04:10,860  -->  00:04:14,680
So it's a basic geometry back from high school.

62

00:04:14,850  -->  00:04:20,220
And there you go that's just how Euclidean distances calculate and that's what we're going to be working

63

00:04:20,220  -->  00:04:20,340
with.

64

00:04:20,340  -->  00:04:26,220
But again there are other types of distances that you could be invoking in your algorithms and it really

65

00:04:26,220  -->  00:04:33,720
depends on the scenario and what exactly how your algorithm is going to be structured but in our examples

66

00:04:33,720  -->  00:04:39,630
we're going to be working with Euclidean distances because they're more natural type of distance.

67

00:04:39,630  -->  00:04:44,970
And now let's talk about the distance between two clusters so let's say you have two clusters the red

68

00:04:44,970  -->  00:04:45,330
and the blue.

69

00:04:45,330  -->  00:04:47,780
How do you measure the distance between them.

70

00:04:47,840  -->  00:04:50,970
What is the definition of the distance between two clusters.

71

00:04:50,970  -->  00:04:56,500
It's not as obvious as it might sound at the start because there can be a couple of options for instance

72

00:04:56,520  -->  00:04:57,050
you can.

73

00:04:57,180  -->  00:05:02,250
Option one is to take the two closest points and measure that and call that the distance between the

74

00:05:02,250  -->  00:05:03,480
two clusters.

75

00:05:03,480  -->  00:05:08,310
Option 2 is actually take the two further points and call that the distance between two classes is also

76

00:05:08,310  -->  00:05:09,390
a valid approach.

77

00:05:09,390  -->  00:05:14,280
Option 3 Take the average take the average of all the distances between the different points in the

78

00:05:14,320  -->  00:05:18,740
classes all the combinations of different points and take the average of that distance.

79

00:05:18,750  -->  00:05:23,520
Option 4 is take the distance between the centroid and find the Central is and find the distance between

80

00:05:23,910  -->  00:05:27,330
the centroid and call that the distance between the two clusters.

81

00:05:27,330  -->  00:05:34,170
So it's a very important part of her called clustering what you define as the distance between two classes

82

00:05:34,170  -->  00:05:39,060
because that can significantly impact the output of your algorithm.

83

00:05:39,060  -->  00:05:42,070
Now we're not going to delve much deeper into this.

84

00:05:42,090  -->  00:05:44,420
It's just something to remember to note.

85

00:05:44,550  -->  00:05:51,870
And based on your particular situation where there's a business problem or other type of data science

86

00:05:51,870  -->  00:05:56,970
problem based on what exactly do you think is the best approach you're going to need to define that

87

00:05:57,030  -->  00:06:02,790
in your algorithm so just keep that in mind that for the hierarchical clustering algorithm the distance

88

00:06:02,790  -->  00:06:08,880
between clusters is a crucial element and you need to remember what exactly you are setting at is you

89

00:06:08,880  -->  00:06:12,630
setting it to be how you defining it in your approach.

90

00:06:12,630  -->  00:06:13,080
All right.

91

00:06:13,110  -->  00:06:14,580
So we talked about that.

92

00:06:14,640  -->  00:06:21,060
Now let's move back to our example so we've already looked at the step by step rules and I I'm a big

93

00:06:21,060  -->  00:06:23,210
fan of step by step approach.

94

00:06:23,340  -->  00:06:28,530
Now we have this step by step approach and it might have seemed a bit overwhelming as always because

95

00:06:28,530  -->  00:06:29,340
we didn't have an example.

96

00:06:29,340  -->  00:06:34,300
But now we're going to have that example and we're going to look at how to build those hierarchical

97

00:06:34,380  -->  00:06:34,960
classes.

98

00:06:35,100  -->  00:06:40,740
So step one make each data point a single point cluster that forms a six clusters Let's have a look

99

00:06:40,740  -->  00:06:41,060
at that.

100

00:06:41,070  -->  00:06:44,070
You can see every single point is a separate cluster.

101

00:06:44,070  -->  00:06:51,270
Next take the two Close's data points and make them one closter where we can see that these two points

102

00:06:51,300  -->  00:06:52,190
are the closest.

103

00:06:52,200  -->  00:06:56,370
So we're putting them together in one class and now we have five classes one two three four and these

104

00:06:56,370  -->  00:06:57,770
two are one class.

105

00:06:57,780  -->  00:06:58,190
All right.

106

00:06:58,200  -->  00:07:03,820
Step three take the two closest clusters are the ones we had and turn them into one cluster.

107

00:07:03,870  -->  00:07:07,690
So out of the classes we have because remember each point are these four.

108

00:07:07,710  -->  00:07:11,160
So if we go back here each point here is a separate class and this is a classroom.

109

00:07:11,190  -->  00:07:15,750
So now just measure the distances between clusters and let's say in our example we're going to be talking

110

00:07:15,750  -->  00:07:20,820
about the distance between clusters as the minimum distance so that would be the distance between those

111

00:07:20,820  -->  00:07:25,500
two classers that would be the distance that would be that that would be an Sanchi measure all the difference

112

00:07:25,500  -->  00:07:30,120
between clusters and you find out that these are actually the closest clusters and you combine them

113

00:07:30,120  -->  00:07:31,410
into one class.

114

00:07:31,490  -->  00:07:36,810
Next you repeat step three so next out of these have coursers out of one two three four clusters you

115

00:07:36,810  -->  00:07:37,950
can see we had five.

116

00:07:37,950  -->  00:07:42,310
Now we have for every time we reduce the number of clusters by one of these four classes which are the

117

00:07:42,310  -->  00:07:43,020
class as well.

118

00:07:43,020  -->  00:07:46,700
The this seems as the closest cluster so we're going to combine that.

119

00:07:46,890  -->  00:07:50,190
Now out of these three clusters which of course it looks like these two are.

120

00:07:50,400  -->  00:07:54,990
So combining them and now we've only got two classes left so the last step would be to combine them

121

00:07:54,990  -->  00:07:58,460
because they are by default going to be the closest.

122

00:07:58,510  -->  00:07:59,040
So there we go.

123

00:07:59,040  -->  00:08:01,620
And so that is the end of our algorithm.

124

00:08:01,620  -->  00:08:02,970
That's how it converges.

125

00:08:03,060  -->  00:08:11,470
You've we've gone through the process of CRE of going from all points seen as an individual cluster

126

00:08:11,470  -->  00:08:16,920
so each point is an individual cluster to now we only have one cluster that combines all of the points

127

00:08:16,940  -->  00:08:17,220
.

128

00:08:17,400  -->  00:08:19,020
And so what's the purpose of all of this.

129

00:08:19,020  -->  00:08:20,550
What's the purpose of this exercise.

130

00:08:20,670  -->  00:08:27,810
The way the hierarchical clustering algorithm works is that it maintains a memory of how we went through

131

00:08:27,810  -->  00:08:32,910
this process and that memory is stored in a danger gram and that's exactly what we're going to talk

132

00:08:32,910  -->  00:08:38,820
in the next tutorial and once we've covered the Dendron grams it will make total sense why the hierarchical

133

00:08:38,820  -->  00:08:42,540
clustering algorithm does what it does and how it works.

134

00:08:42,540  -->  00:08:45,430
So we enjoy today sectorial can't wait to see the next one.

135

00:08:45,450  -->  00:08:47,860
Until then enjoy machine learning.
