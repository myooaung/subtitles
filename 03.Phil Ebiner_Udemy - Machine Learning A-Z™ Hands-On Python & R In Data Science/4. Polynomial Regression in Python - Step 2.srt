1
00:00:00,240 --> 00:00:01,140
Hello my friends.

2
00:00:01,140 --> 00:00:03,930
All right so let's implement quickly and efficiently.

3
00:00:03,930 --> 00:00:07,100
The linear regression model on the whole dataset.

4
00:00:07,200 --> 00:00:11,990
First a quick reminder on what is the linear regression mode that we're about to build.

5
00:00:12,010 --> 00:00:17,550
We're simply about to build this one right because we have only one feature which is the position levels

6
00:00:17,820 --> 00:00:21,280
and that have been invaluable which is of course the salary to predict.

7
00:00:21,330 --> 00:00:21,960
All right.

8
00:00:22,110 --> 00:00:27,390
So that's what we'll do first and then we'll of course implement the polynomial regression model with.

9
00:00:27,390 --> 00:00:32,220
I will tell you how many powers of that same position level feature.

10
00:00:32,220 --> 00:00:33,490
Okay so let's do this.

11
00:00:33,600 --> 00:00:36,940
Starting by creating a new code cell.

12
00:00:36,960 --> 00:00:37,340
All right.

13
00:00:37,350 --> 00:00:41,410
So I hope you did it yourself first to make sure you refresh your skills.

14
00:00:41,550 --> 00:00:48,300
And so if that's the case of course you started from the psychic learn library because that's the library

15
00:00:48,300 --> 00:00:53,970
that contains this linear regression class which allows us to build a linear regression model so cycle

16
00:00:53,970 --> 00:00:59,680
learn from which we gonna get access to the linear model which contains this class.

17
00:00:59,760 --> 00:01:07,250
And from this linear model we're going to import that linear regression class as Google Gulab guesses

18
00:01:07,260 --> 00:01:08,520
perfectly perfect.

19
00:01:08,550 --> 00:01:09,690
That's the first step.

20
00:01:09,690 --> 00:01:14,510
Then we're going to create an object of this class so I know we used to call it regressive.

21
00:01:14,610 --> 00:01:19,530
But this time since we're gonna have to regress regresses you know we're going to have the linear regression

22
00:01:19,650 --> 00:01:21,880
regressive and the polynomial regression regress.

23
00:01:22,200 --> 00:01:26,690
Therefore this time I'm going to call it Lin rake for linear regression.

24
00:01:26,950 --> 00:01:27,410
Okay.

25
00:01:27,510 --> 00:01:34,380
So Lin rec will be created as an object of this linear regression class so I'm copying and pasting that

26
00:01:34,380 --> 00:01:36,400
here and adding some parenthesis.

27
00:01:36,510 --> 00:01:41,700
And remember we don't have to input anything in these parentheses because there is not much to tune

28
00:01:41,750 --> 00:01:43,600
in the linear regression model.

29
00:01:43,860 --> 00:01:44,280
Good.

30
00:01:44,490 --> 00:01:50,130
And finally now we have you know the linear regression model but it is not smart yet it is not trained

31
00:01:50,130 --> 00:01:57,060
yet on this data set you know to understand and learn the correlations between the position levels and

32
00:01:57,060 --> 00:01:58,030
the salaries.

33
00:01:58,060 --> 00:02:03,960
So that's what we're gonna do right now by using this fit method which is exactly a training method

34
00:02:04,200 --> 00:02:07,110
that will train the model on these data.

35
00:02:07,110 --> 00:02:07,820
All right.

36
00:02:07,920 --> 00:02:16,290
So to do this we're gonna call our object first Lynn rag from which we're gonna call the fit method

37
00:02:16,620 --> 00:02:24,000
which remember has to take as input the couple of matrix of features and dependent variable vector of

38
00:02:24,000 --> 00:02:25,320
the training set right.

39
00:02:25,320 --> 00:02:30,450
That's what we did before but remember here that we didn't split the data set into a training set in

40
00:02:30,450 --> 00:02:36,060
a test because we want to leverage the maximum data in order to train our model and therefore this time

41
00:02:36,060 --> 00:02:40,350
we're going to take the whole matrix of Features X and the whole dependent variable vector Y.

42
00:02:40,350 --> 00:02:45,310
All right so here we just input x and y and there we go.

43
00:02:45,330 --> 00:02:50,930
We built the linear regression model in a flashlight so we're going to execute this.

44
00:02:50,940 --> 00:02:51,870
Here we go.

45
00:02:51,870 --> 00:02:57,300
And now we have not only our model but also a trained model on this dataset.

46
00:02:57,810 --> 00:02:58,920
Okay great.

47
00:02:58,920 --> 00:03:05,940
Now we're going to close this and we're going to focus on the heart of the matter which is the polynomial

48
00:03:05,940 --> 00:03:06,810
regression model.

49
00:03:06,810 --> 00:03:11,120
I'm going to teach you now how to build the polynomial regression model.

50
00:03:11,160 --> 00:03:11,420
All right.

51
00:03:11,430 --> 00:03:17,400
So we're going to create a new code cell and now we're gonna go back to this slide to explain exactly

52
00:03:17,400 --> 00:03:18,780
what we're going to do.

53
00:03:18,780 --> 00:03:19,010
All right.

54
00:03:19,020 --> 00:03:22,690
So as you understand what we've just built so far is this model.

55
00:03:22,770 --> 00:03:28,440
We only have one feature the position levels and the unavoidable vector to predict which are the salaries.

56
00:03:28,440 --> 00:03:33,690
Now what we're going to do is we're going to create a multiple in our regression model but instead of

57
00:03:33,690 --> 00:03:41,760
having different features in x1 x2 and x end well these features will be X1 X1 squared and X1 at the

58
00:03:41,760 --> 00:03:42,370
power.

59
00:03:42,560 --> 00:03:49,590
And we'll actually tune this parameter and a bit to try several powers so not to confuse the polynomial

60
00:03:49,650 --> 00:03:55,650
linear regression is not a linear model because you will see that it can learn some nonlinear correlations

61
00:03:55,890 --> 00:04:02,280
but we call it polynomial linear regression because indeed there is this linear combination of the squared

62
00:04:02,310 --> 00:04:06,780
and you know power features x1 X1 squared and X1 and the power.

63
00:04:07,260 --> 00:04:07,920
All right.

64
00:04:07,920 --> 00:04:15,630
And so the process of building this model on Python will be first to create a matrix of the power features

65
00:04:15,690 --> 00:04:20,820
you know a matrix of features but not containing different features like x1 x to an extent but a matrix

66
00:04:20,820 --> 00:04:26,820
of features containing X1 as a first feature then X1 squared as a second feature and then X1 and about

67
00:04:26,850 --> 00:04:29,040
and as an end feature.

68
00:04:29,040 --> 00:04:36,720
So that will be our matrix of features and we will call it X poly and then we will create a linear regress

69
00:04:36,720 --> 00:04:42,870
or object you know from the linear regression class to integrate these bound features of this matrix

70
00:04:42,870 --> 00:04:46,990
of features in this new linear aggressor you see the idea.

71
00:04:47,040 --> 00:04:48,900
So it's a building process in two steps.

72
00:04:48,960 --> 00:04:54,210
We're going to first create the matrix of features containing these features at different powers and

73
00:04:54,390 --> 00:05:00,580
then we'll integrate that into a linear regression model because indeed this is a linear nation of these

74
00:05:00,780 --> 00:05:02,200
powered futures.

75
00:05:02,260 --> 00:05:02,640
All right.

76
00:05:02,650 --> 00:05:03,190
Perfect.

77
00:05:03,190 --> 00:05:05,710
So let's do this first step.

78
00:05:05,710 --> 00:05:12,520
The first step is to actually import this tool that will allow us to create this matrix of powered features

79
00:05:12,820 --> 00:05:15,350
x1 X1 square next one at a power.

80
00:05:15,670 --> 00:05:18,170
And this class is called polynomial feature.

81
00:05:18,190 --> 00:05:24,460
So we're going to import it first which we can get from once again of course the psychic learn library.

82
00:05:24,460 --> 00:05:25,180
There we go.

83
00:05:25,360 --> 00:05:32,560
But this time we're gonna get access to the pre processing module which contains that polynomial features

84
00:05:32,560 --> 00:05:32,950
class.

85
00:05:32,950 --> 00:05:39,190
This is a class because indeed we're kind of pre processing our X1 feature because we want to create

86
00:05:39,400 --> 00:05:43,420
out of X1 well X1 X1 squared and X1 at the power.

87
00:05:43,420 --> 00:05:46,930
And so we're kind of doing pre processing of the features.

88
00:05:46,930 --> 00:05:47,220
All right.

89
00:05:47,220 --> 00:05:54,680
So from this pre pressing module we're going to import well that class po li no meal features.

90
00:05:54,700 --> 00:05:55,200
Perfect.

91
00:05:55,210 --> 00:05:59,740
I really like Google color so pulling on more features and then of course.

92
00:05:59,740 --> 00:06:01,750
Well you know any time we import a class.

93
00:06:01,760 --> 00:06:07,270
Next step is to of course create an object of this class and this object will be exactly the tool that

94
00:06:07,270 --> 00:06:12,950
will allow us to create this matrix of the features x1 X1 squared X1 at the power.

95
00:06:13,000 --> 00:06:16,440
And I will precise what n which was then we will choose several of them.

96
00:06:16,540 --> 00:06:17,200
But there you go.

97
00:06:17,200 --> 00:06:23,200
That's what we build with this polynomial features class and so well we're going to call this object

98
00:06:23,200 --> 00:06:29,950
of this class purely underscore rec as polynomial regressive but it's not exactly the aggressor itself

99
00:06:29,990 --> 00:06:36,190
because you know the final polynomial regressive will be the combination of this matrix of power features

100
00:06:36,310 --> 00:06:41,950
and the linear aggressor and that's why we will call it actually Lin rect too you will see so poorly

101
00:06:41,950 --> 00:06:48,520
rag will be created as an instance or an object of this polynomial features class so we're going to

102
00:06:48,520 --> 00:06:54,610
paste that here adding some parentheses and that is where we're going to choose the n you know that.

103
00:06:54,670 --> 00:06:57,940
And here is exactly chosen in this new class.

104
00:06:58,000 --> 00:06:59,830
So first we're gonna start with two.

105
00:07:00,050 --> 00:07:06,730
Okay we're gonna build a polynomial regression model resulting from the equation y equals B zero plus

106
00:07:06,720 --> 00:07:12,890
b 1 X1 plus b 2 X1 squared where X1 is of course the position levels and y is the salaries.

107
00:07:12,940 --> 00:07:13,200
All right.

108
00:07:13,230 --> 00:07:20,410
So let's create this let's precise degree which is the name of the parameter for that N equals two and

109
00:07:20,410 --> 00:07:21,820
then we'll try three and four.

110
00:07:21,820 --> 00:07:24,790
Okay then next step.

111
00:07:24,790 --> 00:07:31,930
Well the next step is now to proceed to this transformation of this simple matrix of features containing

112
00:07:31,990 --> 00:07:33,490
only x1.

113
00:07:33,490 --> 00:07:37,010
Meaning exactly this columns.

114
00:07:37,090 --> 00:07:40,600
So far this is our matrix of features of only one feature.

115
00:07:40,810 --> 00:07:47,200
And now we're going to transform this matrix of a single feature into this new matrix of features containing

116
00:07:47,320 --> 00:07:52,110
X1 as a first feature X1 squared is the second feature and then that's it.

117
00:07:52,120 --> 00:07:56,680
Because so far we start with an equal to but if we chose for example an equal three well the matrix

118
00:07:56,680 --> 00:08:00,270
of features would be X1 X1 squared and X1 and the power of three.

119
00:08:00,280 --> 00:08:00,890
All right.

120
00:08:01,000 --> 00:08:03,180
So that's what we're gonna do and do this.

121
00:08:03,490 --> 00:08:10,270
Well we're going to take of course are fully rag object again from which we're gonna call the method

122
00:08:10,450 --> 00:08:11,090
fit.

123
00:08:11,290 --> 00:08:17,710
Transform again you're starting to know this method that you know the method that usually proceed to

124
00:08:17,710 --> 00:08:23,860
a transformation and here the transformation is to turn this matrix of a single feature into this new

125
00:08:23,860 --> 00:08:27,670
matrix of features composed of X1 as the first feature and X1 squared.

126
00:08:27,670 --> 00:08:33,770
The second feature all right fit transform then some parenthesis and then according to you now what

127
00:08:33,780 --> 00:08:35,260
do we have to input here.

128
00:08:35,260 --> 00:08:36,380
Well that's pretty obvious.

129
00:08:36,730 --> 00:08:42,940
That's exactly the matrix of features we want to transform into this matrix of squared features let's

130
00:08:42,940 --> 00:08:44,030
say right.

131
00:08:44,050 --> 00:08:50,410
So X X of course that's what we want to transform X so far is only composed of this column.

132
00:08:50,410 --> 00:08:54,100
Okay so fit transform X and now we have our new matrix of features.

133
00:08:54,100 --> 00:09:00,040
However this exactly returns this new matrix of features and now we're going to create a new variable

134
00:09:00,280 --> 00:09:02,770
which will be this new matrix of features itself.

135
00:09:02,770 --> 00:09:10,840
And we're going to call it X underscore poli equals what is returned by this fit transform method applied

136
00:09:10,900 --> 00:09:12,490
to X grades great.

137
00:09:12,490 --> 00:09:18,670
And now we have exactly the matrix of features composed of the position levels and the squares of the

138
00:09:18,670 --> 00:09:20,130
position levels good.

139
00:09:20,350 --> 00:09:22,480
So now according to you what are we going to do.

140
00:09:22,900 --> 00:09:27,970
Well as I said at the beginning it's like now we have a new matrix of features you know composed of

141
00:09:27,970 --> 00:09:33,820
these two variables here and Well very simply we just have to build a linear regression model that will

142
00:09:34,120 --> 00:09:41,110
integrate these features into this equation y equals bizarro plus B1 X1 plus B2 X1 squared right you

143
00:09:41,110 --> 00:09:45,250
see the idea that's where the linear comes from and therefore well you know what to do.

144
00:09:45,280 --> 00:09:50,170
You know how to create such a linear regressive but we have to create a new one which is of course different

145
00:09:50,170 --> 00:09:56,140
than this one because this one is already trained on this matrix of single feature X and therefore having

146
00:09:56,170 --> 00:09:58,320
already learned coefficient.

147
00:09:58,420 --> 00:10:05,330
So now we need to create a new one which we're going to call of course Lynn rag and I'm adding underscore

148
00:10:05,480 --> 00:10:12,980
to which therefore will be a new object of this linear regression class and in some parenthesis.

149
00:10:13,310 --> 00:10:19,610
And well this object will be now trained on this new matrix of features composed of the position levels

150
00:10:19,640 --> 00:10:20,670
at different powers.

151
00:10:20,750 --> 00:10:25,490
And of course then the salary because indeed we need the dependent viable vector to train the linear

152
00:10:25,490 --> 00:10:27,740
regression model and an imaginary model.

153
00:10:27,740 --> 00:10:28,340
All right.

154
00:10:28,430 --> 00:10:35,570
So the next step and final step is to of course call this new linear regress or object that we've just

155
00:10:35,570 --> 00:10:40,940
created from which we're gonna call the fit method which will take as input.

156
00:10:40,940 --> 00:10:46,010
Of course this new matrix of features containing the position levels are different powers.

157
00:10:46,010 --> 00:10:50,150
And of course the same dependent variable vector which are the salaries.

158
00:10:50,150 --> 00:10:51,320
All right let's do this.

159
00:10:51,320 --> 00:10:55,450
Let's input first X Foley and then Y.

160
00:10:55,580 --> 00:10:56,640
And there you go.

161
00:10:56,690 --> 00:10:59,200
You have your polynomial regression model.

162
00:10:59,210 --> 00:11:00,460
Congratulations.

163
00:11:00,470 --> 00:11:05,360
Now you know how to build a nonlinear regression model and that's what we're gonna see clearly with

164
00:11:05,360 --> 00:11:10,520
the next steps when visualizing the linear regression results first and the polynomial regression results.

165
00:11:10,520 --> 00:11:14,010
Now you know how to build such a model and you have them in the toolkit.

166
00:11:14,090 --> 00:11:17,240
So you're getting more power in machine learning more knowledge more skills.

167
00:11:17,240 --> 00:11:18,400
Congratulations.

168
00:11:18,410 --> 00:11:19,250
That's amazing.

169
00:11:19,250 --> 00:11:25,100
And now I can't wait to move on to the next material to show you indeed the different results we get

170
00:11:25,220 --> 00:11:28,050
with linear regression and polynomial regression.

171
00:11:28,100 --> 00:11:29,480
So I'll see you in the next material.

172
00:11:29,480 --> 00:11:33,670
Let's quickly run this cell in order to get the model itself.

173
00:11:33,770 --> 00:11:38,410
And now whenever you're ready let's proceed to the next story or for the visualizations.

174
00:11:38,450 --> 00:11:40,250
And until then enjoy machine learning.
