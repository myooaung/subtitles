WEBVTT
1
00:00:00.330 --> 00:00:01.170
Okay my friends.

2
00:00:01.170 --> 00:00:07.470
So let's mass together this last final step visualizing the decision tree regression results in high

3
00:00:07.560 --> 00:00:08.970
resolution.

4
00:00:08.970 --> 00:00:13.720
All right so we're gonna do kind of the same exercise as what we did previously with the.

5
00:00:13.770 --> 00:00:21.750
As we are you know that funny exercise where we start from the polynomial regression code for the visualization

6
00:00:21.750 --> 00:00:25.120
of the results meaning this one.

7
00:00:25.200 --> 00:00:26.510
So we're going to start from this code.

8
00:00:26.520 --> 00:00:33.030
And then the exercise for you will be to make the right change or changes in order to make this work

9
00:00:33.030 --> 00:00:35.460
for the decision tree regression model.

10
00:00:35.520 --> 00:00:42.210
If we did this successfully for the SB our moral well it will be a piece of cake to do it for the decision

11
00:00:42.210 --> 00:00:47.250
tree regression model because indeed no features killing was needed and therefore we won't have to play

12
00:00:47.250 --> 00:00:52.710
with the transform method or the inverse transfer method and therefore you will just smash this.

13
00:00:52.890 --> 00:00:53.540
Okay.

14
00:00:53.550 --> 00:00:54.570
So let's do this.

15
00:00:54.570 --> 00:01:00.360
That's the exercise and let's take this whole code here you know which plots indeed the regression curve

16
00:01:00.390 --> 00:01:07.710
in high resolution but for the pulling the meal regression model let's base that right here in the new

17
00:01:08.040 --> 00:01:09.890
coat cell and now.

18
00:01:09.990 --> 00:01:10.680
Well there you go.

19
00:01:10.680 --> 00:01:16.890
Please press pause on the video and make the right change or right changes in order to make it work

20
00:01:16.890 --> 00:01:20.790
for Decision Tree regression model.

21
00:01:20.850 --> 00:01:21.980
Okay good.

22
00:01:21.990 --> 00:01:24.270
Now I'm going to implement a solution with you.

23
00:01:24.420 --> 00:01:26.810
And let's start with the obvious change.

24
00:01:26.820 --> 00:01:28.920
Let's replace polynomial here by.

25
00:01:29.070 --> 00:01:32.350
Well decision tree.

26
00:01:32.400 --> 00:01:33.040
Okay.

27
00:01:33.060 --> 00:01:36.990
And then what was the change or changes you had to make.

28
00:01:37.350 --> 00:01:40.590
Well as I told you here it's super easy.

29
00:01:40.590 --> 00:01:45.630
You just have to change two things and actually only one row which is row number four.

30
00:01:45.630 --> 00:01:51.300
Right because this row number three is totally fine because here X and Y are on the right scale you

31
00:01:51.330 --> 00:01:54.270
know because no feature skin was a place that's fine.

32
00:01:54.300 --> 00:01:54.870
And here.

33
00:01:54.930 --> 00:02:01.320
Well the obvious change that we have to make is of course to first replace this regressive by the right

34
00:02:01.320 --> 00:02:04.820
name of the regressive which is regressive.

35
00:02:04.860 --> 00:02:05.970
All right good.

36
00:02:05.970 --> 00:02:06.690
Regressive.

37
00:02:07.110 --> 00:02:13.710
And then here you know remember this poorly rag transformation object was used to transform the matrix

38
00:02:13.710 --> 00:02:17.310
of single feature into this matrix of the same feature at different powers.

39
00:02:17.340 --> 00:02:23.340
Part two three and four and this time here we absolutely don't need that because we're not doing polynomial

40
00:02:23.340 --> 00:02:30.360
regression so I'm removing this and then also a parenthesis here and now guess what.

41
00:02:30.360 --> 00:02:32.090
Well that's done right.

42
00:02:32.100 --> 00:02:38.010
That's the simple visualization code when you know you don't have to apply feature scaling and you don't

43
00:02:38.010 --> 00:02:43.650
have to transform your matrix of features into powered features as in polynomial regression.

44
00:02:44.150 --> 00:02:45.680
So that was as simple as that.

45
00:02:45.720 --> 00:02:46.350
I told you.

46
00:02:46.470 --> 00:02:46.830
Right.

47
00:02:47.040 --> 00:02:52.140
But now that was the good news and now I have some bad news you're going to see that the results are

48
00:02:52.140 --> 00:02:55.380
not going to be pretty because indeed let's do this now.

49
00:02:55.410 --> 00:02:57.120
Let's execute this cell.

50
00:02:57.120 --> 00:02:57.890
There you go.

51
00:02:57.930 --> 00:02:58.640
Playing.

52
00:02:58.710 --> 00:03:03.530
And here are the decision tree regression resolve.

53
00:03:03.720 --> 00:03:09.450
I really want to say this again the decision tree regression model is really not the best adapted to

54
00:03:09.660 --> 00:03:13.700
two dimensional datasets you know with only one feature and one deep variable.

55
00:03:13.770 --> 00:03:20.010
But once again I'd like to remind that the implementation we have here can be very easily adapted to

56
00:03:20.160 --> 00:03:24.140
any other data sets and I will show you what to do at the end of this tutorial.

57
00:03:24.150 --> 00:03:29.490
You know what the change in this code but I still wanted to show you the decision tree regression results

58
00:03:29.520 --> 00:03:30.450
in 2D.

59
00:03:30.450 --> 00:03:32.360
And so why isn't this pretty.

60
00:03:32.400 --> 00:03:38.700
Because you know what this decision tree regression model simply did was to take the real results.

61
00:03:38.700 --> 00:03:45.270
You know the real salary for each position level and then for all the position levels from position

62
00:03:45.270 --> 00:03:49.740
level minus open five and the position level plus open five.

63
00:03:49.780 --> 00:03:55.020
Well it predicted the salary to be the same as the position level in the middle.

64
00:03:55.020 --> 00:03:55.320
All right.

65
00:03:55.320 --> 00:03:56.460
So that's what it simply did.

66
00:03:56.460 --> 00:04:01.560
And that's you know understandable and intuitive you know to understand because you know how decision

67
00:04:01.560 --> 00:04:02.130
trees work.

68
00:04:02.130 --> 00:04:05.950
They work by splitting the data through successive note.

69
00:04:06.060 --> 00:04:10.680
And so at the end you know you get different ranges of the features where the prediction is the same.

70
00:04:11.370 --> 00:04:17.730
And so here all the predicted salaries in this range of position levels are just the same and that's

71
00:04:17.730 --> 00:04:22.530
why we have this stair looking like curve up to the last position level.

72
00:04:22.530 --> 00:04:25.110
And of course here this is not continuous.

73
00:04:25.110 --> 00:04:26.570
This is actually a vertical bar.

74
00:04:26.600 --> 00:04:29.000
So the regression curve here is not continuous.

75
00:04:29.070 --> 00:04:33.870
We jump from position level to the next one every step of one.

76
00:04:33.900 --> 00:04:34.290
Okay.

77
00:04:34.380 --> 00:04:37.400
So not pretty or not relevant at all into these.

78
00:04:37.680 --> 00:04:43.980
But I still recommend to try the decision tree regression model for higher dimensional data sets because

79
00:04:43.980 --> 00:04:47.490
indeed it can actually have great performance.

80
00:04:47.490 --> 00:04:47.960
Okay.

81
00:04:48.060 --> 00:04:49.490
So good to have said that.

82
00:04:49.530 --> 00:04:55.200
And now just a quick bonus I'm going to show you that in low resolution this makes absolutely no sense

83
00:04:55.260 --> 00:04:56.480
because you will see what happened.

84
00:04:56.490 --> 00:04:58.530
I'm just going to add a codicil here.

85
00:04:58.650 --> 00:05:02.880
I'm going to take every from here to here only.

86
00:05:03.040 --> 00:05:05.590
And then I will paste that here.

87
00:05:05.720 --> 00:05:12.660
Then I will just replace X with here by x and x rayed here by X also.

88
00:05:12.770 --> 00:05:17.270
I'm going to plot this and this is the curve in low resolution.

89
00:05:17.400 --> 00:05:22.820
And as you see this absolutely not makes any sense because now since we only made the predictions for

90
00:05:23.060 --> 00:05:26.020
the integers you know going from one to 10.

91
00:05:26.030 --> 00:05:30.710
Well of course since you understood that the predicted salary of the position level number six was just

92
00:05:30.980 --> 00:05:32.730
the real salary itself.

93
00:05:32.750 --> 00:05:32.990
Right.

94
00:05:32.990 --> 00:05:34.920
Because it was trained on the whole dataset.

95
00:05:34.940 --> 00:05:38.200
So here just absolutely doesn't make sense and therefore that's why.

96
00:05:38.390 --> 00:05:43.270
Clearly not wanted to include that in the implementation.

97
00:05:43.280 --> 00:05:43.760
All right.

98
00:05:43.760 --> 00:05:44.950
You understand now.

99
00:05:44.960 --> 00:05:51.530
So now I'm going to finish this tutorial and this section also by showing you what you would only need

100
00:05:51.530 --> 00:05:58.520
to replace when applying this decision tree regression model on your future data sets in higher dimensions.

101
00:05:58.520 --> 00:05:59.660
So let me show you.

102
00:05:59.660 --> 00:06:01.300
Let's take the cells one by one.

103
00:06:01.400 --> 00:06:04.370
Of course you will still need to import the libraries at the beginning.

104
00:06:04.370 --> 00:06:09.380
Then when importing the dataset you will of course need to just replace the name of the data set here

105
00:06:09.680 --> 00:06:12.590
and then maybe to include the first feature.

106
00:06:12.590 --> 00:06:16.790
You know it will probably be a numerical future you know with numerical values.

107
00:06:16.790 --> 00:06:23.570
So make sure to not forget to maybe replace this one by either zero or deleting the lower bound here

108
00:06:23.600 --> 00:06:27.290
so that you include all the columns except the last one.

109
00:06:27.290 --> 00:06:29.490
All right so going back to one.

110
00:06:29.510 --> 00:06:30.200
All right.

111
00:06:30.200 --> 00:06:31.640
And then here of course you keep this.

112
00:06:31.640 --> 00:06:36.880
But make sure that indeed the dependent variable is at the very last column of your dataset.

113
00:06:37.570 --> 00:06:37.890
Okay.

114
00:06:37.910 --> 00:06:43.400
And then you need to check in your data set if there are any missing values and once again if you have

115
00:06:43.400 --> 00:06:44.560
a huge dataset.

116
00:06:44.570 --> 00:06:50.540
Well I would just recommend to apply the missing values tool of your data processing tool kit to your

117
00:06:50.540 --> 00:06:53.610
whole dataset so that you don't have to check where they are.

118
00:06:53.660 --> 00:06:55.660
And even if there are not any missing data.

119
00:06:55.790 --> 00:06:59.360
Well the input will just leave your data set the same.

120
00:06:59.360 --> 00:07:04.910
Then I recommend to check if there is any categorical data in which case you would have to apply your

121
00:07:05.270 --> 00:07:11.420
either one hot encoder tool if you know there is no order relationship in your categorical variable

122
00:07:11.630 --> 00:07:17.780
or the label encoded to if there is indeed an order a relationship like for example the size of a close

123
00:07:17.780 --> 00:07:18.070
or.

124
00:07:18.410 --> 00:07:23.540
Well actually the position levels of a company if they are collected in strings so don't forget to check

125
00:07:23.540 --> 00:07:24.330
that out.

126
00:07:24.410 --> 00:07:30.040
And then once again you don't have to apply feature scaling because indeed the decision to regression

127
00:07:30.040 --> 00:07:35.330
more works with splits you know it splits your features in different successive ranges which is does

128
00:07:35.330 --> 00:07:38.540
a technique that absolutely doesn't need feature scaling right.

129
00:07:38.540 --> 00:07:43.820
It doesn't have to do with any coefficients or whatsoever it is just because you split your data set

130
00:07:43.820 --> 00:07:50.540
through different note in order to collect in different final ranges your final predictions for different

131
00:07:50.540 --> 00:07:52.410
ranges of values of the features.

132
00:07:52.610 --> 00:07:56.320
So you clearly don't need to apply feature scaling.

133
00:07:56.360 --> 00:07:56.880
All right.

134
00:07:56.900 --> 00:08:01.600
And then let's see what we have to do next in order to you know adapted to any data set.

135
00:08:01.610 --> 00:08:03.910
So actually that's the good news here.

136
00:08:03.920 --> 00:08:09.890
You don't have anything to change unless you split your data into a training set and it's asset in which

137
00:08:09.890 --> 00:08:16.220
case you would have to replace X and Y here by X train in y train then feel free to predict any new

138
00:08:16.220 --> 00:08:17.190
results you want.

139
00:08:17.330 --> 00:08:21.980
And if you have several features here well instead of putting the single value here you will have to

140
00:08:21.980 --> 00:08:27.560
put the values of other features like six point five and then if you have let's say the age you can

141
00:08:27.560 --> 00:08:32.600
put 30 and then if you have I don't know for example the number of years of experience you can put five

142
00:08:32.900 --> 00:08:37.760
you know that's how you would predict a single observation when you have several features.

143
00:08:37.790 --> 00:08:39.060
Okay good.

144
00:08:39.080 --> 00:08:44.260
And then final important thing to understand if you have a dataset with multiple features.

145
00:08:44.270 --> 00:08:51.980
Well this is absolutely not relevant because indeed this is only used to visualize the regression result

146
00:08:52.070 --> 00:08:57.400
in two dimensions and therefore if you have several features you will have a dimension higher than two.

147
00:08:57.500 --> 00:09:00.380
So you won't be able to plot anything here.

148
00:09:00.380 --> 00:09:00.740
OK.

149
00:09:00.890 --> 00:09:05.360
So you would just have to delete that cell and delete this text cell as well.

150
00:09:05.390 --> 00:09:10.760
And that's only what you would have to do in order to make this decision tree regression work for your

151
00:09:10.760 --> 00:09:14.250
future multi featured data set.

152
00:09:14.270 --> 00:09:15.300
All right great.

153
00:09:15.350 --> 00:09:16.490
So congratulations.

154
00:09:16.490 --> 00:09:22.760
Now you added this new machinery model in your toolkit and especially in your regression tool kit so

155
00:09:22.790 --> 00:09:23.930
congratulations.

156
00:09:23.930 --> 00:09:30.150
And now we will tackle together the final regression model of this part 2 which will be the random forest

157
00:09:30.170 --> 00:09:36.620
regression and which will smash in no time because you will see that it will be almost the same as the

158
00:09:36.620 --> 00:09:38.090
decision tree regression model.

159
00:09:38.420 --> 00:09:44.210
So either you'll join me in the artist or yours if you want to learn are or you'll join me in the next

160
00:09:44.210 --> 00:09:48.270
section for the next practical activity on random forest.

161
00:09:48.380 --> 00:09:50.300
And until then enjoy machine learning.
