1
00:00:00,640 --> 00:00:02,940
Hello and welcome back to the course on deep learning.

2
00:00:02,970 --> 00:00:09,120
Today we're talking about a clue which is rectified in your units and this is an additional step on

3
00:00:09,120 --> 00:00:12,180
top of our convolution step.

4
00:00:12,180 --> 00:00:16,140
So it's not a separate big step it's a small step in step one be basically.

5
00:00:16,290 --> 00:00:18,230
And what is going on here.

6
00:00:18,240 --> 00:00:24,090
Well we have our input image we have all convolutional there which we've discussed and then on top of

7
00:00:24,090 --> 00:00:26,080
that we're going to apply.

8
00:00:26,400 --> 00:00:27,190
Wait for it.

9
00:00:27,240 --> 00:00:36,720
Our favorite rectifier function and you're familiar with the rectifier function from the previous section

10
00:00:36,750 --> 00:00:47,400
on artificial neural networks and in our So sometimes authors or instructors separate the convolution

11
00:00:47,430 --> 00:00:54,030
and direct fire as two separate steps in our examples which is going to consider them the just one big

12
00:00:54,030 --> 00:00:57,060
step for second evolution then the rectifier.

13
00:00:57,270 --> 00:01:03,750
And the reason why we're applying the rectifier is because you want to increase non-linearity in our

14
00:01:03,780 --> 00:01:13,410
image or in our network and our commercial neural network and our fire acts as that filter or access

15
00:01:13,440 --> 00:01:19,200
that function which breaks up and you arity and the reason why we want to increase nonlinearity in our

16
00:01:19,440 --> 00:01:25,950
network is because images themselves are highly non-linear especially if you're recognizing different

17
00:01:26,010 --> 00:01:32,790
objects next to each other or just on background and stuff like that like the image is going to have

18
00:01:33,180 --> 00:01:38,050
lots of nonlinear elements and the transition between pixels adjacent pixels is often would be nonlinear.

19
00:01:38,040 --> 00:01:41,280
That's you know because its borders is different colors is different.

20
00:01:41,460 --> 00:01:46,020
There's different elements in your images and but at the same time when we're applying a mathematical

21
00:01:46,020 --> 00:01:52,410
operation such as convolution you know and running this feature detection to create our feature maps

22
00:01:52,710 --> 00:01:59,710
we risk that we might create something linear and therefore we need to break up the narrative.

23
00:01:59,970 --> 00:02:05,960
So let's have a look at an example here is a image and original image.

24
00:02:05,970 --> 00:02:13,220
Now when we apply a feature detection detector to this image we get something like this.

25
00:02:13,290 --> 00:02:16,230
So you can see here that black is negative white is positive value as well.

26
00:02:16,230 --> 00:02:22,980
When you apply a feature detector to a like a proper image which has not just zeros and ones but has

27
00:02:22,980 --> 00:02:28,890
lots of different values and you apply as we saw previously you Texas can have negative values in themselves

28
00:02:29,070 --> 00:02:30,920
sometimes you'll get negative values.

29
00:02:31,080 --> 00:02:34,670
And here are the black ones are negative white ones are positive.

30
00:02:34,800 --> 00:02:45,240
And what a rectified linear unit function does is it removes all the black rights in anything below

31
00:02:45,240 --> 00:02:46,490
zero it turns into zero.

32
00:02:46,560 --> 00:02:49,160
And so from this it turns into this right.

33
00:02:49,290 --> 00:02:57,990
And so it's it's pretty hard to see what exactly is the benefit in terms of in terms of breaking up

34
00:02:58,050 --> 00:02:58,910
linearity.

35
00:02:59,400 --> 00:03:01,050
I'll try to explain.

36
00:03:01,080 --> 00:03:07,740
I'll try to show an example on this image but at the end of the day it's this is a very mathematical

37
00:03:07,740 --> 00:03:12,480
concept and would have to go into a lot of math to really explain what is going on.

38
00:03:12,480 --> 00:03:13,800
But let's let's try let's have a look.

39
00:03:13,800 --> 00:03:16,800
So for instance let's look at this.

40
00:03:16,860 --> 00:03:17,580
This building here.

41
00:03:17,580 --> 00:03:18,070
Right.

42
00:03:18,090 --> 00:03:20,150
So this is a building on its own.

43
00:03:20,730 --> 00:03:22,400
Then you can see this shadow.

44
00:03:22,440 --> 00:03:29,010
This black part this shadow over here well you see that it's white the reflection of the light and then

45
00:03:29,010 --> 00:03:33,160
it's a gray and then it gets darker and then it gets darker again.

46
00:03:33,240 --> 00:03:35,860
So and when we take it out we take out that black spot.

47
00:03:35,860 --> 00:03:38,220
So think of it in terms of linearity right.

48
00:03:38,250 --> 00:03:43,700
So the it looks like when you go from white to gray the next step would be black.

49
00:03:43,740 --> 00:03:50,970
Right the next up would be black it's it's a linear progression from bright to dark and therefore this

50
00:03:50,970 --> 00:03:53,490
is kind of like a linear situation.

51
00:03:53,490 --> 00:03:56,560
When you take out the black you break up the linearity.

52
00:03:56,670 --> 00:03:57,800
Let's try another one.

53
00:03:58,050 --> 00:03:59,110
Let's have a look here.

54
00:03:59,220 --> 00:04:01,980
And at the same time it's still that same building right.

55
00:04:01,980 --> 00:04:08,320
It's not it's not like you or your like it's not like you're blending two buildings into each other

56
00:04:08,550 --> 00:04:09,810
but that is secondary.

57
00:04:09,810 --> 00:04:11,940
The main point is breaking up the linearity.

58
00:04:12,210 --> 00:04:13,590
So let's have a look here same thing.

59
00:04:13,590 --> 00:04:19,480
So you see white gray black gray white.

60
00:04:19,590 --> 00:04:22,520
And when you break it up you don't have that anymore right.

61
00:04:22,530 --> 00:04:29,600
You don't have that progression the gradual progression that you just have like an abrupt change.

62
00:04:29,730 --> 00:04:33,540
And that helps introduce non-linearity into your image.

63
00:04:33,540 --> 00:04:42,540
So it's a very rough explanation very kind of like on or on the fingers explanation rather than technical

64
00:04:42,690 --> 00:04:47,370
but hopefully it kind of helps you understand a bit better what we're talking about here.

65
00:04:47,370 --> 00:04:54,870
So here again you can see white gray is a better example even to bright darker darker darker and darker

66
00:04:54,980 --> 00:04:55,680
darker.

67
00:04:55,680 --> 00:05:04,460
So this part looks like it's thinner than you break it up like that again so this is a very rough explanation.

68
00:05:04,480 --> 00:05:08,570
It's not absolutely perfect but at least it gives you some idea of what's going on.

69
00:05:08,800 --> 00:05:14,120
But if you'd like to learn more there's a good paper as always there's always a paper.

70
00:05:14,200 --> 00:05:20,370
This one is by CCJ corps from the University of California and it's called Understanding convolutional

71
00:05:20,380 --> 00:05:22,980
neural networks who have a mathematical model.

72
00:05:23,200 --> 00:05:28,840
And basically they're He answers to questions and you need to just look at the first one.

73
00:05:28,840 --> 00:05:33,820
And the question is why is not a nonlinear activation function is essential at the filter output of

74
00:05:33,820 --> 00:05:36,130
all intermediate layers.

75
00:05:36,220 --> 00:05:44,280
So that kind of explains it in a bit more detail both in terms of intuition and mostly in terms of mathematics.

76
00:05:44,320 --> 00:05:47,970
So that's an interesting paper where you can get some more additional information on this topic.

77
00:05:48,100 --> 00:05:53,360
And if you really want to dig in and explore some some cool stuff here.

78
00:05:53,360 --> 00:05:55,690
Then there's another paper that you might be interested in.

79
00:05:55,690 --> 00:06:01,720
It's called delving deeper into rectifier surpassing human level level performance on image and that

80
00:06:01,720 --> 00:06:02,830
classification.

81
00:06:02,920 --> 00:06:09,190
And here the author is combing hair and others from Microsoft Research.

82
00:06:09,400 --> 00:06:17,630
They propose a different type of rectified leaner unit function.

83
00:06:17,650 --> 00:06:21,830
They propose the parametric rectified function which you see here on the right.

84
00:06:22,030 --> 00:06:26,740
And they argue that it delivers better results without sacrificing performance.

85
00:06:26,740 --> 00:06:30,200
So interesting read if you'd like to get a bit more into this topic.

86
00:06:30,490 --> 00:06:32,020
And that's all for today.

87
00:06:32,280 --> 00:06:38,380
Really you layer is pretty simple so for adjusting just applying the rectifier function and I look forward

88
00:06:38,380 --> 00:06:39,230
to seeing you next time.

89
00:06:39,250 --> 00:06:40,770
Until then enjoy learning.
