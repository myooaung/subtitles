WEBVTT
1
00:00:01.090 --> 00:00:08.170
Welcome to Section thirty four principal component analysis or PCA before you get some hands on experience

2
00:00:08.170 --> 00:00:15.010
coming up with PCa in the practical exercises we're going to take a look at the intuition behind it.

3
00:00:15.040 --> 00:00:23.140
PCa is considered to be one of the most used unsupervised algorithms and can be seen as the most popular

4
00:00:23.200 --> 00:00:26.040
dimensionality reduction algorithm.

5
00:00:26.830 --> 00:00:34.270
PCa is used for operations such as visualization feature extraction noise filtering and can be seen

6
00:00:34.270 --> 00:00:38.470
in algorithms used for stock market predictions and gene analysis.

7
00:00:38.470 --> 00:00:40.030
Just to name a few.

8
00:00:43.000 --> 00:00:48.380
The goal of PCa is to identify and detect the correlation between variables.

9
00:00:48.460 --> 00:00:53.470
If there is a strong correlation and it's found then you could reduce the dimensionality which really

10
00:00:53.470 --> 00:00:55.170
what PCA is intended for.

11
00:00:55.330 --> 00:01:00.940
You find the directions of maximum variance in high dimensional data and then you project it into a

12
00:01:00.940 --> 00:01:02.570
smaller dimensional subspace.

13
00:01:02.680 --> 00:01:08.470
While retaining most of the information usually again with PCa The goal to reduce the dimensions of

14
00:01:08.470 --> 00:01:17.650
a D dimensional dataset by projecting onto a k dimensional subspace where k is less than D and for a

15
00:01:17.830 --> 00:01:21.940
overall breakdown and wrap up of the PCA we can see here.

16
00:01:22.620 --> 00:01:30.620
That the main functions of the PCA algorithm would be to standardize the data obtain the eigenvectors

17
00:01:30.650 --> 00:01:36.410
and eigenvalues to then sort the eigenvalues in descending order.

18
00:01:37.700 --> 00:01:45.840
Construct the projection matrix W from the selected K eigenvectors and to transform the original dataset.

19
00:01:46.220 --> 00:01:47.380
And you can explore it further.

20
00:01:47.390 --> 00:01:48.620
If you follow that link.

21
00:01:48.860 --> 00:01:55.670
But one thing I want to examine here with PCa and I think the visualization is really helpful if we

22
00:01:55.670 --> 00:01:56.630
visit the following link.

23
00:01:56.630 --> 00:02:02.120
It's going to take us to this page where we can actually view it in 2D and three examples.

24
00:02:02.120 --> 00:02:07.400
Now with PCa in a Tuti you can start to see the relationship in how PCA is playing out amongst the variables

25
00:02:07.400 --> 00:02:08.590
in the data.

26
00:02:09.140 --> 00:02:15.840
You can also on this site drag them around drag the data point around to see the PCA.

27
00:02:15.890 --> 00:02:18.550
The coordinates are just within the system.

28
00:02:18.710 --> 00:02:23.950
But really what I think is helpful is the three-D example with the 3D example.

29
00:02:23.990 --> 00:02:30.440
You can actually see the relationship the data within this model and comparing it to the two D within

30
00:02:30.440 --> 00:02:31.820
the higher dimensional space.

31
00:02:31.820 --> 00:02:35.790
Obviously it can be a much easier visualization.

32
00:02:35.900 --> 00:02:41.960
I think this is helpful really to kind of grasp what PCA is doing and if we drag the data points around

33
00:02:41.960 --> 00:02:52.180
again just for a test we can click on the show PCA to reset that and will show it and we can see the

34
00:02:52.180 --> 00:02:52.710
PCA.

35
00:02:52.710 --> 00:02:57.270
Here you could actually move the model around since it's not on that to d plot.

36
00:02:57.280 --> 00:02:59.300
We can visualize it within three days.

37
00:02:59.560 --> 00:02:59.790
All right.

38
00:02:59.800 --> 00:03:02.760
So to wrap PCA up short and sweet.

39
00:03:03.040 --> 00:03:08.890
PCa is not like linear regression although it may look like it because rather than attempting to predict

40
00:03:08.980 --> 00:03:16.930
the values PCA is attempting to learn about the relationship between x and y values quantified by finding

41
00:03:16.930 --> 00:03:19.200
a list of principal axes.

42
00:03:19.900 --> 00:03:22.350
And I think one of the best ways is to look at the visualizations.

43
00:03:22.370 --> 00:03:24.800
You don't compare it to 2D and 3D dimension.

44
00:03:24.890 --> 00:03:28.570
The analysis and the visualizations that we looked at before.

45
00:03:28.610 --> 00:03:32.280
In addition you know on a side note PCA does have a weakness.

46
00:03:32.300 --> 00:03:39.800
It is highly affected by outliers in the data but PCA is considered to be one of the most used and it's

47
00:03:39.800 --> 00:03:41.300
extremely popular to use.

48
00:03:41.300 --> 00:03:45.620
And I think once you start working through the practical it will make more sense if you have any questions

49
00:03:45.680 --> 00:03:48.240
please let us know and enjoy machine learning.
