1
00:00:00,330 --> 00:00:02,500
Hello and welcome to the art of Tauriel.

2
00:00:02,700 --> 00:00:08,250
So we just trained our artificial neural network on the training set and now time to make the predictions

3
00:00:08,420 --> 00:00:09,520
on the test set.

4
00:00:09,750 --> 00:00:13,370
So lucky for us we already have everything ready here.

5
00:00:13,380 --> 00:00:17,800
Thanks to our classification templates that we pasted in the first tutorial.

6
00:00:17,820 --> 00:00:21,480
So actually this section predicts the tests that result.

7
00:00:21,690 --> 00:00:27,990
And this section makes the confusion matrix thanks to which we will obtain the accuracy on the test

8
00:00:27,990 --> 00:00:28,500
set.

9
00:00:28,590 --> 00:00:35,180
That is some accuracy on new observations on which artificial neural network model wasn't trained.

10
00:00:35,190 --> 00:00:40,110
So first let's take care of this section and let's see what we need to change.

11
00:00:40,110 --> 00:00:46,440
So first of all this first line gets the predicted probabilities thanks to this predict function but

12
00:00:46,530 --> 00:00:50,850
that's was the product function used for built in our packages.

13
00:00:51,090 --> 00:00:56,160
But here since we are using the H2 package that is kind of special There are actually some things that

14
00:00:56,160 --> 00:00:58,900
we need to change here but a very few things.

15
00:00:58,920 --> 00:01:04,500
So first ask for all the functions we used with this age to a package.

16
00:01:04,620 --> 00:01:10,170
Will you notice that when we use a function we first take the H2 package then a dot and then the name

17
00:01:10,170 --> 00:01:10,950
of the function.

18
00:01:11,130 --> 00:01:13,920
Well we need to do the same here for the predictive function.

19
00:01:13,920 --> 00:01:19,400
So here we just need to add h to o dot products.

20
00:01:19,620 --> 00:01:19,870
OK.

21
00:01:19,870 --> 00:01:21,640
So that's the first thing we need to change.

22
00:01:21,840 --> 00:01:24,290
And then let's see let's go inside the function.

23
00:01:24,300 --> 00:01:26,820
So the first argument is classifier.

24
00:01:26,850 --> 00:01:35,270
Let's press here and one to get some information about the Predict function of the H2 model.

25
00:01:35,310 --> 00:01:39,180
So let's go down to have a look at the arguments and let's see what they are.

26
00:01:39,630 --> 00:01:45,480
So as we can see we have only two main arguments and then some additional arguments but that we will

27
00:01:45,480 --> 00:01:46,860
not focus on.

28
00:01:46,950 --> 00:01:52,370
Instead we will focus on the two main arguments here which are the object and new data.

29
00:01:52,620 --> 00:01:59,680
So the first thing we can see is that there is no type argument so here simply we will remove this type

30
00:01:59,680 --> 00:02:05,090
because response argument and input because we actually don't need it.

31
00:02:05,170 --> 00:02:05,570
All right.

32
00:02:05,580 --> 00:02:09,640
And now we are left with the two arguments we are required to input.

33
00:02:09,690 --> 00:02:15,780
That is the object which is our classifier here that is the ancient model itself that we have just built

34
00:02:15,870 --> 00:02:21,540
on the training set and then the second argument new data and this new data argument is expecting of

35
00:02:21,540 --> 00:02:25,970
course the observations of which it has to make the predictions.

36
00:02:25,970 --> 00:02:27,920
All right so that's exactly our test set.

37
00:02:28,020 --> 00:02:31,380
And here we remove the dependent variable called him.

38
00:02:31,410 --> 00:02:33,580
Thanks to this minus three here.

39
00:02:33,720 --> 00:02:39,420
But we need to replace this three because this number three here corresponds to the index of the dependent

40
00:02:39,420 --> 00:02:43,960
variable of the dataset that we worked with in part 3 classification.

41
00:02:44,070 --> 00:02:49,200
And of course the index of our dependent variable is not 3 but is 11.

42
00:02:49,200 --> 00:02:55,710
Remember we already replaced the index three here in this feature in part so we replaced the 4 indexes

43
00:02:55,710 --> 00:02:58,590
three that were here by 11.

44
00:02:58,590 --> 00:03:00,240
And so here we need to do the same.

45
00:03:00,270 --> 00:03:05,340
We will replace this three index here by index 11.

46
00:03:05,340 --> 00:03:12,420
All right so now this is taking the tested observations as new data that it will predict the probabilities

47
00:03:12,660 --> 00:03:18,510
that the dependent viable exited equals one for the observations in the test and therefore it will predict

48
00:03:18,510 --> 00:03:23,660
for each customer in the test set the probability that this customer leaves the bank.

49
00:03:23,700 --> 00:03:29,530
And since we have the real results of whether the customers of the test set left or stayed in the bank

50
00:03:29,940 --> 00:03:33,480
Well we will compare our predictions to these real results.

51
00:03:33,480 --> 00:03:38,850
These actual results and that's how we'll get the accuracy by computing the number of correct predictions

52
00:03:39,090 --> 00:03:43,630
divided by the total number of observations in the test set that is 2000.

53
00:03:43,980 --> 00:03:49,110
And then if we get a good accuracy then maybe we'll get a good and powerful model and if that's the

54
00:03:49,110 --> 00:03:52,830
case we will give it to the bank on the plate and tell the bank.

55
00:03:53,010 --> 00:03:58,740
OK now you can rent all your customers all the customers in the bank by their probability to leave the

56
00:03:58,740 --> 00:03:59,290
bank.

57
00:03:59,430 --> 00:04:01,180
That is for each of your customers.

58
00:04:01,350 --> 00:04:06,500
You can predict with a good accuracy and we'll be able to tell them precisely what the accuracy is.

59
00:04:06,690 --> 00:04:11,610
You'll be able to predict with a good accuracy the probability that the customer leaves the bank and

60
00:04:11,610 --> 00:04:17,520
then you can add therefore I can give you a ranking of all your customers ranked by their probability

61
00:04:17,520 --> 00:04:22,920
to leave the bank from the highest probability to the lowest probability and therefore you can do some

62
00:04:22,920 --> 00:04:28,290
customer segmentation and consider for example the top 10 percent probability that the customers leave

63
00:04:28,290 --> 00:04:29,220
the bank.

64
00:04:29,220 --> 00:04:35,700
And in this segment you can analyze deeper the factors that lead the customers to leave the bank by

65
00:04:35,700 --> 00:04:41,610
using some data mining techniques like for example doing her chi squared test or applying to that summary

66
00:04:41,610 --> 00:04:46,680
function on your independent variables to understand which independent variables have the most impact

67
00:04:46,920 --> 00:04:53,450
on the dependent variable that is which independent variable explains the most why customers are leaving.

68
00:04:53,610 --> 00:04:55,080
Well you know how to do that.

69
00:04:55,200 --> 00:05:01,110
That's exactly what we did in part two and three when we used the summary function to get values and

70
00:05:01,200 --> 00:05:06,960
statistical significance levels to see which independent variables are the most autistic the significant

71
00:05:07,200 --> 00:05:09,750
and therefore explain the best the dependent variable.

72
00:05:09,750 --> 00:05:11,630
That is why customers are leaving.

73
00:05:11,670 --> 00:05:15,460
So that's the purpose behind making these predictions on the test set.

74
00:05:15,480 --> 00:05:20,640
It's just to get the accuracy on your observations to validate the model so that we can give this model

75
00:05:20,640 --> 00:05:21,470
to the bank.

76
00:05:21,720 --> 00:05:24,270
All right so now let's make the predictions.

77
00:05:24,270 --> 00:05:27,330
So we are almost done here.

78
00:05:27,360 --> 00:05:34,060
We just need to add one more thing which is again related to the fact that we are using the H-2 package.

79
00:05:34,230 --> 00:05:41,190
And as you can see in this new data arguments well this new data is of course the test set but this

80
00:05:41,190 --> 00:05:44,400
test set is expected to be an H2 frame.

81
00:05:44,460 --> 00:05:51,420
Right now it is a standard data frame but our H2 o product function is expecting an age to overwrite.

82
00:05:51,630 --> 00:05:56,070
So how can we convert this data frame into a page to a friend.

83
00:05:56,220 --> 00:06:04,170
Well by doing exactly the same as what we did to convert this training said data frame into this age

84
00:06:04,170 --> 00:06:12,570
to frame that is by applying on the test set as that age to function.

85
00:06:12,570 --> 00:06:16,580
All right so I'm putting the test that in the function like that.

86
00:06:16,830 --> 00:06:17,580
And here we go.

87
00:06:17,580 --> 00:06:24,660
Now I think everything is ready we are ready to make the predictions which so far will be the prediction

88
00:06:24,660 --> 00:06:29,790
of the probabilities that the Classic was one that is the probabilities that the customers leave the

89
00:06:29,790 --> 00:06:30,330
bank.

90
00:06:30,540 --> 00:06:37,000
So let's select this and get the predicted probabilities.

91
00:06:37,220 --> 00:06:38,350
And here we go.

92
00:06:38,450 --> 00:06:45,870
We now have the problem Fred vector containing all the predicted probabilities in the form of an environment.

93
00:06:46,160 --> 00:06:50,160
So that's good but we cannot have a look at these predicted probabilities yet.

94
00:06:50,180 --> 00:06:53,840
We will need to convert it back into a standard vector.

95
00:06:53,840 --> 00:06:56,550
But before we do that convert it into a vector.

96
00:06:56,810 --> 00:07:05,210
Well we need to apply this line as well which will you know transform the probabilities into the predictions

97
00:07:05,300 --> 00:07:07,060
in the form one or zero.

98
00:07:07,190 --> 00:07:11,020
That is exactly the predictions of the dependent variable exited.

99
00:07:11,300 --> 00:07:13,880
And to do this we're using this if else function.

100
00:07:13,880 --> 00:07:20,270
And basically what we do is we choose a threshold such that if the predicted probability is above the

101
00:07:20,270 --> 00:07:22,600
threshold then we predict one.

102
00:07:22,670 --> 00:07:27,740
And if the predicted probability is below the threshold then we predict zero.

103
00:07:28,010 --> 00:07:33,500
So that's a natural threshold to take when we get our predictions in terms of probabilities know that

104
00:07:33,590 --> 00:07:37,110
it is not necessarily always 50 percent 0.5.

105
00:07:37,190 --> 00:07:37,820
That's the case.

106
00:07:37,820 --> 00:07:43,160
For example in medicine when we have to predict some sensitive information like for example predicting

107
00:07:43,160 --> 00:07:44,570
if a tumor is malignant.

108
00:07:44,660 --> 00:07:46,130
Well that's more sensitive.

109
00:07:46,160 --> 00:07:51,140
So in that case we'd better be sure of our predictions and therefore we would choose a higher threshold

110
00:07:51,470 --> 00:07:53,120
like for example 80 percent.

111
00:07:53,390 --> 00:07:55,960
But here we are predicting if a customer leaves the bank.

112
00:07:56,000 --> 00:07:58,380
So we are fine with the 50 percent threshold.

113
00:07:58,640 --> 00:07:59,680
So that's OK.

114
00:07:59,690 --> 00:08:06,830
And by the way there is a more simple way to get these predictions in the forms you're a one without

115
00:08:06,890 --> 00:08:16,130
using if else function it's by simply removing this one in zero here and removing this if else and by

116
00:08:16,130 --> 00:08:23,850
using this spread larger than 0.5 because this will return a boolean which will be true if property

117
00:08:23,910 --> 00:08:31,610
is larger than 0.5 and false if Prop red is below 0.5 and why spread in the form of these billions.

118
00:08:31,610 --> 00:08:35,330
True and False will be accepted in this confusion matrix here.

119
00:08:35,390 --> 00:08:36,600
So that's more simple.

120
00:08:36,650 --> 00:08:41,380
And now let's get this predictions in the form of billions.

121
00:08:41,390 --> 00:08:45,160
All right so I'm going to select this line and execute.

122
00:08:45,200 --> 00:08:45,590
All right.

123
00:08:45,590 --> 00:08:52,100
Now we have our white print in the form of billions but it is still a nature to object because it is

124
00:08:52,100 --> 00:08:56,420
the result in the first place of this age to predict function.

125
00:08:56,420 --> 00:09:03,020
So it is still an age to object and therefore Now what we have to do is to convert this age to object

126
00:09:03,260 --> 00:09:09,360
back into a vector because the stable function here will only accept a vector a standard vector.

127
00:09:09,680 --> 00:09:12,980
And of course we'll never accept this age to object.

128
00:09:13,190 --> 00:09:16,880
So let's convert it back into a vector and that's actually really simple.

129
00:09:16,880 --> 00:09:21,850
It's actually kind of the same as converting a data frame into a nature to frame.

130
00:09:21,860 --> 00:09:25,350
But instead of using age to here we will use vector.

131
00:09:25,550 --> 00:09:37,090
So here we simply have to type Y pred equals as Dot vector and the parenthesis of course whitebread.

132
00:09:37,230 --> 00:09:41,970
All right so let's check it out and we're going to select this line and Exiguus.

133
00:09:42,180 --> 00:09:48,360
And now as you can see why press became this vector of integers containing 2000 elements.

134
00:09:48,570 --> 00:09:51,840
And that's the standard vector are we were used to working with.

135
00:09:52,260 --> 00:09:58,950
So we can actually have a look at the predictions of the test observations by typing here in the console

136
00:09:59,140 --> 00:10:00,540
y present.

137
00:10:00,570 --> 00:10:01,140
Here we go.

138
00:10:01,140 --> 00:10:05,810
That's all the predictions of the tested observations 2000 predictions.

139
00:10:05,820 --> 00:10:09,890
So here we go according to the model the first customer stayed in the bank.

140
00:10:09,930 --> 00:10:11,820
The second customer stayed in the bank.

141
00:10:11,850 --> 00:10:14,690
The third customer left the bank.

142
00:10:14,700 --> 00:10:17,920
The fourth one stayed the fifth stayed etc..

143
00:10:18,030 --> 00:10:22,950
So if you win you can actually compare these predictions with the real results that are in the last

144
00:10:22,950 --> 00:10:23,710
column.

145
00:10:23,880 --> 00:10:26,020
To set this column here.

146
00:10:26,190 --> 00:10:33,180
So for example 0 0 1 0 0 0 are the real outcomes of the first first customers.

147
00:10:33,180 --> 00:10:39,420
And if we compare that with the predictions Well we see that the predictions are quite correct because

148
00:10:39,420 --> 00:10:43,060
here we get as well 0 0 1 0 0 0.

149
00:10:43,170 --> 00:10:45,580
So the Pfeiffer's predictions are correct.

150
00:10:45,750 --> 00:10:50,040
So that smells pretty good for our accuracy that we are about to compute because when we look at the

151
00:10:50,040 --> 00:10:53,170
first observations we can only see correct predictions.

152
00:10:53,190 --> 00:10:55,580
So now actually I can't wait to see the accuracy.

153
00:10:55,680 --> 00:10:57,290
So let's computed right now.

154
00:10:57,450 --> 00:11:03,270
We will start by making the comparison metrics and of course here we need to replace this index 3 here

155
00:11:03,270 --> 00:11:06,990
by 11 because this corresponds to the index of the dependent variable.

156
00:11:07,200 --> 00:11:11,360
And so now we are ready to make this comparison matrix.

157
00:11:11,370 --> 00:11:14,750
So I'm going to select this line and execute.

158
00:11:14,790 --> 00:11:17,110
Here we go confusion matrix created.

159
00:11:17,250 --> 00:11:18,680
So now let's have a look.

160
00:11:18,750 --> 00:11:22,970
I'm going to type CME here in the console and press enter.

161
00:11:23,280 --> 00:11:25,290
That's our confusion matrix.

162
00:11:25,290 --> 00:11:27,210
We can see a lot of correct predictions.

163
00:11:27,210 --> 00:11:28,170
That's good.

164
00:11:28,170 --> 00:11:36,510
1536 correct predictions of customers who stayed in the bank and one hundred and ninety five correct

165
00:11:36,510 --> 00:11:41,940
predictions of customers who left the bank and then we have two hundred and twelve plus fifty seven

166
00:11:42,000 --> 00:11:46,940
incorrect actions of customers who either left or stayed in the back.

167
00:11:47,210 --> 00:11:48,660
So this looks pretty good.

168
00:11:48,750 --> 00:11:50,530
And now let's not wait anymore.

169
00:11:50,670 --> 00:11:52,190
Let's compute the accuracy.

170
00:11:52,380 --> 00:11:55,830
So the accuracy is the total number of correct predictions.

171
00:11:55,830 --> 00:12:03,420
That is one hundred five thousand thirty six plus one hundred ninety five divided by the total number

172
00:12:03,420 --> 00:12:05,250
of observations in the test set.

173
00:12:05,400 --> 00:12:10,830
That is the total number of predictions actually which is two thousand.

174
00:12:10,840 --> 00:12:11,390
All right.

175
00:12:11,390 --> 00:12:13,050
So let's check it out.

176
00:12:13,080 --> 00:12:15,970
Let's see if we can offer this model to the bank.

177
00:12:16,110 --> 00:12:17,800
Let's see if we'll get the bonus.

178
00:12:18,000 --> 00:12:25,810
Let's find out about its accuracy on 3 3 2 1 GO eighty six point five percent.

179
00:12:25,980 --> 00:12:27,620
That's actually not bad at all.

180
00:12:27,690 --> 00:12:35,060
Eighty six point five percent well let's say 87 87 percent means that on one hundred observations 87

181
00:12:35,070 --> 00:12:36,690
predictions should be correct.

182
00:12:36,690 --> 00:12:37,980
So this is pretty good.

183
00:12:37,980 --> 00:12:43,110
And besides we haven't done any parameter chilling and you will see that by doing some parameter tuning

184
00:12:43,470 --> 00:12:46,150
using some techniques like k fold cross-validation.

185
00:12:46,200 --> 00:12:49,080
Well we can get an even better accuracy score.

186
00:12:49,080 --> 00:12:49,860
No worries.

187
00:12:49,860 --> 00:12:55,750
We will do that in parts and you can actually already practice to improve this accuracy score.

188
00:12:55,780 --> 00:12:57,940
And please let me know if you get an awesome one.

189
00:12:58,230 --> 00:13:03,960
And now just one last thing since we were connected to this H-2 instance it's better to disconnect from

190
00:13:03,960 --> 00:13:14,190
it now and to do this we only need to apply a last function of age 2 which is the age to oh Dot shut

191
00:13:14,670 --> 00:13:15,030
down.

192
00:13:15,030 --> 00:13:17,580
Here it is with no arguments inside.

193
00:13:17,610 --> 00:13:22,000
You just need to select this and this will disconnect you from the server.

194
00:13:22,060 --> 00:13:23,640
So let's executes.

195
00:13:23,640 --> 00:13:28,950
Are you sure you want to shut down the H-2 instance running at this address then you just need to type

196
00:13:28,950 --> 00:13:29,660
here.

197
00:13:29,660 --> 00:13:30,440
Keep it or why.

198
00:13:30,480 --> 00:13:31,910
And then answer.

199
00:13:32,160 --> 00:13:33,890
And now we are disconnected.

200
00:13:33,990 --> 00:13:36,630
True means yes we did disconnect.

201
00:13:36,630 --> 00:13:37,980
So congratulations.

202
00:13:37,980 --> 00:13:43,530
You have built your first artificial neural network on our using the H2 package.

203
00:13:43,530 --> 00:13:48,390
I was very happy to build this first deep learning model with you and we're getting to the end of this

204
00:13:48,390 --> 00:13:52,430
section next section will be about convolutional neural networks.

205
00:13:52,620 --> 00:13:57,480
Another branch of machine learning specialized for computer vision because it will consider spatial

206
00:13:57,480 --> 00:14:03,500
structure and the data exactly as it is the case for images where the position of the pixels matters.

207
00:14:03,690 --> 00:14:05,560
So we will see that in the next section.

208
00:14:05,640 --> 00:14:07,400
And until then enjoy machine learning
