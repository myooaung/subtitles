WEBVTT
1
00:00:00.300 --> 00:00:01.140
Hello, my friends.

2
00:00:01.260 --> 00:00:07.530
Welcome to part two of this implementation, where we're going to build together the artificial neural

3
00:00:07.620 --> 00:00:08.310
network.

4
00:00:08.460 --> 00:00:09.990
I'm super excited to start.

5
00:00:10.290 --> 00:00:12.060
We will do it in four steps.

6
00:00:12.410 --> 00:00:16.290
And the first step will initialize the Hénin as a sequence of layers.

7
00:00:16.800 --> 00:00:22.560
Then we will add the input layer and the first hidden layer composed of a certain number of neurons

8
00:00:22.740 --> 00:00:23.910
which will choose together.

9
00:00:24.330 --> 00:00:29.520
Then we'll add the second hidden layer, you know, in order to build indeed a deep learning model as

10
00:00:29.520 --> 00:00:31.830
opposed to a shallow learning model.

11
00:00:32.190 --> 00:00:37.350
And then finally, we will add the output layer, which will contain what we want to predict.

12
00:00:37.700 --> 00:00:39.000
All right, let's do this.

13
00:00:39.120 --> 00:00:42.510
We're going to tackle these four steps in this one same tutorial.

14
00:00:42.570 --> 00:00:48.300
So let's do this, starting with this one, initializing the Hénin as a sequence of layers.

15
00:00:48.690 --> 00:00:50.150
So let's create a new Kotel.

16
00:00:50.460 --> 00:00:53.010
And now let me explain how we need to proceed.

17
00:00:53.610 --> 00:00:59.520
So the first thing we have to do is obviously to create a variable that will be nothing else than the

18
00:00:59.550 --> 00:01:01.290
artificial neural network itself.

19
00:01:01.380 --> 00:01:02.190
And guess what?

20
00:01:02.370 --> 00:01:08.220
This artificial neural network viable will be created as an object of a certain class.

21
00:01:08.520 --> 00:01:14.610
And that certain class is the sequential class, which allows exactly to build an artificial neural

22
00:01:14.610 --> 00:01:15.120
network.

23
00:01:15.240 --> 00:01:19.590
But as a sequence of layers as opposed to a computational graph.

24
00:01:19.690 --> 00:01:19.960
Right.

25
00:01:20.030 --> 00:01:25.200
You sign the intuition lectures that an artificial newel network is actually a sequence of layers,

26
00:01:25.200 --> 00:01:26.910
you know, starting from the input layer.

27
00:01:27.090 --> 00:01:32.070
And then successively we have the fully connected layers up to the final output layer.

28
00:01:32.430 --> 00:01:34.560
That's what I mean by a sequence of layers.

29
00:01:34.860 --> 00:01:38.580
And then the other type of new network is indeed a computational graph.

30
00:01:38.730 --> 00:01:42.060
Which are, you know, neurons connected anyway, not in successive layers.

31
00:01:42.300 --> 00:01:44.720
And an example of this is Boltzmann machines.

32
00:01:44.850 --> 00:01:45.200
Right.

33
00:01:45.300 --> 00:01:50.850
Restricted both machines or deep Bozman machines are great examples of computational graph.

34
00:01:51.180 --> 00:01:55.470
Of course, they're not covered in this course because this is really advanced deep learning.

35
00:01:55.650 --> 00:01:58.300
But they are covered in are deepening.

36
00:01:58.350 --> 00:02:03.490
It is at course, if you are really interested in deep learning and want to go deeper into this branch.

37
00:02:03.630 --> 00:02:06.070
Well, I'll be happy to welcome you into the debrie.

38
00:02:06.200 --> 00:02:06.880
It is at course.

39
00:02:06.930 --> 00:02:12.690
But now let's just get the right introduction to deep learning with only fully connected new networks.

40
00:02:12.930 --> 00:02:18.270
And so that's why we will create our new variable, which we're gonna call Hénin, and which will be

41
00:02:18.270 --> 00:02:21.930
nothing else than the artificial neural network we're going to build.

42
00:02:22.200 --> 00:02:27.990
And we will create that variable as an object of the sequential class.

43
00:02:28.220 --> 00:02:28.630
Okay.

44
00:02:28.830 --> 00:02:32.490
But of course, the sequential class is not taken from nowhere.

45
00:02:32.580 --> 00:02:40.620
It is actually taken from the models module, from the Keris library, which since denser flow 2.0 belongs

46
00:02:40.620 --> 00:02:41.170
to tens of.

47
00:02:41.510 --> 00:02:41.750
Right.

48
00:02:41.790 --> 00:02:44.380
Before we had tensor flow and Keris separated.

49
00:02:44.700 --> 00:02:50.250
But now, since the new tensor flow, the new version of Tentacle 2.0, well, Keris was integrated

50
00:02:50.400 --> 00:02:51.300
into tensor flow.

51
00:02:51.690 --> 00:02:55.800
And therefore, the way to call the sequential class here is to go first.

52
00:02:56.020 --> 00:03:01.560
Tenzer Flow, which has a short T.F. from which we're going to call a Kiraz Library.

53
00:03:02.040 --> 00:03:05.990
And from which we're going to call the models module.

54
00:03:06.390 --> 00:03:06.840
Perfect.

55
00:03:06.900 --> 00:03:09.390
And from which we call indeed that sequential class.

56
00:03:09.870 --> 00:03:16.500
And so that all this, you know, creates this a N variable, which represents our artificial neural

57
00:03:16.500 --> 00:03:23.070
network, created as an instance of that sequential class which initializes or artificial neural network

58
00:03:23.130 --> 00:03:24.630
as a sequence of layers.

59
00:03:24.780 --> 00:03:25.830
And that's our first step.

60
00:03:26.220 --> 00:03:27.210
Congratulations.

61
00:03:27.300 --> 00:03:31.470
Now, you really major step into how to build an artificial neural network.

62
00:03:31.560 --> 00:03:36.960
So let's move on to the next step, which is to add the input layer and the first hidden layer.

63
00:03:37.290 --> 00:03:43.650
And that's where we're going to start using the famous dance class in Tenzer Flow and even in PI Torch,

64
00:03:43.680 --> 00:03:46.910
which is another great library to build new networks.

65
00:03:47.190 --> 00:03:53.610
The way to add a fully connected layer into an artificial Newbill network at whatever phase you are,

66
00:03:53.610 --> 00:03:59.070
you know, whatever the state of your artificial Nool network is, well, is to use the dance class.

67
00:03:59.100 --> 00:04:04.620
And the way we use it is very simply by taking our artificial newel network object.

68
00:04:04.770 --> 00:04:10.770
In that instance of the sequential class from which we're gonna call one of the methods of the sequential

69
00:04:10.770 --> 00:04:12.930
class and that method is add.

70
00:04:13.080 --> 00:04:17.790
You know, we certainly hope that there is add method inside a sequential class.

71
00:04:18.030 --> 00:04:18.300
Right.

72
00:04:18.330 --> 00:04:24.410
So that's the method we need right now to add anything we want, whether it is a hidden layer or a drop

73
00:04:24.410 --> 00:04:29.640
out layer which allows to prevent overfitting or, you know, we will see with convolutional neural

74
00:04:29.640 --> 00:04:31.020
networks that we can also add.

75
00:04:31.130 --> 00:04:34.170
Well, a convert to delayer, which is a convolutional layer.

76
00:04:34.440 --> 00:04:35.970
Well, we can add anything.

77
00:04:36.000 --> 00:04:40.020
But right now, what we want to add is a simple, fully connected layer.

78
00:04:40.350 --> 00:04:45.150
And the way to add this is to enter in these parenthesis because add is the method.

79
00:04:45.540 --> 00:04:52.030
Well, it is to add exactly that fully connected layer, which will be a new object.

80
00:04:52.050 --> 00:04:55.230
You know, it will be a new instance of a new class.

81
00:04:55.260 --> 00:04:57.960
And that new class is, of course, the dance class.

82
00:04:58.290 --> 00:04:59.770
So the fully connected layer will.

83
00:04:59.850 --> 00:05:05.460
Built to build will be created as an object of the dense glass, and therefore now the only thing we

84
00:05:05.460 --> 00:05:07.920
have to do is to call that dance class.

85
00:05:07.960 --> 00:05:11.190
And this will create that fully connected layer object.

86
00:05:11.250 --> 00:05:14.610
And at the same time, it will automatically add the input layer.

87
00:05:15.210 --> 00:05:15.480
All right.

88
00:05:15.480 --> 00:05:16.710
So let's go the dance class.

89
00:05:16.800 --> 00:05:19.890
And once again, the dance class is not taken from nowhere.

90
00:05:19.920 --> 00:05:22.560
It belongs to a certain path of libraries.

91
00:05:22.890 --> 00:05:26.730
And of course, the route of that library is our sense of flow library.

92
00:05:27.060 --> 00:05:31.590
Then from which we're going to call once again the Keris library.

93
00:05:31.980 --> 00:05:32.730
There we go.

94
00:05:33.030 --> 00:05:36.760
From which this time we're not going to call the morals module.

95
00:05:36.900 --> 00:05:40.740
But actually, this one, you know, it's top of the list layers.

96
00:05:40.920 --> 00:05:42.690
That's exactly what we need to add here.

97
00:05:42.960 --> 00:05:45.270
This is the module that contains the different tools.

98
00:05:45.390 --> 00:05:51.030
And by tools, I mean classes to add, well, any layer you want in your artificial new network.

99
00:05:51.270 --> 00:05:52.230
So layers here.

100
00:05:52.920 --> 00:05:57.390
And speaking of these classes, well, that's from this layer as module that we're going to call our

101
00:05:57.390 --> 00:06:03.050
dance class, which as any class can take several arguments in here.

102
00:06:03.270 --> 00:06:05.310
We have to indeed enter these arguments.

103
00:06:05.340 --> 00:06:11.220
The most important one is this one unit, which corresponds exactly to the number of neurons, you know,

104
00:06:11.220 --> 00:06:12.720
to the number of hidden neurons.

105
00:06:12.960 --> 00:06:16.830
You want to have in this first hidden layer, you know, not in the input layer.

106
00:06:16.830 --> 00:06:19.650
We will automatically have our different features.

107
00:06:19.800 --> 00:06:24.750
You know, in the input layer, the input neurons will simply be all these features, starting from

108
00:06:24.750 --> 00:06:25.470
credit scores.

109
00:06:25.500 --> 00:06:29.660
That will be one neuron, then another input neuron, then another one, you know.

110
00:06:29.700 --> 00:06:34.090
Up to this one, all these will be the input neurons in the input layer.

111
00:06:34.380 --> 00:06:38.790
But then when we create that first hidden layer, we will have some hidden neurons inside.

112
00:06:39.090 --> 00:06:45.180
And in this dense function now, well, we can specify, of course, how many hidden neurons we want

113
00:06:45.180 --> 00:06:45.600
to have.

114
00:06:46.060 --> 00:06:51.240
And now now comes the most frequently asked question in deep learning.

115
00:06:51.450 --> 00:06:52.680
There's very famous question.

116
00:06:52.950 --> 00:06:55.410
How do we know how many neurons we want?

117
00:06:55.650 --> 00:06:57.270
Is there a rule of thumb?

118
00:06:57.300 --> 00:06:59.370
Or should we just experiment?

119
00:06:59.780 --> 00:07:02.760
Well, unfortunately, there is no rule of thumb.

120
00:07:03.300 --> 00:07:08.460
It is just based on experimentation or, you know, we call it the work of an artist.

121
00:07:08.580 --> 00:07:11.930
You have to experiment with different hyper parameters.

122
00:07:11.940 --> 00:07:16.920
You know, we call them hyper Ramras in the sense that these are parameters that won't be trained during

123
00:07:16.920 --> 00:07:17.880
the training process.

124
00:07:18.240 --> 00:07:20.370
So unfortunately, there is no rule of thumb.

125
00:07:20.550 --> 00:07:25.920
And therefore, we just have to pick one number here, which wouldn't sound irrelevant or extravagant.

126
00:07:26.100 --> 00:07:28.050
And that number will be six.

127
00:07:28.260 --> 00:07:32.370
I actually tried several numbers and I got more or less the same accuracy in the end.

128
00:07:32.460 --> 00:07:33.480
So it's all good.

129
00:07:33.480 --> 00:07:34.830
You can try different ones if you want.

130
00:07:35.070 --> 00:07:36.720
But six is totally fine.

131
00:07:36.750 --> 00:07:41.790
So here inside this dance class, we will enter our first parameter, which is unit.

132
00:07:42.180 --> 00:07:45.090
Well, units equals six.

133
00:07:45.390 --> 00:07:45.930
Perfect.

134
00:07:46.140 --> 00:07:46.440
All right.

135
00:07:46.470 --> 00:07:51.480
And now the next parameter that is important, you know, among this huge list of parameters, you can

136
00:07:51.480 --> 00:07:52.140
see many of them.

137
00:07:52.170 --> 00:07:53.220
But no worries.

138
00:07:53.220 --> 00:07:58.260
We will keep the default value for all of them except this one, which corresponds, of course, to

139
00:07:58.260 --> 00:07:59.790
the activation function.

140
00:08:00.060 --> 00:08:06.120
And you saw the intuition lecturers with Kyril that the activation function in the hidden layers of

141
00:08:06.120 --> 00:08:10.780
a fully connected neural network must be the rectifier activation function.

142
00:08:11.070 --> 00:08:13.710
And therefore, that's exactly what we must specify here.

143
00:08:13.800 --> 00:08:16.680
We, of course, don't want no activation function.

144
00:08:17.040 --> 00:08:20.670
So here we have to specify that we want to rectify activation function.

145
00:08:20.790 --> 00:08:26.490
And the way to specify this is to enter here in our activation parameter.

146
00:08:26.790 --> 00:08:31.890
Well, the code name for the rectifier activation function, which is in quotes.

147
00:08:32.310 --> 00:08:33.360
Well, Relu.

148
00:08:33.840 --> 00:08:36.690
That's the code name for the rectifier activation function.

149
00:08:37.050 --> 00:08:43.800
And that is all you have to enter here in order to make a fully working first fully connected, hidden

150
00:08:43.800 --> 00:08:44.190
layer.

151
00:08:44.430 --> 00:08:45.450
Congratulations.

152
00:08:45.660 --> 00:08:49.440
Now you know how to build actually, you know, a shallow neural network.

153
00:08:49.470 --> 00:08:53.400
And you will know in a second how to build a deep neural network.

154
00:08:53.670 --> 00:08:59.220
Because the way to actually add a second hidden layer here couldn't be more simple.

155
00:08:59.430 --> 00:09:03.330
The only thing that you have to do is just copy this line of code.

156
00:09:03.600 --> 00:09:09.450
And then in a new line of code here for the second here and there, you just need to paste it.

157
00:09:09.930 --> 00:09:17.550
That's what I mean by this add method can add any new layer at whatever stage of the construction process

158
00:09:17.550 --> 00:09:19.170
of your Eynon you're into.

159
00:09:19.410 --> 00:09:19.610
Right.

160
00:09:19.740 --> 00:09:24.150
You can use this add method to add anything and the weight add a second hidden layer.

161
00:09:24.300 --> 00:09:27.120
It's just the same as adding the first hidden layer.

162
00:09:27.450 --> 00:09:30.150
Unless, of course, you know, you want to change the number of Hillen neurons.

163
00:09:30.180 --> 00:09:35.370
But you know, six million neurons in the first thin layer and six other ones in the second hidden layer

164
00:09:35.660 --> 00:09:36.420
is just fine.

165
00:09:36.470 --> 00:09:40.170
But once again, feel free to change the hyper parameter values here.

166
00:09:40.410 --> 00:09:42.690
Maybe you will get a better accuracy in the end.

167
00:09:42.720 --> 00:09:46.860
And if that's the case, well, please share it in the comments or by private message.

168
00:09:47.640 --> 00:09:48.930
Okay, however.

169
00:09:48.960 --> 00:09:50.610
Now add the output layer.

170
00:09:50.730 --> 00:09:54.690
You have to do something special, you know, something different than what we did here.

171
00:09:54.810 --> 00:09:55.970
So let's do this together.

172
00:09:55.980 --> 00:09:58.850
Let's create a new code cell and.

173
00:09:59.070 --> 00:09:59.520
Well, let's.

174
00:09:59.840 --> 00:10:02.750
Actually based what we just copied before.

175
00:10:02.870 --> 00:10:08.870
Once again here, but this time we'll have to change two things which correspond actually to the values

176
00:10:08.870 --> 00:10:10.130
of these two parameters.

177
00:10:10.580 --> 00:10:13.190
But first, let me explain why all the rest is the same.

178
00:10:13.280 --> 00:10:17.750
Well, that's, of course, because, you know, we're adding a new layer and this add Mithun can add

179
00:10:17.870 --> 00:10:20.750
any layer you want, including, of course, the output layer.

180
00:10:20.930 --> 00:10:25.160
So here we're still using the ADD method to add this final output layer.

181
00:10:25.430 --> 00:10:30.710
And then of course, we still want our output layer to be fully connected to that second hidden layer

182
00:10:30.710 --> 00:10:33.590
and therefore we're using again here the dance class.

183
00:10:33.920 --> 00:10:34.940
So all good here.

184
00:10:35.030 --> 00:10:38.480
But then these two parameters have to be changed.

185
00:10:38.750 --> 00:10:43.670
And if you follow the intuition lecturer's, you should know what must be these two changes.

186
00:10:43.940 --> 00:10:44.200
All right.

187
00:10:44.210 --> 00:10:45.320
So let's go with this one.

188
00:10:45.470 --> 00:10:48.590
According to you, what do we need to replace here?

189
00:10:48.740 --> 00:10:49.940
Well, that's of course, there's value.

190
00:10:49.970 --> 00:10:53.750
But according to you, six has to be replaced by which value?

191
00:10:54.230 --> 00:10:59.420
Well, to get the answer, we need to have a look at our dependent variable again, which is this one,

192
00:10:59.690 --> 00:11:04.250
because remember, the output layer contains the dimensions of the output.

193
00:11:04.310 --> 00:11:05.810
You know, the output you want to predict.

194
00:11:06.240 --> 00:11:12.440
And here, since we actually want to predict a binary variable which can take the value one or zero,

195
00:11:12.760 --> 00:11:18.440
well, the dimension is actually one, right, because we only need one neuron to get that final prediction,

196
00:11:18.440 --> 00:11:19.070
zero 01.

197
00:11:19.430 --> 00:11:25.730
However, if we were doing classification with a non binary dependent variable like dependent variable

198
00:11:25.730 --> 00:11:27.140
that has three classes.

199
00:11:27.290 --> 00:11:28.470
Let's say ABC.

200
00:11:28.790 --> 00:11:30.860
Well, we would actually need three dimensions.

201
00:11:30.890 --> 00:11:32.360
You know, three output neurons.

202
00:11:32.600 --> 00:11:34.160
Two once again, one hutting code.

203
00:11:34.340 --> 00:11:39.140
That dependent variable, because, of course, once again, there is no relationship order between

204
00:11:39.140 --> 00:11:40.470
the classes A, B and C..

205
00:11:40.610 --> 00:11:46.520
So A, for example, would have to be encoded by one 00, then B, would have to be encoded by zero

206
00:11:46.520 --> 00:11:50.330
one zero and C would have to be encoded by zero zero one.

207
00:11:50.420 --> 00:11:55.910
And therefore you need three neurons getting these value zero and one to encode your three classes,

208
00:11:55.970 --> 00:11:56.660
a BNC.

209
00:11:57.020 --> 00:12:01.010
But here, since we actually have binary variable binary outcome.

210
00:12:01.250 --> 00:12:06.020
Well, you only need one neuron to encode these outcomes into one or zero.

211
00:12:06.260 --> 00:12:13.730
And therefore, the value of that unit parameter here that we have to replace right now is actually

212
00:12:14.030 --> 00:12:14.450
one.

213
00:12:14.870 --> 00:12:19.130
OK, one output neuron encoding the dependent variable.

214
00:12:19.490 --> 00:12:23.330
And then second change corresponds, of course, to that activation function.

215
00:12:23.360 --> 00:12:26.240
And more specifically, to the value of the activation function.

216
00:12:26.660 --> 00:12:31.820
Well, once again, remembering the intuition, lecturer's that for the activation function of the output

217
00:12:31.820 --> 00:12:32.210
layer.

218
00:12:32.420 --> 00:12:38.930
While you don't want to have a rectifier activation function but a sigmoid activation function.

219
00:12:39.410 --> 00:12:40.130
Why is that?

220
00:12:40.370 --> 00:12:47.450
It's because having a sigmoid activation function allows to get not only ultimately the predictions,

221
00:12:47.720 --> 00:12:48.920
but even better.

222
00:12:49.040 --> 00:12:53.300
It will give you the probabilities that the binary outcome is one.

223
00:12:53.690 --> 00:12:59.910
So that we will not only get the predictions of whether the customers choose to leave or not the bank.

224
00:13:00.110 --> 00:13:05.270
But we will also have for each customer the probability that the customer leaves the bank.

225
00:13:05.300 --> 00:13:09.950
And all this thanks to that sigmoid activation function.

226
00:13:10.340 --> 00:13:14.660
So you definitely want that sigmoid activation function for the output layer.

227
00:13:14.690 --> 00:13:17.080
Only, you know, all the other layers.

228
00:13:17.090 --> 00:13:19.610
You know, the other fully connected layers will get.

229
00:13:19.760 --> 00:13:20.180
Indeed.

230
00:13:20.360 --> 00:13:22.400
That Rectifier activation function.

231
00:13:22.960 --> 00:13:30.230
And now now I really must say congratulations, because we're actually done with the creation of this

232
00:13:30.230 --> 00:13:32.510
very first artificial new network.

233
00:13:32.750 --> 00:13:34.280
So you can be proud of yourself.

234
00:13:34.580 --> 00:13:36.830
You just built an artificial brain.

235
00:13:37.190 --> 00:13:37.970
Was that hard?

236
00:13:37.990 --> 00:13:39.530
Was that overwhelming?

237
00:13:39.830 --> 00:13:40.760
I don't think so.

238
00:13:40.910 --> 00:13:43.970
And that is the beauty of Tensor Flow 2.0.

239
00:13:44.240 --> 00:13:45.890
So I hope you enjoy this.

240
00:13:45.950 --> 00:13:48.650
I hope you enjoyed building your very first artificial brain.

241
00:13:48.890 --> 00:13:50.170
But that's not over now.

242
00:13:50.270 --> 00:13:52.020
We only have a brain so far, you know.

243
00:13:52.070 --> 00:13:57.510
But which is totally stupid, actually, because it was not trained yet on the data set.

244
00:13:57.830 --> 00:14:00.650
So we're going to make it smart and we're gonna make it smart.

245
00:14:00.800 --> 00:14:08.780
In part three, training the N.N. in which we will first compile the A and then with an optimizer and

246
00:14:08.780 --> 00:14:11.750
then a lost function and then we will train.

247
00:14:11.780 --> 00:14:16.840
Finally, our artificial neural network on the whole training set in a certain number of epochs.

248
00:14:17.000 --> 00:14:21.080
And you will see that the training process will be very exciting to visualize.

249
00:14:21.470 --> 00:14:22.940
I can't wait to show this to you.

250
00:14:23.240 --> 00:14:25.760
Let's tackle Part three together in the next tutorial.

251
00:14:26.000 --> 00:14:27.950
And until then, enjoy machine learning.
