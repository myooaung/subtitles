WEBVTT
1

00:00:00.270  -->  00:00:05.640
Hello and welcome to this art Tauriel in your section you already introduced to nonlinear regression

2

00:00:05.640  -->  00:00:09.470
models the polynomial regression model and your regression model.

3

00:00:09.630  -->  00:00:15.090
And today we're going to introduce a new non-linear regression model which is the decision tree regression

4

00:00:15.210  -->  00:00:15.970
model.

5

00:00:16.290  -->  00:00:21.330
So for the best of all we had for a data set and our specific problem was the polynomial regression

6

00:00:21.330  -->  00:00:21.870
model.

7

00:00:21.990  -->  00:00:24.220
Let's see how Decision Tree regression will do.

8

00:00:24.450  -->  00:00:27.090
And compared to the previous ones.

9

00:00:27.120  -->  00:00:28.830
So let's start with the basics.

10

00:00:28.830  -->  00:00:30.900
Let's set the right for there is working directories.

11

00:00:30.900  -->  00:00:37.080
Right now I'm going to the regression folder and we are now in Decision Tree regression on who's getting

12

00:00:37.080  -->  00:00:38.250
to the end.

13

00:00:38.280  -->  00:00:44.050
So this is the right folder containing the position services the file make sure of that and it becomes

14

00:00:44.070  -->  00:00:46.700
more of a set as working directory.

15

00:00:46.890  -->  00:00:52.140
All good and now let's take a regression template to build this model efficiently.

16

00:00:52.200  -->  00:00:57.600
So we take everything from here to here and pasting it here.

17

00:00:57.750  -->  00:01:03.340
Right and now we just need to change a few things so let's change the basics let's replace your regression

18

00:01:03.340  -->  00:01:10.010
will by Decision Tree regression.

19

00:01:10.010  -->  00:01:10.960
All right.

20

00:01:11.010  -->  00:01:15.750
We're going to copy that and put that same title here.

21

00:01:15.750  -->  00:01:16.170
All right.

22

00:01:16.180  -->  00:01:18.520
Visualizing the decision true aggression results.

23

00:01:18.750  -->  00:01:20.530
And same here.

24

00:01:20.550  -->  00:01:21.270
All right.

25

00:01:21.270  -->  00:01:26.550
And now let's change the most important thing which is the part where we create our repressor so I'm

26

00:01:26.550  -->  00:01:28.700
going to remove that come into line here.

27

00:01:28.920  -->  00:01:31.610
And now let's build our decision tree regression model.

28

00:01:31.770  -->  00:01:37.500
So it's going to be the same thing as any time we're going to import a package and then use a function

29

00:01:37.500  -->  00:01:40.040
from this package to build our aggressor.

30

00:01:40.260  -->  00:01:44.810
So this package for this is an aggression and all will be the our park package.

31

00:01:44.910  -->  00:01:51.650
For those of you who don't have the our part package in your packages here I will type this command

32

00:01:51.690  -->  00:01:53.230
line so that you can install it.

33

00:01:53.340  -->  00:01:55.710
So as you can see mine is already here are part.

34

00:01:55.710  -->  00:01:56.740
Just check if you have it.

35

00:01:56.790  -->  00:01:59.330
And if you don't have it just type here.

36

00:01:59.360  -->  00:02:05.820
Install packages and then in the parenthesis in quotes you type the name of the package which is our

37

00:02:05.820  -->  00:02:07.280
part.

38

00:02:07.290  -->  00:02:07.800
All right.

39

00:02:07.820  -->  00:02:10.120
Then you just select this line executes.

40

00:02:10.140  -->  00:02:12.170
And this one stealth package.

41

00:02:12.210  -->  00:02:15.890
All right but I'm going to put this line as comment.

42

00:02:16.020  -->  00:02:17.860
Just press command for shift.

43

00:02:18.190  -->  00:02:25.500
And now let's also add this line library and in parenthesis the name of the package we want to select

44

00:02:25.510  -->  00:02:25.550
.

45

00:02:25.590  -->  00:02:28.580
Well import our part right.

46

00:02:28.770  -->  00:02:31.620
And as you can see our part is not selected here.

47

00:02:31.620  -->  00:02:35.770
And when this line of code will be executed then this will be selected here.

48

00:02:35.950  -->  00:02:39.350
All right and now we are ready to start building the model.

49

00:02:39.390  -->  00:02:42.280
So as I just said we're going to take a function from this.

50

00:02:42.300  -->  00:02:46.450
Our part library and this function is actually our part as well.

51

00:02:46.470  -->  00:02:50.690
So let's use this function and as usual we're going to call every aggressor regressors.

52

00:02:50.850  -->  00:02:52.360
So here it is.

53

00:02:52.560  -->  00:02:55.110
So that's our decision tree regression regrind sir.

54

00:02:55.320  -->  00:02:58.680
And let's not use the our parts function.

55

00:02:58.680  -->  00:03:01.850
All right so now let's see what arguments we need to input.

56

00:03:01.860  -->  00:03:09.200
The best way to do that is to press one here so that we get the info about the our part library.

57

00:03:09.200  -->  00:03:15.210
So usually we have it here but then we just need to click on this link here and that will give us info

58

00:03:15.210  -->  00:03:16.630
about this arbor library.

59

00:03:16.790  -->  00:03:17.010
OK.

60

00:03:17.010  -->  00:03:18.280
So let's see what we have.

61

00:03:18.540  -->  00:03:19.910
The first argument is formula.

62

00:03:20.010  -->  00:03:21.490
So you know what to do here.

63

00:03:21.510  -->  00:03:28.170
We just need to write formula equals the dependent variable then a Tilda and then a dot that represents

64

00:03:28.230  -->  00:03:29.750
all your independent variables.

65

00:03:29.760  -->  00:03:32.420
All right but that we know perfectly then data.

66

00:03:32.460  -->  00:03:36.790
So data is the data set on which we want to build our decision tree regression model.

67

00:03:36.870  -->  00:03:42.740
So since we did not build any training set data is going to be the data set exactly like polynomial

68

00:03:42.750  -->  00:03:45.460
regression and as regression.

69

00:03:45.470  -->  00:03:51.450
All right then weights is an optional arguments you can add some weights to make your model more advanced

70

00:03:51.460  -->  00:03:51.510
.

71

00:03:51.540  -->  00:03:54.990
But you know this is more advanced that we will not cover that right now.

72

00:03:55.290  -->  00:04:00.000
And then you have some other arguments that are optional but that can help you make your model even

73

00:04:00.000  -->  00:04:00.950
more robust.

74

00:04:00.960  -->  00:04:06.890
Or you know include some regularisation techniques or zation to prevent overfitting that sort of things

75

00:04:06.900  -->  00:04:06.930
.

76

00:04:06.950  -->  00:04:12.720
But right now we just want to build a simple this isn't true at all because we have a simple data set

77

00:04:13.020  -->  00:04:15.400
and so we only need formula and data.

78

00:04:15.420  -->  00:04:19.890
So let's go back to our and let's put these arguments.

79

00:04:19.890  -->  00:04:27.390
So the first argument was formula and then as I mentioned it's formula equals salary because it's our

80

00:04:27.390  -->  00:04:31.470
dependent variable than Tilda just pressing.

81

00:04:31.550  -->  00:04:36.280
And here and then a dot or actually level.

82

00:04:36.600  -->  00:04:41.190
But you know I want to make this script that can be also applied to your data sets.

83

00:04:41.190  -->  00:04:42.560
So I'll use a dot.

84

00:04:42.570  -->  00:04:43.230
All right.

85

00:04:43.230  -->  00:04:44.750
So that's the first argument.

86

00:04:44.970  -->  00:04:52.930
Now let's give the second one the second one was data the data equals data set.

87

00:04:53.370  -->  00:04:57.130
And now our regressors ready to be built only with these two arguments.

88

00:04:57.150  -->  00:04:58.140
Very simple.

89

00:04:58.140  -->  00:04:59.700
Let's do it.

90

00:04:59.710  -->  00:05:00.720
So it's really the be all.

91

00:05:00.750  -->  00:05:08.330
And that means our code is ready to be executed because we don't need to change anything more.

92

00:05:08.620  -->  00:05:14.630
So let's execute the sections one by one and let's see what happens with decision tree regression.

93

00:05:15.100  -->  00:05:20.870
So I'm going to select the first section executes days that's one important.

94

00:05:20.890  -->  00:05:21.810
Here it is.

95

00:05:21.820  -->  00:05:27.570
So the independent variable contains 10 levels here and the dependent variable contains 10 salary's

96

00:05:27.610  -->  00:05:29.920
here associated to these 10 levels here.

97

00:05:29.920  -->  00:05:34.730
And our goal is to predict the salary of a 6.5 level.

98

00:05:34.780  -->  00:05:36.820
The data is not linearly distributed.

99

00:05:36.820  -->  00:05:38.600
This is not a linear problem.

100

00:05:38.650  -->  00:05:44.140
So that's why we're making some non linear regression models to make this prediction right.

101

00:05:44.170  -->  00:05:48.440
So there's no need to split the data sets into a training set in a test.

102

00:05:48.440  -->  00:05:53.740
And because as you can see this is a very small data set then no need for features getting because for

103

00:05:53.740  -->  00:05:59.050
decision trees we don't need to do any feature scaling because the way this model is built is based

104

00:05:59.050  -->  00:06:03.390
on conditions on the independent variable and not on Euclidean distances.

105

00:06:03.610  -->  00:06:04.990
So we're fine with that.

106

00:06:04.990  -->  00:06:07.380
We definitely don't need to plan features scanning.

107

00:06:07.660  -->  00:06:14.770
And we can move on to the next step which is to create our models so let's create it executing.

108

00:06:14.770  -->  00:06:15.220
All right.

109

00:06:15.220  -->  00:06:17.620
Perfect regressors is created.

110

00:06:17.800  -->  00:06:20.750
And now let's get our final verdict.

111

00:06:20.920  -->  00:06:27.580
OK so hard in the UK according to this person and now let's see the predicted salary according to our

112

00:06:27.580  -->  00:06:28.290
model.

113

00:06:28.600  -->  00:06:37.150
So executing this and we get a two hundred forty nine thousand dollars while much higher than the salary

114

00:06:37.150  -->  00:06:38.420
mentioned by this person.

115

00:06:38.470  -->  00:06:44.000
But let's not drop hasty conclusions and let's see what's happening on the graph here.

116

00:06:44.260  -->  00:06:50.260
So I'm going to select all this and let's see what's happening with the decision tree regression results

117

00:06:50.260  -->  00:06:51.880
.

118

00:06:51.900  -->  00:06:53.340
All right that's what I thought.

119

00:06:53.410  -->  00:06:53.890
OK.

120

00:06:54.070  -->  00:06:55.860
So we don't need to zoom it.

121

00:06:55.860  -->  00:06:57.560
We clearly see what's happening here.

122

00:06:57.580  -->  00:07:04.120
It's flooding a straight horizontal line exactly like we saw in SVR with for Python.

123

00:07:04.120  -->  00:07:09.040
For those of you who didn't follow the pricing tutorial know that we already encountered this situation

124

00:07:09.040  -->  00:07:12.190
when we get a straight horizontal line.

125

00:07:12.190  -->  00:07:14.230
And actually as we are.

126

00:07:14.230  -->  00:07:20.120
This was due to the fact that we didn't apply features scaling to our dataset.

127

00:07:20.170  -->  00:07:23.140
So what do you think the problem is here.

128

00:07:23.200  -->  00:07:27.950
Do you think it's due to the fact that we didn't apply features skin like as we are and we need to act

129

00:07:27.980  -->  00:07:31.490
like you're just going to get a model fitting properly the data set.

130

00:07:31.870  -->  00:07:36.810
Well as I mentioned in the beginning of this to toile we definitely don't need to apply features going

131

00:07:36.830  -->  00:07:42.560
into decision trees because Decision Tree regression models are based on conditions on the variables

132

00:07:42.600  -->  00:07:45.270
that has nothing to do with Euclidean distances.

133

00:07:45.400  -->  00:07:49.690
And you know when we need to if you're just getting it's because the machine only models are based on

134

00:07:49.780  -->  00:07:55.600
Euclidean distances and we need to put all the independent variables in the same scale so that one independent

135

00:07:55.600  -->  00:07:57.960
variable is not dominating another one.

136

00:07:58.270  -->  00:08:00.240
But this is not the problem here.

137

00:08:00.250  -->  00:08:02.300
This is not about future scanning.

138

00:08:02.300  -->  00:08:07.810
You can try to apply features skinning here and we execute this but you'll get the same problem with

139

00:08:07.810  -->  00:08:09.090
a straight horizontal line.

140

00:08:09.100  -->  00:08:12.130
And of course this is actually the decision tree model.

141

00:08:12.160  -->  00:08:18.340
This is actually one model of decision tree but this is of course not the best version of decision tree

142

00:08:18.340  -->  00:08:19.800
regression we want to get.

143

00:08:20.020  -->  00:08:26.110
So can you start seeing what's the problem here and especially after watching the intuition tutorial

144

00:08:26.110  -->  00:08:29.100
made by Kiril Can you spot the problem.

145

00:08:29.350  -->  00:08:36.480
I'm going to tell you this problem is related to the number of splits because you know the way to dition

146

00:08:36.490  -->  00:08:42.010
true aggression more was made is that it's making some splits based on different conditions so the more

147

00:08:42.010  -->  00:08:45.160
conditions you have in your own variables the more you have splits.

148

00:08:45.160  -->  00:08:50.650
And here we clearly have no place here because you know all the predictions are equal to two hundred

149

00:08:50.650  -->  00:08:52.450
and fifty thousand dollars.

150

00:08:52.450  -->  00:08:57.250
So you know it took all the different salaries for the different 10 levels here and made an average

151

00:08:57.520  -->  00:09:00.200
and just gave the average for all the levels.

152

00:09:00.280  -->  00:09:08.200
So no conditions here no split and therefore that's absolutely not interesting especially for the potential

153

00:09:08.200  -->  00:09:09.530
that this country can have.

154

00:09:09.700  -->  00:09:15.550
So what we'll do now is add a parameter here that will set a condition on the splits.

155

00:09:15.580  -->  00:09:17.060
You know that's what I was telling you.

156

00:09:17.080  -->  00:09:23.410
We have several parameters in this Arbre library and we can use these optional parameters to improve

157

00:09:23.500  -->  00:09:25.270
our model and make it more robust.

158

00:09:25.420  -->  00:09:27.480
Well this is exactly what we're going to do now.

159

00:09:27.490  -->  00:09:30.340
We are going to get back to our parts.

160

00:09:30.340  -->  00:09:32.460
I'm going to press one here.

161

00:09:32.650  -->  00:09:34.810
Actually this time our party is showing up.

162

00:09:34.810  -->  00:09:35.110
OK.

163

00:09:35.110  -->  00:09:36.340
So great.

164

00:09:36.370  -->  00:09:37.240
Our board is here.

165

00:09:37.300  -->  00:09:44.380
And as I mentioned our part has several parameters that we can use to make a much more robust and the

166

00:09:44.380  -->  00:09:49.550
one we're interested in right now is one parameter that will correct this problem we had with this place

167

00:09:49.560  -->  00:09:49.960
.

168

00:09:49.990  -->  00:09:53.380
So this parameter is actually the control parameter.

169

00:09:53.380  -->  00:09:57.970
And right now I'm going to give you a little trick to solve this problem on the split.

170

00:09:57.970  -->  00:10:04.880
We had just obtained here so I'm going to add this third optional argument you know to improve our model

171

00:10:04.880  -->  00:10:04.900
.

172

00:10:04.900  -->  00:10:09.730
Right now we're doing some model performance improvement so that's a thing that machinery scientists

173

00:10:09.790  -->  00:10:11.320
do very often in their job.

174

00:10:11.410  -->  00:10:16.660
So don't we will get more advanced sections on it especially when we cover cross-validation to find

175

00:10:16.660  -->  00:10:22.870
the best morals selecting the best parameters but here we'll just do some simple model performance improvement

176

00:10:23.230  -->  00:10:27.680
and we'll just add this control parameter and then I'm going to give you this little trick.

177

00:10:27.700  -->  00:10:33.430
So the trick is to take our part library again.

178

00:10:33.430  -->  00:10:38.390
So we're taking our part control here which is a function and in this function we're going to add an

179

00:10:38.410  -->  00:10:41.750
argument as you can see here on this yellow rectangle.

180

00:10:41.740  -->  00:10:46.320
Here we have the first argument that is men's plates and that's exactly what we're interested in.

181

00:10:46.360  -->  00:10:51.790
And that's what will solve our problem because you know actually we didn't have any split here because

182

00:10:51.790  -->  00:10:53.290
it just took the average.

183

00:10:53.290  -->  00:10:57.310
So it's like we had no conditions on the independent variables and no splits.

184

00:10:57.370  -->  00:11:03.040
So to make sure we have some splits and some conditions on the independent variables we will actually

185

00:11:03.160  -->  00:11:08.800
set mini splits to 1 and that will solve the problem.

186

00:11:08.800  -->  00:11:09.560
So let's try.

187

00:11:09.670  -->  00:11:14.470
That's no real select this section to create a new aggressor.

188

00:11:14.680  -->  00:11:18.290
So our party is already imported so I don't need to like that again.

189

00:11:18.520  -->  00:11:21.320
And let's create this aggressor.

190

00:11:21.710  -->  00:11:22.520
Ok done.

191

00:11:22.540  -->  00:11:25.760
Perfect your aggressor created all properly.

192

00:11:25.980  -->  00:11:26.340
OK.

193

00:11:26.350  -->  00:11:32.410
Now let's perhaps first visualize the results to see if our model is now correct before getting to the

194

00:11:32.410  -->  00:11:33.490
final verdict.

195

00:11:33.760  -->  00:11:37.010
Because you know we need to validate the model now.

196

00:11:37.690  -->  00:11:40.920
So I'm selecting the section and let's see where we get.

197

00:11:40.960  -->  00:11:43.860
Let's cross our fingers.

198

00:11:43.900  -->  00:11:44.610
Here we go.

199

00:11:44.620  -->  00:11:46.440
I'm going to zoom on the plot.

200

00:11:47.200  -->  00:11:52.810
And now the first word I have to say here is trap or red flag.

201

00:11:52.810  -->  00:11:55.560
Right now we're just in front of a Neutra.

202

00:11:55.690  -->  00:11:56.920
The model is improved.

203

00:11:56.920  -->  00:12:00.860
Definitely we can definitely see that we have more than one spits here.

204

00:12:00.970  -->  00:12:04.360
For example here that's a split that's another split here and here.

205

00:12:04.480  -->  00:12:07.940
So OK we solved the problem of the number of splits.

206

00:12:08.150  -->  00:12:12.020
That's according to what Karylle explained in the intuition to toile.

207

00:12:12.040  -->  00:12:16.020
Do you think that's the real shape of a decision tree regression model.

208

00:12:16.330  -->  00:12:21.550
Well because you know what Carol explained is the algorithm of decision tree regression is that by considering

209

00:12:21.550  -->  00:12:27.730
the entropy in the information again it's splitting the independent variables into several intervals

210

00:12:27.740  -->  00:12:27.870
.

211

00:12:28.000  -->  00:12:32.740
So in the intuition tutoyer you had two independent variables so the different intervals formed some

212

00:12:32.740  -->  00:12:36.910
rectangles in which you took the average of the dependent variable values.

213

00:12:37.000  -->  00:12:42.280
But here since we are in one dimension that means that the algorithm would only take intervals here

214

00:12:42.280  -->  00:12:45.960
of the independent variable for example that should be one interval.

215

00:12:46.060  -->  00:12:48.520
And here it looks like it's the second one.

216

00:12:48.520  -->  00:12:49.970
And here is the third one.

217

00:12:49.980  -->  00:12:51.840
And here a fourth one.

218

00:12:51.850  -->  00:12:52.230
OK.

219

00:12:52.270  -->  00:12:58.140
So basically it looks like we have four conditions and for intervals that are making the splits.

220

00:12:58.330  -->  00:13:03.530
But as you understood in the intuition tutorial it's taking the average in each interval.

221

00:13:03.580  -->  00:13:09.270
So if it's taking the average How do you want to have this straight line here that is not horizontal

222

00:13:09.280  -->  00:13:10.410
because you know what.

223

00:13:10.400  -->  00:13:16.060
This isn't true aggression is doing is that in each interval it's calculating the average of the dependent

224

00:13:16.060  -->  00:13:22.150
variable salaries and therefore for all the levels contained in this interval the value of the prediction

225

00:13:22.150  -->  00:13:26.900
should be a constant equal to this average of the dependent variable in this interval.

226

00:13:27.040  -->  00:13:31.420
And here as we can see it's not a constant you know the prediction here is not the same as the prediction

227

00:13:31.420  -->  00:13:31.880
here.

228

00:13:31.990  -->  00:13:38.500
So either it's considering an infinity of intervals with different constants in each of those infinite

229

00:13:38.530  -->  00:13:44.650
intervals or either we have a problem here of course it's not the first option of course the decision

230

00:13:44.650  -->  00:13:50.150
tree regression is not considering an infinity of intervals between this level and this level.

231

00:13:50.200  -->  00:13:52.390
So it's definitely the second option.

232

00:13:52.390  -->  00:13:54.650
So now do you see where the problem comes from.

233

00:13:54.940  -->  00:14:02.590
Well the answer is in our regression templates because what we observe here is only due to the resolution

234

00:14:02.590  -->  00:14:08.980
we picked to plot these decision tree regression results because we were actually plotting the predictions

235

00:14:09.250  -->  00:14:12.220
for each of the 10 levels incremented by 1.

236

00:14:12.220  -->  00:14:16.940
That means that here you know it's only plotting the predictions of the 10 salary's corresponding to

237

00:14:16.940  -->  00:14:18.040
the 10 levels.

238

00:14:18.040  -->  00:14:23.410
And then it's joined in the predictions by a straight line here because it had no predictions to plot

239

00:14:23.620  -->  00:14:27.220
in this interval here of the inviolable level.

240

00:14:27.550  -->  00:14:33.160
And the fact that it's a problem for this new non-linear regression model the decision tree model is

241

00:14:33.160  -->  00:14:37.450
due to a very specific reason for the previous non-linear regression models.

242

00:14:37.450  -->  00:14:42.670
We could use the code that generated this plot because the models were actually continuous.

243

00:14:42.670  -->  00:14:48.130
So for example in the polynomial regression model well between these prediction and these prediction

244

00:14:48.280  -->  00:14:51.030
Well it was actually almost a straight line here.

245

00:14:51.040  -->  00:14:54.520
However right now we're facing a new kind of regression model.

246

00:14:54.700  -->  00:14:59.110
Remember the first kind of regression model we studied was the linear regression model.

247

00:14:59.110  -->  00:15:03.230
Then the second kind of regression all we saw was the non linear regression model.

248

00:15:03.370  -->  00:15:06.900
And now we're facing a new kind of regression model.

249

00:15:06.940  -->  00:15:11.520
It's the nonlinear and non-continuous regression model.

250

00:15:11.680  -->  00:15:16.670
Indeed all the previous regression models that we saw whether there were linear are or not linear or

251

00:15:16.870  -->  00:15:18.440
they were all continuous.

252

00:15:18.520  -->  00:15:24.370
But here the decision tree regression model is not continuous and this is the first non-continuous machine

253

00:15:24.580  -->  00:15:26.380
model we are seeing together.

254

00:15:26.680  -->  00:15:30.640
And so you what is the best way to visualize non-continuous regression model.

255

00:15:30.700  -->  00:15:36.390
Well as I was telling you the answer is the solution for this is in our regression template.

256

00:15:36.460  -->  00:15:43.030
So let's have a look and actually we need to take the code section that visualize the regression model

257

00:15:43.030  -->  00:15:45.790
results in higher resolution.

258

00:15:45.790  -->  00:15:47.180
So let's take this.

259

00:15:47.350  -->  00:15:54.250
And let's actually go back to a decision tree regression file and replace this code here because this

260

00:15:54.250  -->  00:15:59.740
is totally not appropriate for decision tree regression model because it's a non-continuous model and

261

00:15:59.740  -->  00:16:02.560
so we need to replace this code by the same.

262

00:16:02.600  -->  00:16:06.190
But for the high resolution and now let's check it out.

263

00:16:06.250  -->  00:16:12.550
The model is actually ready we can actually replace that by Decision Tree regression.

264

00:16:14.380  -->  00:16:20.050
And now let's check it out and you will see what the real decision tree regression model looks like

265

00:16:20.140  -->  00:16:21.060
in one d.

266

00:16:21.070  -->  00:16:25.500
So let's do it let's select all this and execute.

267

00:16:26.050  -->  00:16:27.660
And here it is.

268

00:16:27.820  -->  00:16:31.550
That's what it looks like and actually it's a non-continuous model.

269

00:16:31.570  -->  00:16:37.270
We should even have some strictly vertical line here you know to represent better than contiguity.

270

00:16:37.480  -->  00:16:40.350
And to do this we just need to increase the resolution.

271

00:16:40.360  -->  00:16:46.150
So I'm going to put up on 0 1 and you'll see that we'll get the real representation of the decision

272

00:16:46.150  -->  00:16:47.870
tree regression results.

273

00:16:47.920  -->  00:16:50.290
So let's do it.

274

00:16:50.290  -->  00:16:52.180
Here it is now almost vertical.

275

00:16:52.210  -->  00:16:55.950
And that's a clear representation of the decision true Gresham mo.

276

00:16:55.990  -->  00:16:57.350
So now let's zoom on it.

277

00:16:57.550  -->  00:17:04.030
And now it makes much more sense because as Carol explained based on the entropy in the information

278

00:17:04.020  -->  00:17:08.960
again it splits the whole range of your independent variable into different intervals.

279

00:17:09.100  -->  00:17:15.160
So here we can clearly see what intervals are the first interval is from 1 to 6.5.

280

00:17:15.190  -->  00:17:19.040
The second interval is from 6.5 to 8.5.

281

00:17:19.150  -->  00:17:23.450
Then the third interval is from eight point five to 9.5.

282

00:17:23.590  -->  00:17:27.200
And finally the last interval is from nine point five to 10.

283

00:17:27.370  -->  00:17:33.100
So there we go we can clearly see the intervals now and as Karylle explained in the intuition to Tauriel

284

00:17:33.400  -->  00:17:39.130
the decision tree regression model is considering the average of the dependent variable values in each

285

00:17:39.190  -->  00:17:40.280
of the intervals.

286

00:17:40.360  -->  00:17:43.540
This one this one this one and this one.

287

00:17:43.540  -->  00:17:48.970
So for example if we consider this interval here the average of the salaries in this interval will that's

288

00:17:48.970  -->  00:17:52.440
very simple it's actually 250000.

289

00:17:52.490  -->  00:18:01.390
And so for each of the level between 6.5 and 8.5 the salaries will be predicted to be $250000.

290

00:18:01.390  -->  00:18:06.940
So we already know what our model will predict for our 6.5 level here.

291

00:18:06.970  -->  00:18:09.630
It's going to predict 250 K.

292

00:18:09.790  -->  00:18:14.950
So speaking of this prediction and now as we get the decision tree regression graphic results very well

293

00:18:15.130  -->  00:18:21.400
let's actually check that the previous salary of this employee that had a 6.5 level in its previous

294

00:18:21.400  -->  00:18:25.830
company is actually two hundred and fifty k let's check it out.

295

00:18:26.110  -->  00:18:28.490
Let's select this line and execute.

296

00:18:28.730  -->  00:18:29.830
And here it is.

297

00:18:29.830  -->  00:18:32.130
Two hundred and fifty thousand.

298

00:18:32.170  -->  00:18:36.360
Exactly like we predicted because we can clearly see that on this plot.

299

00:18:36.520  -->  00:18:38.770
So now I just want to say two things to conclude.

300

00:18:38.890  -->  00:18:44.320
The decision tree regression model is not an interesting model in one d but it can be a very interesting

301

00:18:44.320  -->  00:18:47.040
and very powerful model in more dimensions.

302

00:18:47.080  -->  00:18:50.200
So that's why you can use this code here for your data sets.

303

00:18:50.260  -->  00:18:54.170
You have the code here that builds the model and this one here that makes a prediction.

304

00:18:54.190  -->  00:18:58.690
But then you won't be able to use this code because you will have probably a lot of independent variables

305

00:18:58.750  -->  00:19:02.550
and therefore a lot of dimensions that you know will give you the explanation in 2-D.

306

00:19:02.590  -->  00:19:07.150
I'm giving you the explanation of one piece of that now in your mind you can perfectly represent in

307

00:19:07.150  -->  00:19:10.270
your head a decision tree regression and how it works.

308

00:19:10.570  -->  00:19:15.780
And now we'd like to end this tutorial by an enigma in the next section you will see around them forests

309

00:19:16.150  -->  00:19:21.640
and around them for us is actually real simple it's just a team of several decision trees.

310

00:19:21.640  -->  00:19:28.630
So knowing that this is the result of one tree what do you think we'll get with a team of 10 trees or

311

00:19:28.630  -->  00:19:31.300
even a hundred trees or 500 trees.

312

00:19:31.300  -->  00:19:34.940
The first question is do you think we'll get this shape of some stairs here.

313

00:19:35.110  -->  00:19:40.540
And the second question is do you think we'll get a much more accurate prediction like a prediction

314

00:19:40.540  -->  00:19:46.160
that is getting very close to the 160 K that is supposed to be the real salary.

315

00:19:46.180  -->  00:19:49.220
So these are the two questions I will let you think about that.

316

00:19:49.300  -->  00:19:52.700
And I look forward to giving you the solution in the next section.

317

00:19:52.720  -->  00:19:54.520
Until then enjoy mission learning
