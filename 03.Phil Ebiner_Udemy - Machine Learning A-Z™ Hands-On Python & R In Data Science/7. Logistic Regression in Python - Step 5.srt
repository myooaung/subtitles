1
00:00:00,420 --> 00:00:01,290
Hello my friends.

2
00:00:01,380 --> 00:00:01,710
All right.

3
00:00:01,710 --> 00:00:07,440
Let's see if you got this right again predicting the test results and displaying the vector of the predictions

4
00:00:07,530 --> 00:00:12,240
next to the vector of the real results mean the real purchase decisions.

5
00:00:12,540 --> 00:00:18,330
And I really want you to juggle with all your two kids you know whether it is the data crossing to kids

6
00:00:18,450 --> 00:00:24,390
or your other machinery models where you have indeed several tools inside because indeed now the tool

7
00:00:24,390 --> 00:00:30,270
we would like to use is that you know little piece of code that allows to display the two vectors of

8
00:00:30,270 --> 00:00:32,760
predicted results and real results.

9
00:00:32,760 --> 00:00:40,740
So what I expected you to do you know to be the most efficient as you can was to go to part two regression

10
00:00:41,070 --> 00:00:47,640
and then into the multiple in our regression further and then open this MULTIPLE LINEAR REGRESSION implementation

11
00:00:48,090 --> 00:00:55,800
which indeed contains insights so I already opened it that to allowing to display the two vectors of

12
00:00:56,070 --> 00:00:58,170
predicted results and real results.

13
00:00:58,170 --> 00:01:00,810
I'm talking of course about this tool right.

14
00:01:00,810 --> 00:01:02,790
We implemented it many times.

15
00:01:02,790 --> 00:01:08,130
That's why I didn't want to do it again here in our logistic regression implementation plus I really

16
00:01:08,130 --> 00:01:14,970
want to train you and incentivize you to be the most efficient as you can by juggling between your different

17
00:01:15,270 --> 00:01:16,760
models implementations.

18
00:01:16,890 --> 00:01:22,530
And so if you did this if you had the reflex to grab that tool right here in multiple in our regression

19
00:01:22,620 --> 00:01:25,500
or many other models where we implemented that.

20
00:01:25,650 --> 00:01:27,200
Well congratulations.

21
00:01:27,240 --> 00:01:28,370
You did amazing.

22
00:01:28,410 --> 00:01:30,710
All right so here we're gonna take this.

23
00:01:30,780 --> 00:01:33,110
And of course if you re implemented it yourself.

24
00:01:33,120 --> 00:01:39,410
That's also amazing especially if you manage to be more efficient than how we were about to be.

25
00:01:39,540 --> 00:01:39,950
All right.

26
00:01:39,960 --> 00:01:47,110
Because what we only have to do here is to copy this little piece of code you know that tool and paste

27
00:01:47,120 --> 00:01:54,510
it in the new code sell here to predict the test results because indeed we have all the same names here

28
00:01:54,510 --> 00:02:00,510
for the vector of predictions white bread which will be the result of the predict method applied to

29
00:02:00,510 --> 00:02:07,550
test set and called of course from our not regress or object but classifier object.

30
00:02:07,560 --> 00:02:08,510
There we go.

31
00:02:08,640 --> 00:02:10,630
That's the first change you had to make.

32
00:02:10,830 --> 00:02:16,890
And then well since this time you know our predicted purchase decisions and the real purchase decisions

33
00:02:17,010 --> 00:02:18,930
are either 0 or 1.

34
00:02:18,930 --> 00:02:24,630
Well we don't need to add anything here to you know forced a number of decimals after the comma to be

35
00:02:24,930 --> 00:02:25,690
only two.

36
00:02:25,710 --> 00:02:25,980
Right.

37
00:02:25,980 --> 00:02:27,920
Here we are only dealing with integers.

38
00:02:27,990 --> 00:02:29,130
So we can remove this.

39
00:02:29,160 --> 00:02:30,350
We don't need this.

40
00:02:30,450 --> 00:02:34,380
And then final question here do we have to change anything.

41
00:02:34,380 --> 00:02:35,980
Well absolutely not.

42
00:02:36,000 --> 00:02:41,790
And that's what I mean by you know grabbing a tool and applying it on your numeral by only having to

43
00:02:41,790 --> 00:02:43,230
change one or two things.

44
00:02:43,260 --> 00:02:48,820
Here we only changed the name of the moral type you know from aggressor to less fire.

45
00:02:49,440 --> 00:02:50,850
Okay so let's check it out.

46
00:02:50,850 --> 00:02:52,020
Let's see if it works.

47
00:02:52,050 --> 00:03:01,770
Let's press play here and indeed we get the two vectors next to each other with first on the left.

48
00:03:01,770 --> 00:03:07,370
Your vector of predictions you know of the predicted purchase decisions for all the customers.

49
00:03:07,370 --> 00:03:09,480
Of course the test set right.

50
00:03:09,480 --> 00:03:15,690
This was applied to excess here so that all the customers of the test set and on the right in the second

51
00:03:15,690 --> 00:03:19,500
column you have the real purchasing decisions.

52
00:03:19,530 --> 00:03:24,540
And so here what's interesting to see is to compare the predicted purchase decisions to the real ones

53
00:03:24,720 --> 00:03:27,360
for all the customers in the test.

54
00:03:27,360 --> 00:03:30,050
All right so let's see for the first customer of the test.

55
00:03:30,060 --> 00:03:34,490
You know remember of age 30 an estimated salary eighty seven thousand dollars.

56
00:03:34,550 --> 00:03:42,060
Well the prediction is no this customer didn't buy the new SUV and the real result is indeed no.

57
00:03:42,090 --> 00:03:45,370
In reality that customer didn't buy the new SUV.

58
00:03:45,420 --> 00:03:46,140
Good.

59
00:03:46,140 --> 00:03:47,760
Same for the second customer.

60
00:03:47,820 --> 00:03:53,260
That customer was predicted not to buy that new SUV and indeed it did not buy that new SUV.

61
00:03:53,280 --> 00:03:59,940
Third customer this time the third customer actually bought that new SUV and our model predicted that

62
00:03:59,940 --> 00:04:02,340
indeed this new customer bought it.

63
00:04:02,340 --> 00:04:05,340
Well it's funny we actually have a lot of correct predictions.

64
00:04:05,340 --> 00:04:06,720
That's amazing right.

65
00:04:06,720 --> 00:04:08,720
All this so far is correct.

66
00:04:08,760 --> 00:04:09,900
This is correct.

67
00:04:09,930 --> 00:04:10,740
And here we go.

68
00:04:10,740 --> 00:04:13,800
We have our first incorrect prediction here.

69
00:04:13,800 --> 00:04:20,130
Our logistic regression model predicted that this particular customer didn't buy the SUV because we

70
00:04:20,130 --> 00:04:21,840
have a prediction of zero here.

71
00:04:21,840 --> 00:04:26,380
But in reality that customer bought that new amazing SUV.

72
00:04:26,380 --> 00:04:26,650
OK.

73
00:04:26,660 --> 00:04:28,920
Because the real result here is a one.

74
00:04:28,950 --> 00:04:30,360
Then here it is correct correct.

75
00:04:30,360 --> 00:04:31,320
And here we go.

76
00:04:31,320 --> 00:04:38,040
Another incorrect prediction where our model predicted again that this customer didn't buy the SUV whereas

77
00:04:38,040 --> 00:04:41,310
in reality that customer bought the new SUV.

78
00:04:41,310 --> 00:04:41,700
All right.

79
00:04:41,730 --> 00:04:47,730
You see so that looks really really good actually we will get a very nice confusion matrix I will explain

80
00:04:47,750 --> 00:04:54,630
very soon what it is and mostly a very good accuracy because the accuracy in the test that of course

81
00:04:54,720 --> 00:05:01,650
is simply the number of correct predictions divided by the total number of observations in the test.

82
00:05:01,830 --> 00:05:05,120
And this is exactly what we're about to get in the dessert.

83
00:05:05,130 --> 00:05:08,530
We will not only get the confusion matrix showing.

84
00:05:08,550 --> 00:05:09,310
So there you go.

85
00:05:09,300 --> 00:05:10,740
I'm about to expand what it is.

86
00:05:10,740 --> 00:05:16,740
The confusion matrix will show us exactly the number of correct predictions and the number of incorrect

87
00:05:16,740 --> 00:05:23,520
predictions for the two cases where the real result was 0 0 1 2 basically will have a nice matrix showing

88
00:05:23,730 --> 00:05:26,790
how many mistakes and correct predictions are moral made.

89
00:05:26,940 --> 00:05:35,040
And of course inside the same new step or code cell we will compute the accuracy and we will see what

90
00:05:35,040 --> 00:05:39,770
is the percentage of correct predictions or moral made on the test.

91
00:05:39,810 --> 00:05:42,570
So should I ask you to try to do it on your own.

92
00:05:42,570 --> 00:05:43,890
Well yes why not.

93
00:05:43,890 --> 00:05:49,950
Because you know once again you just have to go to the API of psychic learn and figure out how to make

94
00:05:49,950 --> 00:05:53,150
that confusion matrix and how to compute the accuracy.

95
00:05:53,200 --> 00:05:54,770
And now I'll just give you a little hint.

96
00:05:54,840 --> 00:06:01,670
You will have to look into the metrics module from to learn and then I'm not telling you anything else

97
00:06:01,680 --> 00:06:04,450
you can really find the tools you need with that head.

98
00:06:05,160 --> 00:06:10,350
Okay so we'll implement a solution together in the next story or I will show you how to navigate that

99
00:06:10,390 --> 00:06:12,960
cycle or an API to find what we want.

100
00:06:12,960 --> 00:06:14,840
And until then enjoy machine learning.
