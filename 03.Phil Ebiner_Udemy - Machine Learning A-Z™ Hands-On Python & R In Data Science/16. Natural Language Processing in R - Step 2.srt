1
00:00:00,300 --> 00:00:07,370
Hello and welcome to this art Tauriel soon to be used to toil report the dataset which contains 1000

2
00:00:07,380 --> 00:00:12,780
reviews of a restaurant and for each of the review we have information whether the review is positive

3
00:00:12,930 --> 00:00:13,790
or negative.

4
00:00:13,980 --> 00:00:20,450
So one when the review is positive and zero when the reviews negative and we are trying to build a machine

5
00:00:20,450 --> 00:00:26,580
or any model that we'll be able to classify each new review and tell if this new review is positive

6
00:00:26,670 --> 00:00:27,560
or negative.

7
00:00:27,870 --> 00:00:33,520
So basically what we're led to do is something we already did that is classification in part 3.

8
00:00:33,750 --> 00:00:39,330
But this time we are working on text and therefore we need to figure out a way to create a model where

9
00:00:39,330 --> 00:00:45,540
we can have some independent variables to train a machine and classification model to learn some correlations

10
00:00:45,540 --> 00:00:51,120
between the independent variables and the dependent variable which is of course the liked column.

11
00:00:51,120 --> 00:00:57,840
So now the goal is simply to create some independent variables and what could be those independent variables.

12
00:00:57,840 --> 00:01:05,400
Well the idea in natural language processing in the back of our model is to create a model which will

13
00:01:05,400 --> 00:01:10,700
basically be a huge table a table with a rose or nothing else in the reviews.

14
00:01:10,770 --> 00:01:17,490
So this table will have 1000 rows because we have 1000 reviews one row for each review and the columns

15
00:01:17,850 --> 00:01:22,020
will simply be all the words that we can find in the reviews.

16
00:01:22,170 --> 00:01:26,630
You know we take the 1000 reviews we look at all the words and those 1000 reviews.

17
00:01:26,820 --> 00:01:32,790
And we're going to create one column for each word and then each cell in the table will correspond to

18
00:01:32,790 --> 00:01:33,320
one row.

19
00:01:33,330 --> 00:01:39,260
There is one review and one column that is one word in all these words and these 1000 views.

20
00:01:39,390 --> 00:01:43,830
And then this there is going to be the number of times the word appears in the review.

21
00:01:43,980 --> 00:01:46,510
So for example here is the first word wow.

22
00:01:46,740 --> 00:01:50,160
Well there's going to be a column for this while we're here.

23
00:01:50,340 --> 00:01:53,370
And so for the first line that corresponds to the first review.

24
00:01:53,520 --> 00:01:59,940
Well this well column will get a one because the word WOW appears once in the first review but then

25
00:01:59,940 --> 00:02:05,180
for all the other reviews that is all the other rows we don't see any Wow here so all the cells in this

26
00:02:05,190 --> 00:02:08,300
first column for the other rows will get a zero.

27
00:02:08,490 --> 00:02:11,690
And that's what will be our model in the end.

28
00:02:11,910 --> 00:02:17,810
It's going to be this huge table with 1000 rows that are going to be the reviews and a lot of columns

29
00:02:17,810 --> 00:02:22,760
that are going to be all the worse that we can find in this 1000 reviews here.

30
00:02:22,770 --> 00:02:28,380
So now you might start to figure out why we would need to clean the reviews it's because since we're

31
00:02:28,380 --> 00:02:31,170
going to take all the worst in these reviews here.

32
00:02:31,350 --> 00:02:37,110
Well we're going to get a lot of columns because we create one column for each word but we don't want

33
00:02:37,110 --> 00:02:42,040
to get too many columns because the more we get some Collins and the harder it will be for our machine

34
00:02:42,040 --> 00:02:45,510
or any model to run properly to execute efficiently.

35
00:02:45,510 --> 00:02:50,760
Not only our machine model will have more trouble to execute properly but also it will have more trouble

36
00:02:50,760 --> 00:02:55,410
understanding the correlations between the presence of the words and the reviews and the information

37
00:02:55,410 --> 00:03:00,810
whether the review is positive or negative because of course if we keep all the words in these reviews

38
00:03:01,200 --> 00:03:06,600
Well we will get some irrelevant words some words that will not help the machine learning algorithm

39
00:03:06,840 --> 00:03:12,700
to predict if is positive or negative because in other words that we can find in the reviews here.

40
00:03:12,900 --> 00:03:17,730
Well some words give a much better hint in telling if the review is positive or negative.

41
00:03:17,730 --> 00:03:19,480
Let me give you a simple example.

42
00:03:19,500 --> 00:03:23,210
We have this Loved word here in this review.

43
00:03:23,220 --> 00:03:28,830
This is the word that basically tells us that the reason was positive because there is this love word.

44
00:03:28,980 --> 00:03:35,430
But then if we look at this this word or even place where these two words don't give the machine learning

45
00:03:35,430 --> 00:03:38,890
algorithm a hint whether the reviews positive or negative.

46
00:03:39,010 --> 00:03:45,120
It's of course this word loved by which the machine learning algorithm will understand some correlations

47
00:03:45,120 --> 00:03:49,200
between this love word and the fact that the review is positive.

48
00:03:49,530 --> 00:03:54,720
So that's the whole reason why right now we are going to clean the text.

49
00:03:54,750 --> 00:04:00,540
It's not only to reduce the big table we're going to get to in the end because we want our algorithm

50
00:04:00,540 --> 00:04:03,100
to run properly and not be saturated.

51
00:04:03,480 --> 00:04:09,330
And the other reason is that we want to get the most relevant words to find the best correlations between

52
00:04:09,330 --> 00:04:13,830
the presence of the words and the outcome whether the review is positive or negative.

53
00:04:13,830 --> 00:04:15,720
All right so now we get the point.

54
00:04:15,840 --> 00:04:20,880
So let's start cleaning the reviews and an important thing to understand is that's what we'll do here

55
00:04:20,880 --> 00:04:21,980
to clean the reviews.

56
00:04:22,140 --> 00:04:26,010
Well it's the same technique to clean any other kind of text.

57
00:04:26,220 --> 00:04:31,380
Well I will give you the main tools you will be able to use these tools to clean any text you're working

58
00:04:31,380 --> 00:04:32,010
with.

59
00:04:32,010 --> 00:04:37,920
And of course if your text is a little more complicated like for example NHT pages that contains HDMI

60
00:04:37,920 --> 00:04:38,520
tags.

61
00:04:38,760 --> 00:04:42,990
Well you would need to add a little more tools but you would still use the tools that we are about to

62
00:04:42,990 --> 00:04:43,520
use.

63
00:04:43,620 --> 00:04:48,870
And the good news is that if you want to use more tools to clean more sophisticated text Well you just

64
00:04:48,870 --> 00:04:50,000
need to ask me some questions.

65
00:04:50,020 --> 00:04:54,620
Nick you and I and now help you at these tools to your problem and your text.

66
00:04:54,870 --> 00:05:00,870
But what we'll do here you will definitely do it to perform natural language processing your text files

67
00:05:00,870 --> 00:05:02,350
for your problems.

68
00:05:02,370 --> 00:05:05,130
All right so let's do it let's clean the text.

69
00:05:05,150 --> 00:05:07,690
So that's the next step in natural language processing.

70
00:05:07,830 --> 00:05:13,920
We will clean all the text and then we will create our bag of words model which is this huge table which

71
00:05:13,920 --> 00:05:19,620
is actually called a sparse matrix because we'll get a lot of zeroes in the sparse matrix and then that

72
00:05:19,620 --> 00:05:24,630
means that we'll get a model where we have some independent variables and one dependent variable and

73
00:05:24,630 --> 00:05:30,060
that's when we'll be able to use our machinery intensification morals that were built in part three

74
00:05:30,330 --> 00:05:35,350
to predict the class of a new review that the model will not have seen yet.

75
00:05:35,460 --> 00:05:36,790
So let's do it.

76
00:05:36,870 --> 00:05:40,220
Let's start with the first step of cleaning the text.

77
00:05:40,530 --> 00:05:46,920
And this first step is going to be about initializing a corpus because you know we will not clean the

78
00:05:46,920 --> 00:05:49,320
reviews directly in the data set.

79
00:05:49,320 --> 00:05:55,290
We will instead create a corpus which will contain all the reviews and that will be in this corpus that

80
00:05:55,290 --> 00:05:57,180
will clean all the reviews.

81
00:05:57,180 --> 00:05:59,360
So let's start by creating this corpus.

82
00:05:59,550 --> 00:06:04,770
And in order to create this Corvis we need to import a package and you might need to install it if it's

83
00:06:04,770 --> 00:06:07,290
the first time you're doing natural language processing.

84
00:06:07,290 --> 00:06:13,170
So I'm going to type here the command to install this package this package is called the T.M. package.

85
00:06:13,170 --> 00:06:16,950
It's a very famous package in our natural language processing.

86
00:06:16,980 --> 00:06:21,630
So let's install this package by typing install packages.

87
00:06:21,630 --> 00:06:22,440
Here it is.

88
00:06:22,650 --> 00:06:29,280
And so the name of the package has to be input in quotes which is the tea package.

89
00:06:29,280 --> 00:06:30,030
All right.

90
00:06:30,240 --> 00:06:38,250
So if I go to my packages I will find the test package here it is.

91
00:06:38,380 --> 00:06:41,430
So I already have it installed so I don't need to install it again.

92
00:06:41,560 --> 00:06:44,690
So I will put that in comment.

93
00:06:44,710 --> 00:06:45,480
Here we go.

94
00:06:45,610 --> 00:06:51,160
And of course if you don't have the team package here you will need to install it by executing this

95
00:06:51,160 --> 00:06:53,820
line and everything will run properly.

96
00:06:53,830 --> 00:06:54,500
All right.

97
00:06:54,850 --> 00:06:59,410
So of course after we install the package we need to import the package.

98
00:06:59,410 --> 00:07:04,510
Well mine is already imported but we need to automate all this so as usual we are going to take the

99
00:07:04,510 --> 00:07:09,630
library command and in parenthesis we input the name of the package.

100
00:07:09,640 --> 00:07:12,090
So T.M. again not in quotes.

101
00:07:12,100 --> 00:07:12,850
All right.

102
00:07:12,970 --> 00:07:16,510
And now we are ready to build the corpus.

103
00:07:16,810 --> 00:07:18,700
And so how are we going to call our corpus.

104
00:07:18,790 --> 00:07:20,900
We're going to call it corpus.

105
00:07:20,900 --> 00:07:27,100
All right corpus equals and then we're going to use the corpus function spelled this way the corpus

106
00:07:27,100 --> 00:07:34,840
with capital V in capital C and then in parenthesis we need to input vector source this way.

107
00:07:34,840 --> 00:07:42,080
And again new parenthesis and it's in parenthesis that we input the column that contains the text that

108
00:07:42,100 --> 00:07:44,450
we want to clean in this corpus.

109
00:07:44,650 --> 00:07:48,630
So this column is of course the review column of our dataset.

110
00:07:48,640 --> 00:07:55,410
So here we will put this column which can be taken the following way by typing data set dollar sign.

111
00:07:55,420 --> 00:08:00,790
And then as you can see we have the two variables here that we can pick and the one we want to pick

112
00:08:00,850 --> 00:08:01,980
is review.

113
00:08:02,080 --> 00:08:02,580
All right.

114
00:08:02,580 --> 00:08:07,850
And that takes all the review column of our dataset and that's exactly what we want.

115
00:08:07,960 --> 00:08:12,890
Because this column contains the text and we want to clean the text in the corpus.

116
00:08:12,910 --> 00:08:15,800
All right so that's the first step of cleaning the text.

117
00:08:15,850 --> 00:08:21,280
So let's attack this and press command and control press enter to execute and create the corpus.

118
00:08:21,280 --> 00:08:22,280
Here we go.

119
00:08:22,390 --> 00:08:28,570
As you can see it already says that it's a large corpus and we can even see here that our corpus takes

120
00:08:28,690 --> 00:08:31,030
3.7 megabytes of space.

121
00:08:31,100 --> 00:08:35,070
We will simplify this corpus by cleaning Step-By-Step all the reviews.

122
00:08:35,200 --> 00:08:39,300
And that's what we'll do in the next tutorials until then and machine learning
