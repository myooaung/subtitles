WEBVTT
1
00:00:00.360 --> 00:00:05.140
Hello and welcome to this art Tauriel and welcome to part nine dimensional reduction.

2
00:00:05.160 --> 00:00:11.700
So we are starting this part with our first technique of dimensionality reduction which is PCA principal

3
00:00:11.700 --> 00:00:13.180
component analysis.

4
00:00:13.380 --> 00:00:18.900
And so you know in dimensionality reduction there are two techniques feature selection and feature extraction.

5
00:00:18.900 --> 00:00:24.720
We did feature selection in Part 2 when we implemented the backward elimination model to select the

6
00:00:24.720 --> 00:00:29.730
most relevant features of our matrix of features that is the features that explain the most.

7
00:00:29.730 --> 00:00:34.470
The dependent variable and now we are starting this new technique of dimensionality reduction which

8
00:00:34.470 --> 00:00:41.020
is feature extraction and PCA principal component analysis is one feature extraction technique.

9
00:00:41.190 --> 00:00:47.460
So as a reminder let's say your matrix of features has an independent variables Well what PCA will do

10
00:00:47.460 --> 00:00:51.880
is that it will extract a smaller number of your independent variables.

11
00:00:51.960 --> 00:00:57.390
But there are going to be new independent variables like new dimensions and these new independent variables

12
00:00:57.390 --> 00:01:02.730
extracted are going to be some new independent variables that explain the most the variance of your

13
00:01:02.730 --> 00:01:03.420
data set.

14
00:01:03.630 --> 00:01:06.380
And that is regardless of your dependent variable.

15
00:01:06.570 --> 00:01:13.080
And that makes speciate an unsupervised mode in the sense that we don't consider the dependent variable

16
00:01:13.200 --> 00:01:14.330
in the model.

17
00:01:14.370 --> 00:01:15.450
So that's BCA.

18
00:01:15.510 --> 00:01:20.020
And remember in Part 2 in part 3 we worked with one or two independent variables.

19
00:01:20.160 --> 00:01:22.470
Well that was for two specific purposes.

20
00:01:22.470 --> 00:01:26.740
The first purpose is that we needed a graphic visualization of our results.

21
00:01:26.850 --> 00:01:32.370
And since each independent variable corresponded to one dimension in the plot well we could visualize

22
00:01:32.370 --> 00:01:35.700
our result with at most two independent variables.

23
00:01:35.700 --> 00:01:41.250
And the second reason is that thanks to this PCA dimensionality reduction technique well even if we

24
00:01:41.250 --> 00:01:43.990
have a lot of independent variables at the beginning.

25
00:01:44.220 --> 00:01:50.550
Well we can end up with much less independent variables but that's going to be relevant independent

26
00:01:50.550 --> 00:01:56.100
variables because these independent variables will explain the most the variance of your data.

27
00:01:56.370 --> 00:02:01.560
And therefore since we can reduce this number of independent variables Well we can end up with two or

28
00:02:01.560 --> 00:02:06.550
three independent variables and therefore visualize the results as we did in part 3.

29
00:02:06.750 --> 00:02:11.280
And this is exactly what we're going to do in this tutorial in the following tutorials of the section

30
00:02:11.490 --> 00:02:16.740
and the following sections when we cover other dimensionality reduction techniques like LDA and also

31
00:02:16.740 --> 00:02:22.140
Colombe say well we will have many features at the beginning and therefore it will be impossible to

32
00:02:22.140 --> 00:02:23.590
visualize the results.

33
00:02:23.610 --> 00:02:29.770
But then when we apply PCa or LDK we will reduce the number of features down to two and therefore we'll

34
00:02:29.790 --> 00:02:31.950
be able to visualize the results.

35
00:02:31.950 --> 00:02:36.520
So let's start right now and let's start by setting the right folder as working directory.

36
00:02:36.570 --> 00:02:38.220
So as you know we go to our machine.

37
00:02:38.260 --> 00:02:41.870
It is a folder then part nine dimensional reduction.

38
00:02:42.060 --> 00:02:46.540
And here we are in the first section of this part nine principal components analysis.

39
00:02:46.560 --> 00:02:48.000
That's our first technique.

40
00:02:48.000 --> 00:02:52.350
Let's click on it and that's the folder we want to set as a working directory make sure that you have

41
00:02:52.350 --> 00:02:53.930
the one that says you file.

42
00:02:53.940 --> 00:02:58.470
And if that's the case you're ready to click on this more button here to set this folder as working

43
00:02:58.470 --> 00:03:00.260
directory perfect.

44
00:03:00.480 --> 00:03:06.720
And now we're going to open another file that we made in part 3 classification which is our logistic

45
00:03:06.840 --> 00:03:07.570
regression.

46
00:03:07.620 --> 00:03:13.980
Our file because what we're going to do is take this logistic regression code then we are going to change

47
00:03:13.980 --> 00:03:19.340
the name of the data set because we will be working on a new data set which will be the wind that says

48
00:03:19.350 --> 00:03:23.290
you file and we will apply PCA on this data set.

49
00:03:23.460 --> 00:03:26.650
And of course I will explain quickly the business problem behind it.

50
00:03:26.880 --> 00:03:31.420
So I'm going to take everything from here down to the bottom.

51
00:03:31.440 --> 00:03:32.300
Here we go.

52
00:03:32.350 --> 00:03:38.280
Copy and I'm going to paste that in my PCa our file this way.

53
00:03:38.280 --> 00:03:41.670
So let's go up and let's change the name of the data set.

54
00:03:41.670 --> 00:03:44.430
This is not social network at Sears V.

55
00:03:44.550 --> 00:03:48.900
This is now one that says that's perfect.

56
00:03:49.080 --> 00:03:54.720
So no what we're going to do is first import the data set and then apply data processing maybe we will

57
00:03:54.720 --> 00:03:59.790
need to change some things like the indexes here but this will be very quick.

58
00:03:59.790 --> 00:04:02.110
So first let's import the data sets.

59
00:04:02.160 --> 00:04:07.460
So I'm going to select this line and execute it we go the data set is one important.

60
00:04:07.530 --> 00:04:08.330
Here it is.

61
00:04:08.430 --> 00:04:10.970
And now let's expand the business problem behind it.

62
00:04:11.160 --> 00:04:16.290
Okay so first of all this is a very famous data set well known in the machine or in literature and that

63
00:04:16.290 --> 00:04:21.960
you can find on the UCI machine or any repository as you can see here and you can find this page at

64
00:04:21.960 --> 00:04:22.630
this link.

65
00:04:23.480 --> 00:04:27.990
So basically first what are the independent variables and what is the dependent variable.

66
00:04:28.190 --> 00:04:34.730
Well the independent variables are all the variables from this one alcohol up to this one.

67
00:04:35.000 --> 00:04:39.440
And this last variable customer segment is the dependent variable.

68
00:04:39.530 --> 00:04:44.600
So in the original data set this the parent variable is not called customer segment this is actually

69
00:04:44.690 --> 00:04:46.220
the origin of the wine.

70
00:04:46.310 --> 00:04:51.250
But let's imagine that we as data scientist are working for a wine business owner.

71
00:04:51.440 --> 00:04:55.360
And this one business owner gathered all these informations in this data set.

72
00:04:55.370 --> 00:05:00.130
And so first what this business owner did is that it gathered all the information of these independent

73
00:05:00.140 --> 00:05:04.800
variables here that are chemical's informations of several wines.

74
00:05:05.090 --> 00:05:11.360
And this business owner applied some clustering technique to find some segments of customers that like

75
00:05:11.510 --> 00:05:17.520
specific wine depending on the information of the wine and by applying these clustering techniques.

76
00:05:17.600 --> 00:05:20.970
This business owner identified three segments of customers.

77
00:05:21.020 --> 00:05:22.390
That's the first one here.

78
00:05:22.460 --> 00:05:23.870
Then we have the second one.

79
00:05:23.990 --> 00:05:30.310
And eventually the third one so based on these informations and thanks to its clustering techniques.

80
00:05:30.430 --> 00:05:36.160
Well this one business owner managed to find some segments of customers each segment having a specific

81
00:05:36.160 --> 00:05:38.390
preference for a specific wine.

82
00:05:38.590 --> 00:05:41.780
So basically this business only found three types of wines.

83
00:05:41.950 --> 00:05:47.410
Each type of one corresponding to one segment of customers and therefore three segments of customers.

84
00:05:47.560 --> 00:05:50.000
And why does it create added value for its business.

85
00:05:50.140 --> 00:05:55.030
Well that's because now what this business owner can do is take all these informations of the wines

86
00:05:55.270 --> 00:06:00.940
as well as the information about the customer segments and make a classification model like logistic

87
00:06:00.940 --> 00:06:05.790
regression in which the independent variables are all these variables and that have been invaluable

88
00:06:05.960 --> 00:06:12.550
in the customer segment and therefore for each new wine it can predict to which customer segment it

89
00:06:12.550 --> 00:06:13.930
should recommend this one.

90
00:06:14.170 --> 00:06:16.680
So that adds a lot of value for this business owner.

91
00:06:16.750 --> 00:06:21.520
But then if this business owner wants to have a clear visual look at the prediction regions and the

92
00:06:21.520 --> 00:06:26.680
prediction boundary of the classification model that we're going to build to be able to see if the predictions

93
00:06:26.710 --> 00:06:29.400
are in the right spot of the customer segments.

94
00:06:29.500 --> 00:06:35.110
Well it cannot be done with all these independent variables because of course we cannot represent these

95
00:06:35.110 --> 00:06:38.580
many independent variables in one plot that's impossible.

96
00:06:38.590 --> 00:06:44.650
So what we need to do is apply some dimensionality reduction techniques to extract two independent variables

97
00:06:44.650 --> 00:06:50.110
that explain the most the variance and then we'll be able to see the prediction regions and the prediction

98
00:06:50.110 --> 00:06:55.510
boundary and therefore will clearly be able to see where the customer segments are and where are these

99
00:06:55.510 --> 00:06:57.690
predictions of the customer segments are.

100
00:06:57.910 --> 00:07:03.460
According to the extracted features of all the informations of our independent variables and remember

101
00:07:03.460 --> 00:07:07.360
these extracted features are called the principal components.

102
00:07:07.360 --> 00:07:12.550
All right so now that we understand the challenge and the business problem let's apply PCA to see how

103
00:07:12.550 --> 00:07:18.010
we can reduce the dimensionality of this dataset because indeed it contains 13 dimensions because it

104
00:07:18.010 --> 00:07:24.730
contains 13 independent variables and we'll see how we can use PCA to reduce the number of independent

105
00:07:24.730 --> 00:07:27.280
variables down to two independent variables.

106
00:07:27.280 --> 00:07:28.340
But be careful.

107
00:07:28.420 --> 00:07:33.760
It's important to understand that the new two independent variables that we'll have in the end are going

108
00:07:33.760 --> 00:07:39.070
to be new ones as opposed to feature selection where you know we end up with two independent variables

109
00:07:39.280 --> 00:07:45.970
that are among these original 13 independent variables here with PCa will get you extractors one and

110
00:07:45.970 --> 00:07:51.220
that's the important distinction to make between feature selection and feature extraction.

111
00:07:51.220 --> 00:07:57.340
All right so before we apply pca as usual we need to preprocess the data and this is actually going

112
00:07:57.340 --> 00:08:02.690
to be very quick because our template is ready we will just need to change just a few things.

113
00:08:02.710 --> 00:08:09.190
So first dataset equals There is a 3:5 that's just to select the independent variables that matter for

114
00:08:09.190 --> 00:08:09.980
our problem.

115
00:08:10.090 --> 00:08:14.640
But here everything matters we just want to reduce the dimensionality of this data set.

116
00:08:14.680 --> 00:08:19.490
So we will keep all our independent variables here and therefore we don't need this slide here.

117
00:08:19.510 --> 00:08:21.880
So I will just remove it.

118
00:08:22.330 --> 00:08:22.690
OK.

119
00:08:22.690 --> 00:08:25.870
So first section and in the days that ready well executed.

120
00:08:25.870 --> 00:08:27.520
Now let's move on to the next section.

121
00:08:27.830 --> 00:08:32.230
So the next section is about splitting the data sets into the training set and the test set.

122
00:08:32.560 --> 00:08:39.700
And here be careful we just need to change this name of the dependent variable because in the disaggregation

123
00:08:39.700 --> 00:08:44.830
we're dealing with the social network at the file and the dependent variable was purchased.

124
00:08:44.890 --> 00:08:49.210
But now for our new business problem the dependent variable is not called purchased.

125
00:08:49.210 --> 00:08:51.330
It is called customer segment.

126
00:08:51.400 --> 00:08:56.290
So we just need to replace purchase here by customer segment.

127
00:08:56.290 --> 00:08:56.910
Here we go.

128
00:08:57.790 --> 00:09:01.130
All right do we keep a split ratio of 75 percent.

129
00:09:01.210 --> 00:09:04.440
Well let's rather take 80 percent.

130
00:09:04.450 --> 00:09:09.010
But that's as you wanted just that 80 percent is a good split racial take.

131
00:09:09.040 --> 00:09:10.450
So we will go with that.

132
00:09:10.450 --> 00:09:13.650
And then here for training set and test that we don't need to change anything.

133
00:09:13.750 --> 00:09:18.110
So we are ready to split our data set into the training set and the assets.

134
00:09:18.130 --> 00:09:19.110
Let's do it.

135
00:09:19.180 --> 00:09:25.490
I'm going to select all the section here and press command plus enter to execute.

136
00:09:25.510 --> 00:09:26.300
Here we go.

137
00:09:26.560 --> 00:09:32.000
The training set is now created as well as a test at great.

138
00:09:32.140 --> 00:09:38.440
So ready to move on to the next section the next section is about feature scaling and for PCa it is

139
00:09:38.440 --> 00:09:44.550
way better to apply future scaling you can actually apply it by playing with the parameters of the PCA

140
00:09:44.560 --> 00:09:46.970
function that we're going to use afterwards.

141
00:09:47.000 --> 00:09:52.990
But let's take this feature scanning part of our code template to put our features on the same scale.

142
00:09:53.020 --> 00:09:56.100
So here we just need to change the indexes.

143
00:09:56.200 --> 00:10:00.420
We actually need to specify indexes of the features we want to scale.

144
00:10:00.430 --> 00:10:06.330
So basically the features we want to scale are all the features from alcohol to Proline.

145
00:10:06.340 --> 00:10:11.680
And so what we can do is specify that we want to get all the variables except the last one customer

146
00:10:11.680 --> 00:10:13.960
segment that has indexed 14.

147
00:10:14.140 --> 00:10:22.270
So therefore here instead of putting indexes of the features we can replace it by minus 14 we can remove

148
00:10:22.270 --> 00:10:22.850
that.

149
00:10:22.870 --> 00:10:26.800
Let's copy this because we will do the same for the others.

150
00:10:26.830 --> 00:10:35.410
So lets replace that here by minus 14 and minus 14 here as well and eventually minus 14.

151
00:10:35.410 --> 00:10:35.740
All right.

152
00:10:35.740 --> 00:10:38.020
Now the feed just getting part is ready.

153
00:10:38.080 --> 00:10:44.290
So we are ready to select the section and press commander control enter to execute.

154
00:10:44.380 --> 00:10:49.930
And now all our valuables are skilled as you can see we can clearly see that all our features are on

155
00:10:49.930 --> 00:10:50.940
the same scale.

156
00:10:51.100 --> 00:10:55.470
And of course the customer segments kept its labels 1 2 and 3.

157
00:10:55.930 --> 00:10:58.020
And same for the test set lets make sure that.

158
00:10:58.090 --> 00:10:58.440
All right.

159
00:10:58.450 --> 00:10:59.520
Perfect.

160
00:10:59.530 --> 00:11:01.340
So just getting done.

161
00:11:01.370 --> 00:11:04.960
And actually the preprocessing phase is completed.

162
00:11:04.960 --> 00:11:07.170
So we did that quite efficiently.

163
00:11:07.250 --> 00:11:12.200
Thats good because now we're getting to the exciting part applying PCA to our data.

164
00:11:12.220 --> 00:11:15.010
So actually we will do that right here.

165
00:11:15.120 --> 00:11:17.920
You apply BCA right after the data processing phase.

166
00:11:18.100 --> 00:11:23.170
And just before you fit you know disaggregation model to the training said because of course you want

167
00:11:23.170 --> 00:11:27.670
to train your model on your new data sets with the new extracted features.

168
00:11:27.670 --> 00:11:31.230
That is with the two new extracted features that will explain the most variants.

169
00:11:31.450 --> 00:11:36.720
And after you trained your grassfire you're ready to predict the test results make the commission metrics.

170
00:11:36.860 --> 00:11:39.280
Then you can also visualize the trends that results.

171
00:11:39.280 --> 00:11:43.090
Remember the section is applied on a dataset that contains two features.

172
00:11:43.260 --> 00:11:47.050
And so we will see what we get by extracting these two new features.

173
00:11:47.050 --> 00:11:52.870
All right so to finish the story I'm just going to introduce this new section here that I'm going to

174
00:11:52.870 --> 00:11:56.550
call applying PCA.

175
00:11:56.560 --> 00:11:57.010
All right.

176
00:11:57.040 --> 00:12:03.220
And then the next tutorial we're going to apply PCa and then eventually we will build our moral on our

177
00:12:03.220 --> 00:12:04.520
new reduced days.

178
00:12:04.540 --> 00:12:06.720
So I look forward to doing that in the next tutorial.

179
00:12:06.750 --> 00:12:08.440
And until then enjoy machine learning.
