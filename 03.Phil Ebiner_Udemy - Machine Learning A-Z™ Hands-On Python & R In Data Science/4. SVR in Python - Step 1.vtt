WEBVTT
1
00:00:00.300 --> 00:00:07.210
Hello my friends and welcome to this new practical activity on this time Support Vector regression.

2
00:00:07.530 --> 00:00:13.920
So have to start by telling you that the model we're about to implement will be slightly more advanced

3
00:00:14.130 --> 00:00:16.900
than the models were built before but it's totally fine.

4
00:00:16.950 --> 00:00:19.970
We will do it together and we will succeed 100 percent.

5
00:00:20.460 --> 00:00:22.890
So why is it slightly more advanced.

6
00:00:22.980 --> 00:00:28.920
Well that's because we will have to play a lot with feature scaling and I can tell you that after this

7
00:00:28.950 --> 00:00:34.830
implementation of the as we are moral you will be a master in feature scaling because you will know

8
00:00:35.010 --> 00:00:41.160
not only how to apply the feature scaling transformation but also how to apply the inverse transformation

9
00:00:41.190 --> 00:00:43.740
you know to go back to the original scaling.

10
00:00:44.010 --> 00:00:49.970
So you will know all the sap tools that say a feature scaling and you will handle them like a pro.

11
00:00:50.000 --> 00:00:50.550
All right.

12
00:00:50.700 --> 00:00:53.090
So if you're ready let's start.

13
00:00:53.130 --> 00:00:58.810
And just before we go Inside the folder let us make sure here that we are all on the same page.

14
00:00:58.830 --> 00:01:04.200
I gave you the link to this folder containing all the codes and data sets right before this tutorial

15
00:01:04.220 --> 00:01:06.720
and an Oracle so make sure to connect to it.

16
00:01:06.720 --> 00:01:12.660
And now we should all be on the same page and therefore we're going to go of course to Part 2 regression

17
00:01:12.930 --> 00:01:17.390
and then of course to support vector regression as we are.

18
00:01:17.880 --> 00:01:18.390
All right.

19
00:01:18.390 --> 00:01:23.340
And as usual we're gonna start with Python so we're gonna go to that Python folder inside which will

20
00:01:23.340 --> 00:01:30.030
find the same data set as before because you know I would like to compare the performance of different

21
00:01:30.030 --> 00:01:37.200
regression models on this dataset which proved to have some non-linear relationships between the feature

22
00:01:37.200 --> 00:01:43.760
which is the level you know the position level going from one to 10 and the salary going from 45000

23
00:01:43.800 --> 00:01:45.540
to one million dollars per year.

24
00:01:45.930 --> 00:01:52.050
So here again this is exactly the same scenario we want to train this time a support vector regression

25
00:01:52.200 --> 00:01:58.560
moral to learn and understand the correlations between these position levels and these salaries and

26
00:01:58.680 --> 00:02:07.020
to quickly reminded context we are hiring a new person who is expecting a 160 K salary justifying this

27
00:02:07.020 --> 00:02:12.420
by the fact that this person earned a 160 K salary in the previous company.

28
00:02:12.420 --> 00:02:17.250
This data is exactly the data of the previous company you know with the different position levels from

29
00:02:17.250 --> 00:02:20.590
business analyst to CEO and their corresponding salary.

30
00:02:20.730 --> 00:02:26.400
And so not only we want to train a moral to learn these relationships but also we want to deploy this

31
00:02:26.400 --> 00:02:32.760
model to predict the salary that this person had in this previous company knowing that indeed this person

32
00:02:32.910 --> 00:02:39.510
was a region manager for a couple years and therefore is considered to have a position level of six

33
00:02:39.510 --> 00:02:40.320
point five.

34
00:02:40.320 --> 00:02:43.880
So that's the exact same scenario exact same data set into now.

35
00:02:43.880 --> 00:02:50.820
Let us build the support vector regression model on this dataset to see if it performs better than the

36
00:02:50.820 --> 00:02:53.400
previous model which showed great results.

37
00:02:53.420 --> 00:02:55.440
The pulling them your regression model.

38
00:02:55.440 --> 00:02:55.920
All right.

39
00:02:55.920 --> 00:02:56.660
So let's do this.

40
00:02:56.670 --> 00:03:05.030
Let's close this and let's open our Support Vector regression implementation either with Google laboratory

41
00:03:05.060 --> 00:03:10.650
if you love it or was Jupiter notebook because indeed it is an IP y and B file.

42
00:03:10.690 --> 00:03:13.140
All right so that's the whole implementation.

43
00:03:13.140 --> 00:03:16.510
Let me quickly show you the structure of the implementation.

44
00:03:16.530 --> 00:03:22.170
We're gonna start by importing the libraries as you rule then importing the dataset then applying feature

45
00:03:22.170 --> 00:03:22.780
scaling.

46
00:03:22.890 --> 00:03:27.690
So this is interesting this time we have to apply feature scaling because in the as we are moral there

47
00:03:27.690 --> 00:03:33.970
is not this you know explicit equation of the dependent variable with respect to the features.

48
00:03:34.110 --> 00:03:40.890
And mostly there are not those coefficients multiplying each of the features and therefore not compensating

49
00:03:41.100 --> 00:03:44.340
with lower values for the features taking high values.

50
00:03:44.340 --> 00:03:44.930
No.

51
00:03:44.940 --> 00:03:51.900
This time the Support Vector regression model has an implicit equation of you know the dependent variable

52
00:03:51.960 --> 00:03:53.910
with respect to the features.

53
00:03:53.910 --> 00:04:00.670
So we don't have such coefficients and we will definitely have to apply features killing for this model.

54
00:04:00.690 --> 00:04:05.340
So you see you start to understand when and when not to apply which is killing.

55
00:04:05.340 --> 00:04:10.350
Well you know we don't have to apply feature scaling for linear regression models where you have indeed

56
00:04:10.350 --> 00:04:14.430
those coefficients that can compensate with the high values of the features.

57
00:04:14.430 --> 00:04:19.170
And so of course these include simple in our regression multiple in our regression and pulling them

58
00:04:19.170 --> 00:04:26.400
all regression and you will see later on in of course many other models and for other models which usually

59
00:04:26.400 --> 00:04:33.840
have an implicit equation you know an implicit relationship between the dependent variable Y and the

60
00:04:33.840 --> 00:04:35.020
features X..

61
00:04:35.070 --> 00:04:41.130
Well usually for these models we have to apply features killing all right then we will of course trained

62
00:04:41.160 --> 00:04:42.920
as we are model on the whole dataset.

63
00:04:42.950 --> 00:04:44.340
You know this time is the same.

64
00:04:44.340 --> 00:04:50.160
We won't split the whole dataset into a training set in a test because we want to leverage the maximum

65
00:04:50.160 --> 00:04:56.520
data to learn these correlations between those position levels and the salaries so we won't do that

66
00:04:56.520 --> 00:05:02.220
split and we will directly train that as we are moral on the whole is it then after this training well

67
00:05:02.220 --> 00:05:08.490
we'll have a smart as your model which therefore we're going to use to predict this new result and exactly

68
00:05:08.790 --> 00:05:12.510
that salary of this position level of six point five.

69
00:05:12.600 --> 00:05:17.430
And we will compare of course that prediction with the prediction of the pulling them your regression

70
00:05:17.430 --> 00:05:23.730
model which I've kept here and then of course we will visualized as we are results in low resolution

71
00:05:23.730 --> 00:05:25.650
and high resolution to see.

72
00:05:25.650 --> 00:05:29.260
Of course the regression curve of the SVR model.

73
00:05:29.280 --> 00:05:29.760
All right.

74
00:05:29.760 --> 00:05:33.700
I'm sure you notice that I did not click on each of these contents.

75
00:05:33.720 --> 00:05:34.850
Well I did that on purpose.

76
00:05:34.860 --> 00:05:37.710
It's because I don't want to reveal the prediction.

77
00:05:37.720 --> 00:05:40.510
You know the predicted salary by the SVR model.

78
00:05:40.590 --> 00:05:46.800
I want us to save the surprise until you know we execute that cell to return that predicted salary.

79
00:05:46.800 --> 00:05:47.150
All right.

80
00:05:47.160 --> 00:05:48.720
Let's keep the suspense.

81
00:05:48.720 --> 00:05:55.230
And now whenever you're ready let's create a copy of this file because this one is an read only mode

82
00:05:55.290 --> 00:06:03.060
and therefore we want to create a copy in order to re implemented from scratch together and make sure

83
00:06:03.060 --> 00:06:06.900
to focus because you know this will be quite more advanced than before.

84
00:06:06.900 --> 00:06:10.270
So let's put that right here and there we go now.

85
00:06:10.300 --> 00:06:14.610
Let's remove the sails and let's try not to look at the final results.

86
00:06:14.640 --> 00:06:15.330
Right.

87
00:06:15.330 --> 00:06:20.640
You just put your eyes around here you don't look at the results because let's try to keep the surprise

88
00:06:20.640 --> 00:06:25.530
of the predicted salary up to the end up to the final execution of the code.

89
00:06:26.010 --> 00:06:26.240
All right.

90
00:06:26.250 --> 00:06:31.980
So let's remove all this and this as well this as well only the coattails right.

91
00:06:31.980 --> 00:06:40.350
Please keep all the tech safe so that we can keep the well highlighted structure of this implementation.

92
00:06:40.350 --> 00:06:41.340
All right.

93
00:06:41.340 --> 00:06:48.750
And now really make sure to not look at the result you know the output of the code cell because that's

94
00:06:49.050 --> 00:06:51.150
where you will see you know the final results.

95
00:06:51.150 --> 00:06:51.510
All right.

96
00:06:51.510 --> 00:06:52.940
I managed to do it.

97
00:06:52.950 --> 00:06:57.690
I did not look at it even if of course I know the result but what I mean is that it was totally possible

98
00:06:57.690 --> 00:06:58.530
not to look at it.

99
00:06:59.130 --> 00:06:59.520
All right.

100
00:06:59.550 --> 00:07:04.560
So that's the whole structure and now we're ready to start this implementation.

101
00:07:04.560 --> 00:07:05.420
And so there you go.

102
00:07:05.550 --> 00:07:09.360
I suggest that we really tackle in a flashlight.

103
00:07:09.360 --> 00:07:10.080
I love seeing this.

104
00:07:10.080 --> 00:07:17.340
I know but we will tackle in one second that data repricing phase except that part because that part

105
00:07:17.580 --> 00:07:19.120
is actually not that direct.

106
00:07:19.140 --> 00:07:21.630
You know there are gonna be some things to explain.

107
00:07:22.290 --> 00:07:23.190
All right so let's do this.

108
00:07:23.190 --> 00:07:26.040
Let's start by empowering the libraries and of course to do this.

109
00:07:26.040 --> 00:07:28.830
We're going to use our data pricing template.

110
00:07:28.830 --> 00:07:30.240
I hope you have it prepared.

111
00:07:30.240 --> 00:07:31.440
So there you go.

112
00:07:31.440 --> 00:07:40.650
Let's first get this code to import the libraries in a new code cell base here.

113
00:07:40.650 --> 00:07:45.780
Then we're going to import the data set and actually to do this we'll get our polynomial regression

114
00:07:46.080 --> 00:07:50.940
implementation because you know this the exact same data set and we don't have to explain that again

115
00:07:51.300 --> 00:07:55.830
because you perfectly know and understand now how this works.

116
00:07:55.830 --> 00:07:56.900
Right.

117
00:07:57.120 --> 00:08:00.510
And well that's the thing I wanted to do in one second.

118
00:08:00.510 --> 00:08:06.630
Now we're going to quickly upload the dataset in order to you know execute the cell after this one of

119
00:08:06.630 --> 00:08:07.500
course.

120
00:08:07.500 --> 00:08:14.310
So now it's connecting to a runtime to enable file browsing and in a second we should be able to see

121
00:08:14.820 --> 00:08:16.440
the upload button.

122
00:08:16.440 --> 00:08:16.890
Perfect.

123
00:08:16.890 --> 00:08:23.880
So upload and then as usual we're going to go into our machine learning it is that folder I like to

124
00:08:23.880 --> 00:08:24.810
put it on my desktop.

125
00:08:24.830 --> 00:08:31.740
But find it wherever you put it on your computer then part two regression then support vector regression

126
00:08:31.970 --> 00:08:32.770
and python.

127
00:08:33.060 --> 00:08:33.900
And there you go.

128
00:08:33.930 --> 00:08:39.480
That's the data set open and it will be uploaded inside the notebook.

129
00:08:39.480 --> 00:08:40.720
Perfect.

130
00:08:40.770 --> 00:08:43.320
All right then we're going to execute these two cells.

131
00:08:44.130 --> 00:08:45.930
And this one as well.

132
00:08:45.930 --> 00:08:46.680
All right.

133
00:08:46.780 --> 00:08:47.880
And perfect.

134
00:08:47.880 --> 00:08:53.640
Now we have the dataset and now we're going to step here for this material and we will tackle this next

135
00:08:53.640 --> 00:08:54.110
step.

136
00:08:54.130 --> 00:08:59.250
Feature scaling in the next one and make sure to be prepared and ready to tackle this because we'll

137
00:08:59.250 --> 00:09:00.430
have a few things to do.

138
00:09:00.450 --> 00:09:06.900
It's not difficult but just make sure to be focused for that next material because I'm going to explain

139
00:09:07.020 --> 00:09:11.430
a new situation where you have to apply feature scaling in a certain way.

140
00:09:11.460 --> 00:09:13.810
So I'll see you in the next tutorial.

141
00:09:13.830 --> 00:09:15.620
And until then enjoy machine learning.
