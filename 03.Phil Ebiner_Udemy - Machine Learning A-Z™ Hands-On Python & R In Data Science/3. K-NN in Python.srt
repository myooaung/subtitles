1
00:00:00,300 --> 00:00:01,140
Hello my friends.

2
00:00:01,140 --> 00:00:08,580
Welcome to this new practical activity on our second classification model which is of course the K nearest

3
00:00:08,720 --> 00:00:09,710
neighbors.

4
00:00:09,780 --> 00:00:15,360
We just tackle the logistic regression model and we actually made a classification template which will

5
00:00:15,480 --> 00:00:22,170
allow us to finish this Cairn and implementation in a flashlight because we will only have one cell

6
00:00:22,230 --> 00:00:23,220
to change.

7
00:00:23,220 --> 00:00:29,010
So it's really important to understand that the previous work we did on logistic regression will actually

8
00:00:29,010 --> 00:00:34,810
serve as a template you know a classification template for the other classification models here.

9
00:00:34,830 --> 00:00:38,060
So I'm going to show you this in actually one single tutorial.

10
00:00:38,070 --> 00:00:44,150
We will be extremely efficient and before we start let's of course make sure we're all on the same page.

11
00:00:44,190 --> 00:00:49,350
So right before this tutorial I gave you the link to this folder which contains all the codes and data

12
00:00:49,350 --> 00:00:49,970
sets.

13
00:00:49,980 --> 00:00:52,030
So make sure to connect to that link.

14
00:00:52,110 --> 00:00:54,200
And now we should all be on the same page.

15
00:00:54,210 --> 00:01:01,920
And therefore my friends we're gonna go into Part 3 classification and then K nearest neighbors K and

16
00:01:02,000 --> 00:01:02,310
N.

17
00:01:03,000 --> 00:01:03,320
All right.

18
00:01:03,330 --> 00:01:09,300
And as usual we're gonna start with Python and inside this folder you will indeed find two files.

19
00:01:09,390 --> 00:01:14,400
Of course the K nearest neighbors implementation in IPA in the format which you can open with either

20
00:01:14,670 --> 00:01:16,770
Juba notebook or Google collab.

21
00:01:16,770 --> 00:01:24,750
And the same dataset social network ads containing 400 observations where each observations you know

22
00:01:24,840 --> 00:01:30,290
each row corresponds to customers and for each of these customers we get the H.

23
00:01:30,300 --> 00:01:32,920
That's the first feature The estimated salary.

24
00:01:32,940 --> 00:01:38,430
That's the second feature and their purchase decision whether or not they but yes or no.

25
00:01:38,430 --> 00:01:45,540
The SUV and that's of course the dependent variable and our exercise here is to learn some correlations

26
00:01:45,540 --> 00:01:51,320
between these features The Age and the estimated salary to predict this dependent variable per taste.

27
00:01:51,330 --> 00:01:57,330
And the goal of all this is to of course predict which customers are going to buy the new SUV so that

28
00:01:57,330 --> 00:01:59,450
we can optimize our targeting.

29
00:01:59,490 --> 00:02:00,270
All right.

30
00:02:00,270 --> 00:02:01,950
So exactly the same.

31
00:02:01,980 --> 00:02:04,750
And now we're going to open this implementation.

32
00:02:04,750 --> 00:02:09,240
Kenya is neighbor with either Google collaboratively or Egyptian notebook.

33
00:02:09,240 --> 00:02:10,350
Choose your favorite.

34
00:02:10,530 --> 00:02:15,470
And now it is opening the notebook laying it out in a second.

35
00:02:15,480 --> 00:02:16,400
So learning it.

36
00:02:16,410 --> 00:02:17,040
Laying it out.

37
00:02:17,040 --> 00:02:17,610
There we go.

38
00:02:17,850 --> 00:02:19,320
And that's the notebook.

39
00:02:19,320 --> 00:02:25,470
And as you noticed as you probably noticed I actually kept the logistic regression implementation to

40
00:02:25,470 --> 00:02:33,420
show you indeed that all the code cells inside these two implementations are the same you know all the

41
00:02:33,420 --> 00:02:40,470
code cells are the same except the one where we indeed build and train the model on the training set

42
00:02:40,590 --> 00:02:40,860
right.

43
00:02:40,860 --> 00:02:43,860
So up to here you know all this is the same.

44
00:02:43,860 --> 00:02:45,050
All this is the same.

45
00:02:45,150 --> 00:02:49,450
And once we reached that step where we train the classification model in the train set.

46
00:02:49,450 --> 00:02:53,900
Well that's where we have this single change and then all the rest is the same.

47
00:02:53,940 --> 00:02:59,990
Then we predict a new result you know with the same name for the classifier which we called classifier

48
00:03:00,240 --> 00:03:02,190
then we pretty test results here.

49
00:03:02,190 --> 00:03:03,800
Once again we have nothing to change.

50
00:03:03,810 --> 00:03:06,960
It's all the same right all the same.

51
00:03:06,960 --> 00:03:13,320
So if by any chance you are starting with K and and you know before logistic regression I really encourage

52
00:03:13,320 --> 00:03:18,090
you to do logistic regression for us because all these cells are explained in detail.

53
00:03:18,090 --> 00:03:25,050
And for this key in an implementation we will just re implement you know that cell to train the key

54
00:03:25,050 --> 00:03:27,410
in and oral on the training set right.

55
00:03:27,460 --> 00:03:34,780
So that's why it is a classification template which you can use to build any other classification models.

56
00:03:34,800 --> 00:03:40,380
And together we will of course use this classification template to build all our other classification

57
00:03:40,380 --> 00:03:48,000
models including K and N support vector machine kernel as VM navy base decision tree test vacation and

58
00:03:48,010 --> 00:03:49,560
random forest classification.

59
00:03:49,560 --> 00:03:53,070
And we will all build them in maximum efficiency.

60
00:03:53,130 --> 00:03:53,610
All right.

61
00:03:53,670 --> 00:03:55,120
So let us start with Kiernan.

62
00:03:55,200 --> 00:03:59,500
As you understood the only cell that we have to change here is this one.

63
00:03:59,550 --> 00:04:05,350
So actually since we can not do it here you know re-employment here because this is an read only mode.

64
00:04:05,430 --> 00:04:10,220
Well we'll have to create a new file you know a copy of this file by clicking here this button.

65
00:04:10,350 --> 00:04:12,480
Save a copy and drive.

66
00:04:12,510 --> 00:04:19,680
This will create a copy and inside which we will be able to read implement that cell to build and train

67
00:04:19,890 --> 00:04:21,090
the cane and model.

68
00:04:21,090 --> 00:04:23,190
All right so let's do this.

69
00:04:23,190 --> 00:04:26,280
We just need to scroll down here all the rest of the same.

70
00:04:26,290 --> 00:04:27,420
And there we go.

71
00:04:27,420 --> 00:04:32,090
So what we're gonna do is just remove that cell and there we go.

72
00:04:32,100 --> 00:04:34,610
Now we can re implement that cell.

73
00:04:34,620 --> 00:04:37,010
So let's create a new code cell.

74
00:04:37,110 --> 00:04:43,350
And now I would like you to please press bus on this video to indeed try to do it yourself first because

75
00:04:43,500 --> 00:04:48,780
once again I not only want to train you on machine learning but also I want to train you on how to be

76
00:04:48,840 --> 00:04:54,120
independent and create things on your own you know create Europe machinery models on your own.

77
00:04:54,150 --> 00:05:00,740
I already guided you on how to navigate the psychic learn API to find some information in tools.

78
00:05:00,840 --> 00:05:04,140
And now I would like you to do this same exercise again.

79
00:05:04,140 --> 00:05:11,070
Please press buzz on the video and navigate the circuit here and API to find the class that allows indeed

80
00:05:11,070 --> 00:05:16,630
to build the key and model and then you know how to train it on the training set.

81
00:05:16,650 --> 00:05:17,170
All right.

82
00:05:17,250 --> 00:05:20,640
So in two seconds now we're going to start a solution together.

83
00:05:21,490 --> 00:05:22,070
Here we go.

84
00:05:22,450 --> 00:05:22,750
All right.

85
00:05:22,780 --> 00:05:25,120
So let us suppose you know I'm just like you.

86
00:05:25,130 --> 00:05:30,340
I'm not an expert in machine learning and I would like to build and train a cane and model on the training

87
00:05:30,340 --> 00:05:30,950
set.

88
00:05:31,060 --> 00:05:36,520
So since I actually have no idea of what is the name of the class that does it well I'm gonna go off

89
00:05:36,520 --> 00:05:39,640
course to search it online and inside.

90
00:05:39,640 --> 00:05:41,790
Well we will do it this way.

91
00:05:41,810 --> 00:05:43,930
Suck it learn.

92
00:05:43,930 --> 00:05:45,970
Then we're gonna go to the first link.

93
00:05:46,040 --> 00:05:52,540
All right we will end up in the cycle learn welcoming page then we're gonna go remember to API which

94
00:05:52,540 --> 00:05:57,810
contains all the modules and functions and classes of cycled learn and now.

95
00:05:57,820 --> 00:05:58,640
There we go.

96
00:05:58,650 --> 00:06:04,430
We are looking for the class that allows to build the K and moral.

97
00:06:04,660 --> 00:06:08,560
Let's scroll down a bit and if we have you know too much difficulty to find it.

98
00:06:08,680 --> 00:06:12,580
Well we will do a controller command F to find it.

99
00:06:12,580 --> 00:06:13,960
You know there are many tricks actually.

100
00:06:13,960 --> 00:06:21,280
You can also directly type in your search bar K nearest neighbor class cycle turn or cycle learn K nearest

101
00:06:21,280 --> 00:06:22,880
neighbor class something like that.

102
00:06:23,140 --> 00:06:27,830
But I like to navigate the secondary API because it is really well-made.

103
00:06:28,090 --> 00:06:31,450
And so here by scrolling down do we find it.

104
00:06:31,690 --> 00:06:32,280
Yes.

105
00:06:32,290 --> 00:06:33,670
Here it is.

106
00:06:33,670 --> 00:06:35,090
Nearest neighbors.

107
00:06:35,130 --> 00:06:40,450
So it was actually hard to miss but indeed we had to scroll down a bit but that's OK because it's really

108
00:06:40,450 --> 00:06:44,050
good to get familiar with the cycling library.

109
00:06:44,110 --> 00:06:47,380
All right so that was the name of the module you had to find.

110
00:06:47,380 --> 00:06:53,290
And now the next question is which of these classes because you see these are all the classes of this

111
00:06:53,730 --> 00:06:54,550
neighbors module.

112
00:06:54,550 --> 00:06:57,840
That's actually the name of the module by I learn neighbors.

113
00:06:57,910 --> 00:07:03,340
And these are all the classes that allow you to build machinery tools you know in this nearest neighbors

114
00:07:03,580 --> 00:07:05,230
branch of machine learning.

115
00:07:05,230 --> 00:07:05,490
All right.

116
00:07:05,500 --> 00:07:11,740
So of course the one we're interested in is this one K Nabors classifier.

117
00:07:11,770 --> 00:07:12,430
There you go.

118
00:07:12,430 --> 00:07:13,600
Congratulations.

119
00:07:13,600 --> 00:07:14,290
If you found it.

120
00:07:14,290 --> 00:07:18,730
So let's click it and let's see the whole documentation.

121
00:07:18,730 --> 00:07:23,850
So feel free to read it if you want you can see what are all the parameters and also attributes.

122
00:07:23,980 --> 00:07:30,220
But what we actually simply need is this you know the whole name of the class and the module and the

123
00:07:30,220 --> 00:07:31,260
library cycle.

124
00:07:31,300 --> 00:07:37,270
Because the only thing that we need really to build and train this cane and model is the name of this

125
00:07:37,270 --> 00:07:41,510
class to you know create the object and also the parameters here.

126
00:07:41,590 --> 00:07:46,720
We need to know which parameters we have to enter here in order to build a relevant K and a..

127
00:07:46,720 --> 00:07:47,350
All right.

128
00:07:47,350 --> 00:07:54,010
So first let's take this and let's go back to our implementation to here faced it and then adapted by

129
00:07:54,010 --> 00:08:00,760
you know doing this from from site learn and then from the neighbors module of psychic learn we will

130
00:08:01,150 --> 00:08:09,400
import this class to K Nabors classifier that's the class and then in the next natural step it is to

131
00:08:09,400 --> 00:08:16,030
create an object of this class which will represent exactly the K and then model itself the classifier

132
00:08:16,450 --> 00:08:22,720
and that's why we call it class set fire and then to create an object of this class well we just need

133
00:08:22,720 --> 00:08:24,060
to call the class again.

134
00:08:24,120 --> 00:08:28,620
So I'm copying this basing it here and then adding some parentheses.

135
00:08:28,630 --> 00:08:34,120
All right so that's the first information we need to get from the cyclone API but then the second thing

136
00:08:34,120 --> 00:08:39,380
we need to check also are the parameters here and you have all the descriptions here.

137
00:08:39,400 --> 00:08:45,280
So for example the first one and neighbors equals five AND neighbours is of course the number of neighbours

138
00:08:45,400 --> 00:08:50,290
of your cane and model you remember the intuition lecturers you have the neighbours that you use to

139
00:08:50,410 --> 00:08:55,480
make your predictions and we have to choose a number of neighbours and well you know we can just try

140
00:08:55,480 --> 00:08:56,620
this value five.

141
00:08:56,770 --> 00:09:02,050
I actually know that we will get good results with this but you know in your future missionary projects

142
00:09:02,140 --> 00:09:07,930
if you're using a cane and moral Well I recommend to tune it with several values but five is usually

143
00:09:07,930 --> 00:09:08,180
used.

144
00:09:08,180 --> 00:09:15,660
So let's do this first parameter and nay verse equals five good.

145
00:09:15,970 --> 00:09:19,550
Then next parameter let's see weights equals uniforms.

146
00:09:19,570 --> 00:09:25,000
Uniform is the default value of weight and weight is the weight function used in prediction and well

147
00:09:25,000 --> 00:09:31,120
here we will actually keep the default value uniform which means that all the points in each neighborhood

148
00:09:31,420 --> 00:09:33,030
are weighted equally.

149
00:09:33,090 --> 00:09:33,360
OK.

150
00:09:33,370 --> 00:09:35,120
So they have the same importance.

151
00:09:35,230 --> 00:09:36,040
So we will keep that.

152
00:09:36,040 --> 00:09:38,700
That's fine then algorithm equals zero.

153
00:09:38,710 --> 00:09:39,730
What does that mean.

154
00:09:39,730 --> 00:09:44,950
Well that's basically the algorithm used to compute the nearest neighbors and auto is the best value

155
00:09:44,950 --> 00:09:50,980
to choose because it will decide automatically the most appropriate algorithm based on the values path

156
00:09:50,980 --> 00:09:52,170
to the fit method.

157
00:09:52,210 --> 00:09:55,170
You know the method that trains your model on the training set.

158
00:09:55,210 --> 00:10:00,940
So definitely here it will be simple if which was auto and then you have some other parameters leaf

159
00:10:00,950 --> 00:10:07,980
size of which will keep the default value and P which is the power parameter for the min Koski metric.

160
00:10:07,980 --> 00:10:08,490
So there we go.

161
00:10:08,500 --> 00:10:13,330
That's important that the other parameters we will enter the last two parameters that actually want

162
00:10:13,330 --> 00:10:21,130
to enter are this one metric equals Murkowski and P because indeed metric is actually the distance you

163
00:10:21,130 --> 00:10:26,340
want to use to compute you know the distance between your observation points and the neighbors.

164
00:10:26,410 --> 00:10:31,360
And we actually want to choose the Euclidean distance which is you know the classic distance equal to

165
00:10:31,360 --> 00:10:35,320
the square root of the sum of the squared differences between the coordinates.

166
00:10:35,490 --> 00:10:41,650
And in order to take that classic Euclidean distance Well we had to choose a min Koski metric with p

167
00:10:41,680 --> 00:10:42,740
equals 2.

168
00:10:42,760 --> 00:10:49,960
So basically we're keeping all the default values of this K neighbors classifier class but in order

169
00:10:49,960 --> 00:10:52,130
to make sure that we are using them.

170
00:10:52,210 --> 00:10:53,710
And just to highlight them.

171
00:10:53,860 --> 00:10:58,450
Well let's just write these parameters with their default values anyway because it's important to see

172
00:10:58,540 --> 00:10:59,470
what we're dealing with.

173
00:10:59,470 --> 00:11:02,370
You know what version of K and then we are dealing with.

174
00:11:02,440 --> 00:11:04,090
Okay so let's do this quickly.

175
00:11:04,120 --> 00:11:08,330
Metric equals Min count ski.

176
00:11:08,630 --> 00:11:11,450
And then P equals two.

177
00:11:11,500 --> 00:11:12,460
Perfect.

178
00:11:12,460 --> 00:11:17,660
And so now we have basically a classic K nearest neighbors model with five neighbors.

179
00:11:17,740 --> 00:11:19,720
And the classic Euclidean distance.

180
00:11:19,720 --> 00:11:20,720
Okay.

181
00:11:20,800 --> 00:11:23,360
And now you perfectly know how to finish this.

182
00:11:23,410 --> 00:11:29,230
The last step here is of course to train our classifier which indeed we built so far but is not trained

183
00:11:29,230 --> 00:11:30,790
yet on the training set.

184
00:11:30,790 --> 00:11:34,640
So that's exactly what we need to do as a final step.

185
00:11:34,640 --> 00:11:42,040
And so there you go we call our classifier from which we're going to call our fit method which as usual

186
00:11:42,040 --> 00:11:43,170
takes as input.

187
00:11:43,240 --> 00:11:46,290
First the matrix of features X train.

188
00:11:46,690 --> 00:11:52,320
And second the dependent variable vector Y train of the training set of course.

189
00:11:52,330 --> 00:11:52,660
All right.

190
00:11:52,660 --> 00:11:53,910
Perfect.

191
00:11:53,980 --> 00:11:54,850
And that's it.

192
00:11:54,910 --> 00:11:59,170
You know we're done with this implementation all the rest is the same.

193
00:11:59,170 --> 00:12:04,780
We don't have to change anything else here because indeed since we called our canine model classifier.

194
00:12:04,900 --> 00:12:10,180
Well here to make the predictions we already have the right name of the variable classifier and then

195
00:12:10,180 --> 00:12:15,030
same here to predict the test results classifier and then same for the computer matrix.

196
00:12:15,030 --> 00:12:16,160
Why test why bread.

197
00:12:16,270 --> 00:12:21,190
Which result from our same classifier and then same for the visualisation of the results.

198
00:12:21,190 --> 00:12:22,420
Sorry I just showed them to you.

199
00:12:22,420 --> 00:12:26,020
I hope you didn't see but we're gonna get to that in a second.

200
00:12:26,020 --> 00:12:26,500
There you go.

201
00:12:26,500 --> 00:12:27,250
That's the same.

202
00:12:27,250 --> 00:12:33,010
Same names of the variable classifier X strain y transfer all the rest of the same and that's why I

203
00:12:33,010 --> 00:12:35,800
like to call it a good code template.

204
00:12:35,890 --> 00:12:42,240
All right so now my friends we are going to rerun all this right because we're basically done.

205
00:12:42,460 --> 00:12:48,910
And so to run all this again we're going to first upload the data set because it is not yet uploaded

206
00:12:48,940 --> 00:12:50,080
in the notebook.

207
00:12:50,080 --> 00:12:55,810
So right now the notebook is connecting to a runtime to enable file browsing and in a second we should

208
00:12:55,810 --> 00:12:57,550
see the upload button.

209
00:12:57,550 --> 00:12:58,440
Perfect.

210
00:12:58,540 --> 00:13:02,840
So upload and then make sure to find your whole machinery.

211
00:13:02,870 --> 00:13:05,330
That folder containing all the codes and data sets.

212
00:13:05,410 --> 00:13:10,810
Mine is on my desktop so then we're going to go inside and we're going to go to part 3 classification

213
00:13:11,110 --> 00:13:20,080
and then K nearest neighbors Keenan and in Python and then we select our social network at dataset click

214
00:13:20,080 --> 00:13:21,040
open.

215
00:13:21,040 --> 00:13:23,680
Now it's going to be applauded on the notebook.

216
00:13:23,680 --> 00:13:24,580
Perfect.

217
00:13:24,580 --> 00:13:25,420
There we go.

218
00:13:25,420 --> 00:13:32,470
Now we can run everything and to do this well we're gonna click runtime here and then simply run.

219
00:13:32,560 --> 00:13:35,570
Oh and let's just start from here.

220
00:13:35,580 --> 00:13:36,520
OK great.

221
00:13:36,530 --> 00:13:44,170
R K nearest neighbours classifier was created and trained properly and at the end so all the predictions

222
00:13:44,170 --> 00:13:49,720
are generated again we can actually have a look at them and while we seem to have some great predictions

223
00:13:49,960 --> 00:13:51,540
that remember on the test.

224
00:13:51,700 --> 00:13:56,890
So all this is correct here we have an incorrect prediction meaning that the key and then predicted

225
00:13:56,890 --> 00:14:03,380
this particular customer to buy the SUV whereas in reality that customer didn't buy the SUV then all

226
00:14:03,400 --> 00:14:05,010
this is correct or correct.

227
00:14:05,030 --> 00:14:05,950
All correct.

228
00:14:05,950 --> 00:14:09,460
It seems really good it seems to be some great predictions here.

229
00:14:09,460 --> 00:14:11,290
Here we have another incorrect prediction.

230
00:14:11,320 --> 00:14:16,840
This time the opposite one our model predicted that this particular customer didn't buy the SUV whereas

231
00:14:16,870 --> 00:14:19,560
in reality that customer bought the SUV.

232
00:14:19,850 --> 00:14:20,260
Right.

233
00:14:20,260 --> 00:14:20,740
Correct.

234
00:14:20,770 --> 00:14:22,300
All correct all correct.

235
00:14:22,300 --> 00:14:22,960
So pretty good.

236
00:14:22,960 --> 00:14:23,250
Wow.

237
00:14:23,260 --> 00:14:26,170
I think we actually have very very few incorrect positions.

238
00:14:26,200 --> 00:14:27,980
Let's actually check it out right now.

239
00:14:28,000 --> 00:14:28,390
Okay.

240
00:14:28,430 --> 00:14:34,860
So I didn't see it properly but we actually haven't heard all four plus three incorrect predictions.

241
00:14:34,870 --> 00:14:38,020
That's real really good that's better than the logistic regression model.

242
00:14:38,020 --> 00:14:45,130
Remember we had actually an accuracy of the test set of 89 percent.

243
00:14:45,350 --> 00:14:49,240
So here clearly the Kenya's neighbours did better than the logistic regression model.

244
00:14:49,250 --> 00:14:55,000
And you will perfectly understand why when I show you the visualization so it's actually here.

245
00:14:55,110 --> 00:15:01,880
All right so four plus three equals seven incorrect predictions then 64 correct predictions of the class

246
00:15:01,970 --> 00:15:08,690
0 meaning 64 correct predictions that the customer doesn't buy the SUV and twenty nine correct predictions

247
00:15:08,750 --> 00:15:14,660
of the class one meaning twenty nine incorrect predictions that the customer buys the SUV and then we

248
00:15:14,660 --> 00:15:19,460
have indeed four incorrect predictions of the class one meaning four incorrect predictions that the

249
00:15:19,460 --> 00:15:25,670
customer buys the SUV and three incorrect predictions of the class zero meaning three incorrect predictions

250
00:15:26,060 --> 00:15:29,150
that the customer doesn't buy the SUV.

251
00:15:29,150 --> 00:15:30,560
So very very good.

252
00:15:30,560 --> 00:15:37,730
And now as you can see it is still executing the sell that plots that to the array you know showing

253
00:15:38,150 --> 00:15:44,990
all the prediction regions and prediction boundary of the key and in model first on the training set

254
00:15:45,320 --> 00:15:46,760
and then on the test.

255
00:15:47,270 --> 00:15:52,530
And the reason why it is taking some time here to execute Oh there you go we're about to have it in

256
00:15:52,540 --> 00:15:53,030
a second.

257
00:15:53,030 --> 00:15:53,760
Yes.

258
00:15:53,780 --> 00:16:00,200
So perfect timing but I was going to say the reason why it takes sometimes to execute is simply because

259
00:16:00,200 --> 00:16:05,360
you know the K and model is actually very compute intensive.

260
00:16:05,390 --> 00:16:09,800
You know there are a lot of computations when running the cane and model and that's where it took some

261
00:16:09,800 --> 00:16:10,480
time.

262
00:16:10,560 --> 00:16:12,530
And so now let's interpret the results.

263
00:16:12,920 --> 00:16:19,580
So remember with the logistic regression classifier the prediction boundary was a straight line.

264
00:16:19,580 --> 00:16:24,890
And that's because the logistic regression model is a linear classifier will actually see another type

265
00:16:24,890 --> 00:16:26,680
of linear classifier in this part.

266
00:16:26,750 --> 00:16:33,680
But remember for Linear classifiers the prediction boundary or prediction curve for classification is

267
00:16:33,920 --> 00:16:38,320
a straight line and in three dimensions it's a straight plan and an end dimension.

268
00:16:38,330 --> 00:16:40,020
It's a straight hyper plane.

269
00:16:40,070 --> 00:16:46,090
And so here we had a straight line which therefore resulted in having a lot of incorrect predictions

270
00:16:46,100 --> 00:16:51,140
because as we can see there are a lot of green points here you know green customers who bought the SUV

271
00:16:51,140 --> 00:16:57,360
in reality that fall in the red region where we predict indeed that the customers don't buy the SUV.

272
00:16:57,440 --> 00:17:03,080
And same for these ones here you know here we have a lot of customers who in reality but the SUV but

273
00:17:03,290 --> 00:17:06,350
we're predicted not to because they fall in the red region.

274
00:17:06,350 --> 00:17:12,610
And so what we were hoping for is to build a new classifier for which the decision boundary you know

275
00:17:12,770 --> 00:17:16,430
separating the two prediction regions does something like this.

276
00:17:16,430 --> 00:17:21,860
You know would not be a straight line but some kind of a curve catching all the red points here without

277
00:17:21,860 --> 00:17:24,530
catching those great points here and here.

278
00:17:24,530 --> 00:17:25,060
OK.

279
00:17:25,170 --> 00:17:28,700
That's what we were hoping for and well that's exactly what we got here.

280
00:17:28,730 --> 00:17:34,790
Thanks to our ken and moral we don't get a smooth curve but we get actually some kind of a curve that

281
00:17:34,790 --> 00:17:40,820
indeed catches all the red points here without catching the green points and leaving those grid points

282
00:17:40,820 --> 00:17:45,290
and these ones inside the green region where they should end up right.

283
00:17:45,380 --> 00:17:49,700
And of course here we have some green points that were super hard to catch because they are stuck in

284
00:17:49,700 --> 00:17:51,550
the middle of all those red points here.

285
00:17:51,590 --> 00:17:55,800
So that's fine in any way in machinery we want to avoid overfishing.

286
00:17:55,820 --> 00:18:00,560
You know when we have some to perfect predictions on the training set and therefore probably resulting

287
00:18:00,560 --> 00:18:07,030
in having bad performance bad predictions on the tacit and well speaking of the test let's see how arcane

288
00:18:07,130 --> 00:18:08,840
and did it should be fine.

289
00:18:08,860 --> 00:18:09,620
Yes it is.

290
00:18:09,710 --> 00:18:10,860
Execute it.

291
00:18:11,120 --> 00:18:17,810
And once again we get excellent results right because same the Kiernan prediction curve or prediction

292
00:18:17,810 --> 00:18:22,490
boundary or even decision boundary catches all the right customers.

293
00:18:22,490 --> 00:18:28,610
You know the red customers who didn't buy in reality the SUV therefore leaving the Green customers who.

294
00:18:28,610 --> 00:18:33,380
But in reality the SUV in the right prediction region the green region.

295
00:18:33,770 --> 00:18:34,220
OK.

296
00:18:34,220 --> 00:18:39,440
And of course we still have some wrong predictions incorrect predictions but that's because the training

297
00:18:39,440 --> 00:18:42,070
was properly run without overheating.

298
00:18:42,200 --> 00:18:45,350
And that's why here we can clearly see are incorrect predictions.

299
00:18:45,470 --> 00:18:51,710
These two customers here and this one and this one who are indeed incorrect predictions that the customers

300
00:18:51,710 --> 00:18:56,480
bought the SUV whereas in reality they didn't because these are red points falling in the green region

301
00:18:56,810 --> 00:19:03,890
and the other incorrect predictions here which are incorrect because these are green customers therefore

302
00:19:04,040 --> 00:19:09,820
who in reality but the SUV but we're predicting that two because they fall in the red region.

303
00:19:09,860 --> 00:19:10,350
All right.

304
00:19:10,400 --> 00:19:12,200
So that's already much better.

305
00:19:12,230 --> 00:19:16,450
We get much better results for indeed this particular data set for which.

306
00:19:16,460 --> 00:19:21,200
Well nonlinear classifiers that can have that kind of curve is a benefit.

307
00:19:21,200 --> 00:19:28,220
And indeed we increased the accuracy of the test by 4 percent and now the question is can we do even

308
00:19:28,220 --> 00:19:32,030
better than this you know with our future classification models.

309
00:19:32,030 --> 00:19:38,510
Well that's exactly what we're going to find out in the next sections when building our future classification

310
00:19:38,510 --> 00:19:46,980
models which include as VM kernel SVM Bayh's decision tree classification and random for its classification.

311
00:19:47,070 --> 00:19:51,050
Take a bet on what is going to bring in the end the best accuracy.

312
00:19:51,050 --> 00:19:55,950
And while my friends follow me in the next practical activities to discover the big winner.

313
00:19:56,070 --> 00:19:57,900
And until then enjoy machine learning.
