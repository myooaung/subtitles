1
00:00:00,330 --> 00:00:06,420
Okay my friends we have three steps left and the one we're about to implement is the confusion matrix

2
00:00:06,510 --> 00:00:12,210
which is a simple to The Matrix you know two rows two columns which will show us the number of correct

3
00:00:12,210 --> 00:00:13,260
predictions we did.

4
00:00:13,320 --> 00:00:19,080
In both case you know predicting 0 or 1 and how many incorrect predictions we did in same both cases

5
00:00:19,470 --> 00:00:20,480
0 0 1.

6
00:00:20,480 --> 00:00:20,700
All right.

7
00:00:20,700 --> 00:00:26,310
So that will be a nice way to see quickly where we did right and wrong and also in that same could sell

8
00:00:26,340 --> 00:00:28,830
we will compute the accuracy.

9
00:00:29,160 --> 00:00:29,650
And now.

10
00:00:29,670 --> 00:00:35,070
Well at the end of the previous tutorial I actually asked you to try to figure it out on your own by

11
00:00:35,130 --> 00:00:37,980
looking at the psychic learn API.

12
00:00:38,010 --> 00:00:40,880
So that's exactly what we're gonna do first here.

13
00:00:40,950 --> 00:00:45,210
I'm going to show you how to navigate it and find the information we want.

14
00:00:45,210 --> 00:00:47,160
You know the tool that we need.

15
00:00:47,160 --> 00:00:50,000
Let's go back to the welcoming page site.

16
00:00:50,000 --> 00:00:55,680
Learn then remember you have to go to API here which contains all the classes and functions from the

17
00:00:55,680 --> 00:00:57,090
different modules.

18
00:00:57,090 --> 00:01:04,440
And I actually gave you a hint in the previous tutorial I told you to go to look into a module called

19
00:01:04,820 --> 00:01:06,530
metrics remember.

20
00:01:06,750 --> 00:01:12,900
So we just have to scroll down a bit here and we will find very soon metrics.

21
00:01:12,900 --> 00:01:14,580
It's in the alphabetical order.

22
00:01:14,580 --> 00:01:21,930
So there it is metrics and then it's very well organized as you can see have the regression metrics

23
00:01:22,050 --> 00:01:24,570
in which we already covered the most important ones.

24
00:01:24,630 --> 00:01:26,510
And the classification metrics.

25
00:01:26,820 --> 00:01:29,400
And here we are of course dealing with classification.

26
00:01:29,400 --> 00:01:33,810
Therefore you had to look into here and now we're getting closer we're getting warmer.

27
00:01:33,810 --> 00:01:35,790
What do we see inside.

28
00:01:35,790 --> 00:01:42,600
Well we actually see metrics confusion matrix and that's exactly what we'll take in order to build this

29
00:01:42,750 --> 00:01:44,530
confusion matrix.

30
00:01:44,610 --> 00:01:45,200
All right.

31
00:01:45,210 --> 00:01:50,370
And then what I usually do is simply have a look at one of the examples because it usually contains

32
00:01:50,370 --> 00:01:52,640
the code on how to build such a tool.

33
00:01:52,650 --> 00:01:58,490
You know the computer matrix but I usually give you this example on some random values of dependent

34
00:01:58,490 --> 00:01:59,280
variable vectors.

35
00:01:59,280 --> 00:02:02,700
You know that's like why test you know the vector containing the real results.

36
00:02:02,700 --> 00:02:05,400
That's why pred containing the vector of predictions.

37
00:02:05,400 --> 00:02:11,270
And it tells you how to apply this confusion matrix function onto the vector of real results and effective

38
00:02:11,280 --> 00:02:12,060
predictions.

39
00:02:12,060 --> 00:02:12,300
Okay.

40
00:02:12,330 --> 00:02:18,960
So in order to implement this well let's just take this so we can import indeed that confusion matrix

41
00:02:18,960 --> 00:02:22,820
which belongs to the Matrix module from the second library.

42
00:02:22,830 --> 00:02:23,580
So there we go.

43
00:02:23,580 --> 00:02:26,600
Let's face it at first I just copied it.

44
00:02:26,640 --> 00:02:29,020
Let's create a new code sell and paste this.

45
00:02:29,040 --> 00:02:35,130
So that's how you import a computer matrix and then let's grab that other piece of code you know.

46
00:02:35,130 --> 00:02:44,220
This particular line where indeed we apply the computer matrix function onto the vectors of predictions

47
00:02:44,400 --> 00:02:45,660
and real results.

48
00:02:45,690 --> 00:02:46,400
Let's base this.

49
00:02:46,410 --> 00:02:48,350
So you see what I'm trying to do here.

50
00:02:48,350 --> 00:02:50,240
I don't do this because I'm lazy.

51
00:02:50,250 --> 00:02:53,560
I do this in order to train you to be independent.

52
00:02:53,580 --> 00:02:58,770
You know whenever you need a new information or a new tool that you need and training you on how to

53
00:02:58,770 --> 00:03:04,320
find it in the cycle learn API and I will do the same later on when we start working with tensor flow

54
00:03:04,380 --> 00:03:07,580
you know in the deep learning part of this course part 8.

55
00:03:07,770 --> 00:03:08,210
But.

56
00:03:08,340 --> 00:03:09,690
So you see this is very important.

57
00:03:09,720 --> 00:03:13,400
I really want you to be independent and figure things out on your own.

58
00:03:13,980 --> 00:03:18,420
And now inside this computing matrix function what do we have to replace.

59
00:03:18,420 --> 00:03:24,210
Well you know they called the vector here of real results y true but us since we actually want to distinguish

60
00:03:24,450 --> 00:03:30,570
the vector of real results in the training set and to set well we actually called are y true vectors

61
00:03:30,570 --> 00:03:34,280
let's say y trained for the training set and Y test for the test set.

62
00:03:34,280 --> 00:03:41,430
So here since of course the confusion matrix is usually evaluated on the test set you know for new observations.

63
00:03:41,520 --> 00:03:48,330
Here we have to replace y true by white test so that we will get indeed the confusion matrix showing

64
00:03:48,390 --> 00:03:54,630
the correct predictions and the incorrect predictions for both cases 0 and 1 on the test set.

65
00:03:54,810 --> 00:03:55,290
Great.

66
00:03:55,410 --> 00:04:01,290
So we will actually put the output of this confusion matrix function applied to whiteness and white

67
00:04:01,290 --> 00:04:07,620
bread into a new variable which we're going to call C M which stands for computing matrix and which

68
00:04:07,620 --> 00:04:11,790
will be exactly the output returns by this confusion matrix function.

69
00:04:11,790 --> 00:04:21,300
So there we go and then we'll add a final print c m in order to print indeed that confusion matrix.

70
00:04:21,300 --> 00:04:21,520
Okay.

71
00:04:21,540 --> 00:04:26,910
So these are the three lines of code that allow indeed to build that confusion matrix and print it.

72
00:04:26,970 --> 00:04:34,200
And now remember that I also asked you to compute the accuracy and well to do this we just had to do

73
00:04:34,200 --> 00:04:39,180
exactly the same as what we just did you know to find that information actually try to press pause on

74
00:04:39,180 --> 00:04:42,760
the video and find it yourself if not already.

75
00:04:42,780 --> 00:04:45,980
So we're going to go back to that matrix module.

76
00:04:45,990 --> 00:04:51,540
You know remember the metrics module from the secondary library and we're going to look back into this

77
00:04:51,600 --> 00:04:56,040
classification metric section to find the accuracy.

78
00:04:56,070 --> 00:04:57,870
So according to you where is it.

79
00:04:57,870 --> 00:04:59,510
Well it's hard to miss it.

80
00:04:59,520 --> 00:05:05,510
This is actually the first one accuracy score which computes indeed the accuracy classification score

81
00:05:05,540 --> 00:05:09,060
which is just you know the rate of correct predictions.

82
00:05:09,140 --> 00:05:10,180
So let's do this.

83
00:05:10,190 --> 00:05:18,530
Let's click this link and we will get indeed all the documentation on this accuracy score function which

84
00:05:18,530 --> 00:05:25,520
returns the accuracy of your model on whatever set of data and will apply it of course to the test set.

85
00:05:25,520 --> 00:05:32,480
So first of all here as we can see this accuracy score function belongs of course the same matrix module.

86
00:05:32,480 --> 00:05:39,290
So we don't have to take all of this again we can just take that name of the function here and I will

87
00:05:39,290 --> 00:05:41,540
show you what to do here.

88
00:05:41,600 --> 00:05:48,470
Just next to confusion matrix you add a comma and then you can paste this other function you need you

89
00:05:48,470 --> 00:05:52,960
know which you have to import still from that matrix module from the cyclone library.

90
00:05:53,270 --> 00:05:53,500
OK.

91
00:05:53,520 --> 00:05:55,590
So now we have it and now we can use it.

92
00:05:55,730 --> 00:06:02,150
And therefore just below we will actually call it and in order to do this efficiently we can just take

93
00:06:02,150 --> 00:06:12,690
the example here and right below we paste it and we just replace once again y true by Y test so that

94
00:06:12,690 --> 00:06:16,580
we can get indeed the accuracy under test it.

95
00:06:16,620 --> 00:06:23,100
Okay and here we don't have to do a print because this accuracy score will directly return that accuracy

96
00:06:23,110 --> 00:06:25,200
and that rate of correct predictions.

97
00:06:25,200 --> 00:06:25,950
So there you go.

98
00:06:26,010 --> 00:06:30,030
We can just play this sound you know run the sail and here you go.

99
00:06:30,030 --> 00:06:32,040
Congratulations you have here.

100
00:06:32,070 --> 00:06:39,150
The confusion matrix showing that we have indeed 65 correct predictions of the class 0 meaning the customers

101
00:06:39,150 --> 00:06:42,800
of the test who didn't buy the new SUV then 24.

102
00:06:42,810 --> 00:06:49,860
Correct predictions of the class 1 meaning correct prediction that the customers who bought the SUV

103
00:06:50,220 --> 00:06:56,490
and then three incorrect predictions of the class 1 meaning three incorrect predictions of the customers

104
00:06:56,520 --> 00:06:56,820
who.

105
00:06:57,090 --> 00:07:00,920
But in reality the SUV but were predicted not to.

106
00:07:00,960 --> 00:07:09,390
And finally 8 incorrect predictions of the class 0 meaning 8 customers who in reality didn't buy the

107
00:07:09,390 --> 00:07:12,420
SUV but were predicted to buy it.

108
00:07:12,450 --> 00:07:12,750
OK.

109
00:07:12,780 --> 00:07:15,930
So you see the confusion matrix has no mystery.

110
00:07:15,930 --> 00:07:22,510
It's very easy to read and in a flashlight we can indeed get the main information of our predictions.

111
00:07:22,710 --> 00:07:27,300
And finally that little number that we have here is of course the accuracy.

112
00:07:27,300 --> 00:07:34,810
And here we get 0 point eighty nine which means that we had 89 percent of correct predictions in the

113
00:07:34,910 --> 00:07:42,080
set and actually remember that they are 100 observations in a test which means that we had indeed 89

114
00:07:42,330 --> 00:07:47,840
correct predictions actually 65 year plus 24 is equal indeed to 89.

115
00:07:47,900 --> 00:07:48,200
All right.

116
00:07:48,210 --> 00:07:54,540
But for any size of the test set this would mean that you had 89 percent of correct predictions and

117
00:07:54,540 --> 00:07:59,950
that's exactly the accuracy the accuracy is the rate of correct predictions.

118
00:08:00,090 --> 00:08:00,470
All right.

119
00:08:00,480 --> 00:08:05,040
So now you know how to quickly evaluate a classification model.

120
00:08:05,050 --> 00:08:11,010
You know the accuracy is usually the right metric to use when evaluating your classification models.

121
00:08:11,010 --> 00:08:12,920
So now you have it in the tool kit.

122
00:08:13,080 --> 00:08:16,870
And so here we go for final step of the journey.

123
00:08:16,900 --> 00:08:22,850
We're going to visualize not only the training set results but also the test results.

124
00:08:22,860 --> 00:08:29,580
And this will be super interesting because we will actually see how the logistic regression classifier

125
00:08:29,790 --> 00:08:36,720
was actually trained to classify our customer as you know our observations in to the two different classes.

126
00:08:36,720 --> 00:08:43,380
You know 0 0 1 we will have super nice results showing all the real results in both the training set

127
00:08:43,380 --> 00:08:46,650
and a test set and also the prediction regions.

128
00:08:46,650 --> 00:08:52,830
You know the regions where operations are zero and the other region where the predictions are one and

129
00:08:52,830 --> 00:08:59,340
you will see that as a curve that separates these two regions is exactly the classification curve and

130
00:08:59,550 --> 00:09:04,830
you will see that it will be a straight line for linear models and something else in a straight line

131
00:09:04,830 --> 00:09:06,000
for non-linear models.

132
00:09:06,000 --> 00:09:11,910
I really really can't wait to show you this because you will really see and visualize the difference

133
00:09:11,970 --> 00:09:16,770
between linear classification models and non-linear classification models.

134
00:09:16,770 --> 00:09:22,770
So there you go take a little break now and we'll tackle together this final step to visualize indeed

135
00:09:22,950 --> 00:09:24,050
these two results.

136
00:09:24,060 --> 00:09:25,980
And until then enjoy machine learning.
