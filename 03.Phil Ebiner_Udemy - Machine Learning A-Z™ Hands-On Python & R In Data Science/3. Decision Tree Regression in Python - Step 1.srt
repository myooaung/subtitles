1
00:00:00,210 --> 00:00:05,280
Hello my friends welcome to this new section on now Decision Tree regression.

2
00:00:05,280 --> 00:00:11,310
So we're about to tackle a new practical activity where we will all learn together how to build the

3
00:00:11,310 --> 00:00:13,230
decision to regression model.

4
00:00:13,230 --> 00:00:19,110
You will see that it will be super easy based on what we did before you know with all the feature scaling

5
00:00:19,170 --> 00:00:24,990
and inverse transformation of the SVR we are now we won't have to apply feature scaling and therefore

6
00:00:25,080 --> 00:00:26,760
we will just smash this.

7
00:00:26,760 --> 00:00:27,320
All right.

8
00:00:27,480 --> 00:00:29,190
Are you ready Are you ready to start.

9
00:00:29,250 --> 00:00:34,410
Before we go into this part two regression further let's just make sure that everyone here is on the

10
00:00:34,410 --> 00:00:35,410
same page.

11
00:00:35,430 --> 00:00:40,230
I give you the link to this folder just before this tutorial so you just have to click the link and

12
00:00:40,230 --> 00:00:46,410
then now we should all be on the same page so we're gonna go into part two regression and then Section

13
00:00:46,500 --> 00:00:48,560
8 decision tree regression.

14
00:00:48,570 --> 00:00:50,780
We're almost at the end of the regression path.

15
00:00:50,790 --> 00:00:56,700
Congratulations for the great progress you've made so far and now we're going to go of course to python

16
00:00:57,090 --> 00:01:00,390
in order to find our files for this section.

17
00:01:00,390 --> 00:01:01,870
So there are two files here.

18
00:01:01,930 --> 00:01:07,530
The Python implementation of the dissident regression model in IP one be format which you can open with

19
00:01:07,620 --> 00:01:15,120
either Google collab or Jupiter notebook and our same positions salaries data set containing that same

20
00:01:15,120 --> 00:01:20,910
data of the previous company showing the different position levels from 1 to 10 corresponding to business

21
00:01:20,910 --> 00:01:28,190
and those 2 SEO and the corresponding salaries from 45000 to 1 million dollars per year.

22
00:01:28,200 --> 00:01:28,520
All right.

23
00:01:28,530 --> 00:01:34,200
So this time we're going to train a decision tree regression model to understand the correlations between

24
00:01:34,200 --> 00:01:35,960
these two features.

25
00:01:35,970 --> 00:01:38,690
However I have to say something important here.

26
00:01:38,820 --> 00:01:45,450
The decision tree regression model is not really well adapted to you know these simple datasets.

27
00:01:45,460 --> 00:01:50,830
You know with only one feature and the dependent variable vector you'll see what I mean by that at the

28
00:01:50,840 --> 00:01:51,180
end.

29
00:01:51,270 --> 00:01:53,420
On the visualization graphs.

30
00:01:53,490 --> 00:02:00,450
But having said that I would like you not to worry because the implementation of the decision tree regression

31
00:02:00,450 --> 00:02:06,990
model we were about to build will still work on any other data sets you know with several features.

32
00:02:07,050 --> 00:02:11,010
Here we have one feature but the code we're about to make will work for data sets.

33
00:02:11,010 --> 00:02:13,790
Having any numbers of features.

34
00:02:13,860 --> 00:02:14,100
All right.

35
00:02:14,100 --> 00:02:19,720
So even if the results won't be beautiful in the end well you will still be able to use this decision

36
00:02:19,790 --> 00:02:25,500
tree regression implementation on your other data sets even if they have hundreds of features but then

37
00:02:25,740 --> 00:02:29,490
make sure to add some data processing tools if needed.

38
00:02:29,490 --> 00:02:35,370
For example if your dataset has some categorical data or missing data but you don't have to apply features

39
00:02:35,370 --> 00:02:41,160
killing for decision tree regression and neither for running false regression which will be our next

40
00:02:41,160 --> 00:02:43,340
model you know in the next section.

41
00:02:43,560 --> 00:02:43,990
All right.

42
00:02:44,100 --> 00:02:50,610
So that's the important thing I wanted to say here and now we're going to start our implementation by

43
00:02:50,730 --> 00:02:57,270
double clicking this file here which you can either open with Google cool AB if you like Google cool

44
00:02:57,270 --> 00:02:59,790
add like me or Jupiter notebook.

45
00:02:59,790 --> 00:03:00,080
All right.

46
00:03:00,090 --> 00:03:01,620
So choose your favorite.

47
00:03:01,710 --> 00:03:04,740
And now let's open Google laboratory.

48
00:03:04,740 --> 00:03:10,070
It is opening a notebook and here we go.

49
00:03:10,080 --> 00:03:12,310
That's the whole implementation.

50
00:03:12,330 --> 00:03:12,590
All right.

51
00:03:12,600 --> 00:03:19,260
So as usual now we're going to create a copy of this notebook because this is an read only mode which

52
00:03:19,260 --> 00:03:22,130
means you can't modify it or recode it.

53
00:03:22,170 --> 00:03:29,670
So we're gonna go to file here and then click here save a copy in drive and this will as you can see

54
00:03:29,670 --> 00:03:35,370
create a copy of this notebook on which you will be able to read code on it.

55
00:03:35,400 --> 00:03:39,060
You know read implement this decision tree regression model.

56
00:03:39,210 --> 00:03:40,050
Perfect.

57
00:03:40,050 --> 00:03:47,790
So now you know the next step we're going to delete the code cells but since it is the third time we

58
00:03:47,790 --> 00:03:54,270
actually work on this position salaries data set and of course each time it's the same two first steps

59
00:03:54,570 --> 00:03:58,580
of the data processing phase importing the libraries and importing the data set.

60
00:03:58,590 --> 00:04:00,930
Well this time we want re implement this.

61
00:04:01,020 --> 00:04:03,540
We will just leave them and not delete them.

62
00:04:03,540 --> 00:04:08,450
So we will just delete all the code cells from here you know from the step training the decision to

63
00:04:08,460 --> 00:04:10,390
regression model on the whole dataset.

64
00:04:10,410 --> 00:04:11,580
All right let's do this.

65
00:04:11,580 --> 00:04:18,930
Let us start by deleting this one because we will re implement it together then this one and now this

66
00:04:18,930 --> 00:04:19,410
one.

67
00:04:19,440 --> 00:04:20,160
All right.

68
00:04:20,220 --> 00:04:20,910
Perfect.

69
00:04:20,910 --> 00:04:26,520
Also you can notice that at the end we will only visualize the decision tree regression results in high

70
00:04:26,520 --> 00:04:32,160
resolution because you will see and I will show this to you that the decision tree regression results

71
00:04:32,190 --> 00:04:37,760
in low resolution you know without applying the grid solution will absolutely not make sense.

72
00:04:37,770 --> 00:04:40,700
And I will explain that at the end of this section.

73
00:04:40,710 --> 00:04:41,120
All right.

74
00:04:41,460 --> 00:04:43,390
So we're almost ready to start now.

75
00:04:43,390 --> 00:04:50,370
We are going just to upload you know the dataset by clicking this little folder here and then right

76
00:04:50,400 --> 00:04:57,060
now it is connecting to runtime to enable foul browsing and in a second we should be able to see the

77
00:04:57,210 --> 00:04:58,080
upload button.

78
00:04:58,080 --> 00:05:04,930
Here we go to upload indeed the data set and now we are on my desktop.

79
00:05:05,080 --> 00:05:06,680
That's where I put my machinery.

80
00:05:06,720 --> 00:05:07,390
It is it for them.

81
00:05:07,390 --> 00:05:10,540
But make sure to go wherever you saved it.

82
00:05:10,630 --> 00:05:16,860
And now inside we're gonna go to parts of regression and then decision tree regression and then python.

83
00:05:16,930 --> 00:05:18,070
And then there you go.

84
00:05:18,070 --> 00:05:22,920
You select your position salaries data set and click open.

85
00:05:23,380 --> 00:05:26,770
And this will upload the data set inside the notebook.

86
00:05:26,860 --> 00:05:28,090
And now we're ready to start.

87
00:05:28,090 --> 00:05:30,550
Let's just run these two cells here.

88
00:05:30,550 --> 00:05:37,090
The first one to import the libraries and then the second one to import the data set.

89
00:05:37,090 --> 00:05:37,420
All right.

90
00:05:37,420 --> 00:05:37,870
Perfect.

91
00:05:37,880 --> 00:05:43,840
So now we have the data set and of course the matrix of features X containing only the position levels

92
00:05:44,170 --> 00:05:47,680
and the dependent variable vector containing the salaries.

93
00:05:47,680 --> 00:05:48,320
All right.

94
00:05:48,400 --> 00:05:54,310
And here a quick reminder this model that we are about to build you will truly be able to implement

95
00:05:54,310 --> 00:05:59,710
it on your data sets and you will only have two things to change which are first of course the name

96
00:05:59,710 --> 00:06:05,680
of the dataset here you will put the name of your data set and then in a matrix of features well maybe

97
00:06:05,860 --> 00:06:08,470
you will want to select all the columns.

98
00:06:08,470 --> 00:06:15,490
And here we just excluded the first column because this contained only the positions and strings which

99
00:06:15,610 --> 00:06:18,460
are exactly the same as the levels in this column.

100
00:06:18,460 --> 00:06:24,100
So of course we didn't want included but check your dataset check if you want to include all the columns

101
00:06:24,400 --> 00:06:32,230
and mostly check if you need to apply some of the tools of your data pricing toolkit which are either

102
00:06:32,320 --> 00:06:37,030
you know taking care of missing data or encoding categorical data.

103
00:06:37,150 --> 00:06:42,940
Right so you would need to check the variables and see if there are some categorical variables in strings

104
00:06:43,210 --> 00:06:49,480
if the order matter is like for example the size of a close while you will apply label encoder and if

105
00:06:49,690 --> 00:06:56,020
the order doesn't matter like some countries or some states well you will apply column transformer with

106
00:06:56,290 --> 00:06:57,420
one HUD encoder.

107
00:06:57,430 --> 00:06:58,120
All right.

108
00:06:58,120 --> 00:07:01,420
And then you don't have to apply feature scaling.

109
00:07:01,420 --> 00:07:06,430
You can totally split your data set into the training set in a test if you want to evaluate your model

110
00:07:06,460 --> 00:07:12,730
on your observations but you don't have to apply feature scaling for Decision Tree regression and neither

111
00:07:12,970 --> 00:07:15,760
for random forest regression Why is that.

112
00:07:15,760 --> 00:07:20,110
That's because you know the predictions from a decision tree regression or random forest regression

113
00:07:20,110 --> 00:07:26,170
model are resulting from successive splits of the data you know through the different notes of your

114
00:07:26,170 --> 00:07:31,480
tree and therefore there are not some equations like with the previous models and that's why of course

115
00:07:31,510 --> 00:07:36,760
no feature scaling is needed to you know split the different values of your feature into these different

116
00:07:36,760 --> 00:07:42,700
categories leading to different predictions we can still do this with the original scale of your features

117
00:07:42,910 --> 00:07:46,090
even if your features take different ranges of values.

118
00:07:46,090 --> 00:07:46,480
All right.

119
00:07:46,810 --> 00:07:53,060
So remember this no feature scaling and then check for the other tools but just for your future datasets

120
00:07:53,140 --> 00:07:56,750
where you would like to apply decision tree regression.

121
00:07:57,130 --> 00:07:57,440
Okay.

122
00:07:57,460 --> 00:07:58,060
Perfect.

123
00:07:58,180 --> 00:08:03,220
So we have everything we have the dataset and now we are ready to build the decision tree regression

124
00:08:03,220 --> 00:08:04,900
model on the whole dataset.

125
00:08:04,900 --> 00:08:05,170
Right.

126
00:08:05,170 --> 00:08:06,820
This time we don't want to split it.

127
00:08:06,850 --> 00:08:13,710
We want to leverage the maximum data to understand the correlations in this small amount of information.

128
00:08:13,900 --> 00:08:15,860
So we will turn it on the whole data set.

129
00:08:15,970 --> 00:08:21,180
Then we will predict our final result in the salary of the position that will number six point five.

130
00:08:21,340 --> 00:08:26,890
And then at the end we will visualize what the regression curve of the decision tree regression model

131
00:08:27,280 --> 00:08:27,940
looks like.

132
00:08:28,450 --> 00:08:28,930
All right.

133
00:08:28,930 --> 00:08:31,330
So let's do all this in the next three tutorials.

134
00:08:31,330 --> 00:08:36,420
Starting with this step training the decision tree regression model on the whole dataset.

135
00:08:36,550 --> 00:08:38,410
And until then enjoy machine learning.
