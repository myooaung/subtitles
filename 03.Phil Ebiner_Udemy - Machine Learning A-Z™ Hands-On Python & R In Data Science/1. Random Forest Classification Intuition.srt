1

00:00:00,930  -->  00:00:03,210
Hello and welcome back to the course of machine learning.

2

00:00:03,270  -->  00:00:05,910
Today we're talking about random forests.

3

00:00:06,090  -->  00:00:07,330
Let's have a look inside.

4

00:00:07,620  -->  00:00:11,530
So first thing we're going to learn is a new concept ensemble learning.

5

00:00:11,580  -->  00:00:18,150
What is ensemble learning and learning is when you take multiple machine learning algorithms and put

6

00:00:18,150  -->  00:00:23,010
them together to create one bigger machine learning algorithms so that machine learning algorithm the

7

00:00:23,010  -->  00:00:28,470
final one is actually using leveraging many different other machine learning algorithms and they can

8

00:00:28,470  -->  00:00:31,500
be the same machine learning algorithm as we will see today.

9

00:00:31,500  -->  00:00:36,720
We're going to be looking at the random forest method which combines a lot of decision tree method so

10

00:00:36,960  -->  00:00:40,860
instead of running a decision tree method which we talked about earlier.

11

00:00:40,910  -->  00:00:46,040
So running at once we're going to run it multiple times and that will give us a run of four.

12

00:00:46,060  -->  00:00:50,850
So let's have a look at the step by step process to understand how all of this happens.

13

00:00:50,850  -->  00:00:55,050
So step one is you pick a random K data points from the training set.

14

00:00:55,230  -->  00:00:59,460
Then you build a decision tree associated to those data points.

15

00:00:59,460  -->  00:01:03,490
So instead of building a decision tree based on everything you have in your daughter said you build

16

00:01:03,490  -->  00:01:06,750
a decision tree just based on some of the data points that you have.

17

00:01:06,750  -->  00:01:11,850
So kind of like a subset of ALL your daughter said Next you choose the number of trees you want to build

18

00:01:11,860  -->  00:01:11,960
.

19

00:01:12,150  -->  00:01:18,300
And you repeat steps 1 and 2 and then once you have all those trees and you have a new data point so

20

00:01:18,300  -->  00:01:23,340
when you want to check where a new Torben falls how is classified for a new data point you make each

21

00:01:23,340  -->  00:01:28,580
one of your entry trees predict the category of which the data point belongs to.

22

00:01:28,680  -->  00:01:32,730
And then you assign the new data points to the category that wins the majority vote.

23

00:01:32,730  -->  00:01:34,920
So that's how a random forest works.

24

00:01:34,920  -->  00:01:39,420
So basically you start off with one tree and then you build another tree another tree another tree and

25

00:01:39,600  -->  00:01:44,190
each one of those trees is being built on a randomly selected subset from your daughter.

26

00:01:44,460  -->  00:01:50,670
And even though each one of those trees might not be ideal overall on average they can perform very

27

00:01:50,670  -->  00:01:53,610
well and that's a major advantage of this algorithm.

28

00:01:53,640  -->  00:01:58,910
It's kind of leveraging the power of the crowd so to speak and sort of just relying on one tree.

29

00:01:58,980  -->  00:02:04,990
It's checking what all the trees are going to say and then just taking the majority vote and deciding

30

00:02:05,000  -->  00:02:06,290
the classification based on that.

31

00:02:06,510  -->  00:02:13,590
And that power of numbers can help get rid of certain errors and certain uncertainties in your algorithm

32

00:02:13,650  -->  00:02:15,350
and make it more precise.

33

00:02:15,450  -->  00:02:21,420
And in fact it's such a good solution that when Microsoft were developing Kinect on this device that

34

00:02:21,420  -->  00:02:27,210
allows you to play games on your television that little device over there that attaches to X-Box and

35

00:02:27,210  -->  00:02:34,230
then you can play games without any controller so here that device is using an infrared grid to understand

36

00:02:34,590  -->  00:02:40,620
where the hands arms head and other parts of the body of these people are located and how they're moving

37

00:02:41,160  -->  00:02:47,060
and is using machine learning to understand how the body parts are moving and where exactly they're

38

00:02:47,070  -->  00:02:48,830
located in space.

39

00:02:48,840  -->  00:02:55,290
So when Microsoft was developing Kinect they decided to go with the random forest algorithm over all

40

00:02:55,290  -->  00:03:01,680
of the other machine learning algorithms that were available to them and use the rainforest to develop

41

00:03:01,980  -->  00:03:05,990
this sophisticated piece of hardware or slash software.

42

00:03:06,060  -->  00:03:11,520
And actually they have a interesting article about it so I'm just going to show it to you now if you

43

00:03:11,640  -->  00:03:12,880
can find on the Internet.

44

00:03:12,930  -->  00:03:14,850
So it's a burst of dot com.

45

00:03:14,850  -->  00:03:16,770
This is from there.

46

00:03:17,010  -->  00:03:19,290
And you can definitely find it there.

47

00:03:19,290  -->  00:03:23,880
It's called real time human poser cognition parts from single depth images.

48

00:03:23,910  -->  00:03:30,560
And here it explains exactly so you can actually see the random forest in action so you can see that

49

00:03:30,930  -->  00:03:34,480
this is similar to what we were talking about before in the classification trees.

50

00:03:34,500  -->  00:03:41,050
But here is actually using random for us to understand where body parts are and then based on that the

51

00:03:41,260  -->  00:03:45,810
device finds what it needs to do in that computer game.

52

00:03:45,810  -->  00:03:47,220
So that's how it works.

53

00:03:47,220  -->  00:03:53,790
And so if I just search for the word forest you'll see that decision for his decision for his decision

54

00:03:53,790  -->  00:03:59,700
for us and actually explain that they were able to achieve faster speeds faster processing with the

55

00:03:59,700  -->  00:04:04,800
decision for us and therefore that you know reduce the cost of the hardware that they're required for

56

00:04:04,800  -->  00:04:05,660
this tool.

57

00:04:05,760  -->  00:04:10,530
Interesting article that I chickened out if you want to learn a bit more about a real life practical

58

00:04:10,530  -->  00:04:17,310
application of a decision forest or a random forest and that's it for today's tutorial I hope you enjoyed

59

00:04:17,310  -->  00:04:23,100
learning a bit about an ensemble type of machine learning algorithm definitely the practical side is

60

00:04:23,100  -->  00:04:24,810
going to be quite interesting as well.

61

00:04:24,810  -->  00:04:26,390
And I look forward to seeing you next time.

62

00:04:26,400  -->  00:04:28,050
Until then in Germany in learning
